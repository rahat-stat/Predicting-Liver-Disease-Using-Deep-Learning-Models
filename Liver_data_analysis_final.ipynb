{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8061dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5324772",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"liver_kaggle_clean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b2bfb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f0c92b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "0",
         "rawType": "object",
         "type": "unknown"
        }
       ],
       "ref": "808518dd-b422-4141-8db1-4f5560e714b4",
       "rows": [
        [
         "Age of the patient",
         "int64"
        ],
        [
         "Gender of the patient",
         "object"
        ],
        [
         "Total Bilirubin",
         "float64"
        ],
        [
         "Direct Bilirubin",
         "float64"
        ],
        [
         " Alkphos Alkaline Phosphotase",
         "int64"
        ],
        [
         " Sgpt Alamine Aminotransferase",
         "int64"
        ],
        [
         "Sgot Aspartate Aminotransferase",
         "int64"
        ],
        [
         "Total Protiens",
         "float64"
        ],
        [
         " ALB Albumin",
         "float64"
        ],
        [
         "A/G Ratio Albumin and Globulin Ratio",
         "float64"
        ],
        [
         "Result",
         "int64"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 11
       }
      },
      "text/plain": [
       "Age of the patient                        int64\n",
       "Gender of the patient                    object\n",
       "Total Bilirubin                         float64\n",
       "Direct Bilirubin                        float64\n",
       " Alkphos Alkaline Phosphotase             int64\n",
       " Sgpt Alamine Aminotransferase            int64\n",
       "Sgot Aspartate Aminotransferase           int64\n",
       "Total Protiens                          float64\n",
       " ALB Albumin                            float64\n",
       "A/G Ratio Albumin and Globulin Ratio    float64\n",
       "Result                                    int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b6a48e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Age of the patient', 'Gender of the patient', 'Total Bilirubin',\n",
       "       'Direct Bilirubin', ' Alkphos Alkaline Phosphotase',\n",
       "       ' Sgpt Alamine Aminotransferase', 'Sgot Aspartate Aminotransferase',\n",
       "       'Total Protiens', ' ALB Albumin',\n",
       "       'A/G Ratio Albumin and Globulin Ratio', 'Result'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a865b28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Age of the patient",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Total Bilirubin",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Direct Bilirubin",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Alkphos Alkaline Phosphotase",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Sgpt Alamine Aminotransferase",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Sgot Aspartate Aminotransferase",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Total Protiens",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "ALB Albumin",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "A/G Ratio Albumin and Globulin Ratio",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Result",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Gender of the patient",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "9e609b59-e0a9-4fd9-86f4-3dd42ed8ecb3",
       "rows": [
        [
         "count",
         "16389.0",
         "16389.0",
         "16389.0",
         "16389.0",
         "16389.0",
         "16389.0",
         "16389.0",
         "16389.0",
         "16389.0",
         "16389.0",
         "16389.0"
        ],
        [
         "mean",
         "43.770516810055526",
         "3.3604307767404964",
         "1.5304289462444323",
         "290.8268350723046",
         "80.1472939166514",
         "111.36756360973824",
         "6.487705168100555",
         "3.1365733113673806",
         "0.9466117517847337",
         "1.2830557080968943",
         "0.696442735981451"
        ],
        [
         "std",
         "16.529486957702467",
         "6.20870801739163",
         "2.894558358638046",
         "240.94597184265461",
         "180.01017999860295",
         "280.6659937299455",
         "1.0905487584437004",
         "0.7940058710327423",
         "0.32333678646590314",
         "0.45049701149674126",
         "0.4598077335021286"
        ],
        [
         "min",
         "4.0",
         "0.4",
         "0.1",
         "63.0",
         "10.0",
         "10.0",
         "2.7",
         "0.9",
         "0.3",
         "1.0",
         "0.0"
        ],
        [
         "25%",
         "32.0",
         "0.8",
         "0.2",
         "175.0",
         "23.0",
         "25.0",
         "5.8",
         "2.6",
         "0.7",
         "1.0",
         "0.0"
        ],
        [
         "50%",
         "45.0",
         "1.0",
         "0.3",
         "209.0",
         "35.0",
         "42.0",
         "6.6",
         "3.1",
         "0.93",
         "1.0",
         "1.0"
        ],
        [
         "75%",
         "55.0",
         "2.7",
         "1.3",
         "298.0",
         "62.0",
         "88.0",
         "7.2",
         "3.8",
         "1.1",
         "2.0",
         "1.0"
        ],
        [
         "max",
         "90.0",
         "75.0",
         "19.7",
         "2110.0",
         "2000.0",
         "4929.0",
         "9.6",
         "5.5",
         "2.8",
         "2.0",
         "1.0"
        ]
       ],
       "shape": {
        "columns": 11,
        "rows": 8
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age of the patient</th>\n",
       "      <th>Total Bilirubin</th>\n",
       "      <th>Direct Bilirubin</th>\n",
       "      <th>Alkphos Alkaline Phosphotase</th>\n",
       "      <th>Sgpt Alamine Aminotransferase</th>\n",
       "      <th>Sgot Aspartate Aminotransferase</th>\n",
       "      <th>Total Protiens</th>\n",
       "      <th>ALB Albumin</th>\n",
       "      <th>A/G Ratio Albumin and Globulin Ratio</th>\n",
       "      <th>Result</th>\n",
       "      <th>Gender of the patient</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>16389.000000</td>\n",
       "      <td>16389.000000</td>\n",
       "      <td>16389.000000</td>\n",
       "      <td>16389.000000</td>\n",
       "      <td>16389.000000</td>\n",
       "      <td>16389.000000</td>\n",
       "      <td>16389.000000</td>\n",
       "      <td>16389.000000</td>\n",
       "      <td>16389.000000</td>\n",
       "      <td>16389.000000</td>\n",
       "      <td>16389.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>43.770517</td>\n",
       "      <td>3.360431</td>\n",
       "      <td>1.530429</td>\n",
       "      <td>290.826835</td>\n",
       "      <td>80.147294</td>\n",
       "      <td>111.367564</td>\n",
       "      <td>6.487705</td>\n",
       "      <td>3.136573</td>\n",
       "      <td>0.946612</td>\n",
       "      <td>1.283056</td>\n",
       "      <td>0.696443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>16.529487</td>\n",
       "      <td>6.208708</td>\n",
       "      <td>2.894558</td>\n",
       "      <td>240.945972</td>\n",
       "      <td>180.010180</td>\n",
       "      <td>280.665994</td>\n",
       "      <td>1.090549</td>\n",
       "      <td>0.794006</td>\n",
       "      <td>0.323337</td>\n",
       "      <td>0.450497</td>\n",
       "      <td>0.459808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>2.700000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>32.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>175.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>5.800000</td>\n",
       "      <td>2.600000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>45.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>209.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>6.600000</td>\n",
       "      <td>3.100000</td>\n",
       "      <td>0.930000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>55.000000</td>\n",
       "      <td>2.700000</td>\n",
       "      <td>1.300000</td>\n",
       "      <td>298.000000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>88.000000</td>\n",
       "      <td>7.200000</td>\n",
       "      <td>3.800000</td>\n",
       "      <td>1.100000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>90.000000</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>19.700000</td>\n",
       "      <td>2110.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>4929.000000</td>\n",
       "      <td>9.600000</td>\n",
       "      <td>5.500000</td>\n",
       "      <td>2.800000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Age of the patient  Total Bilirubin  Direct Bilirubin  \\\n",
       "count        16389.000000     16389.000000      16389.000000   \n",
       "mean            43.770517         3.360431          1.530429   \n",
       "std             16.529487         6.208708          2.894558   \n",
       "min              4.000000         0.400000          0.100000   \n",
       "25%             32.000000         0.800000          0.200000   \n",
       "50%             45.000000         1.000000          0.300000   \n",
       "75%             55.000000         2.700000          1.300000   \n",
       "max             90.000000        75.000000         19.700000   \n",
       "\n",
       "       Alkphos Alkaline Phosphotase  Sgpt Alamine Aminotransferase  \\\n",
       "count                  16389.000000                   16389.000000   \n",
       "mean                     290.826835                      80.147294   \n",
       "std                      240.945972                     180.010180   \n",
       "min                       63.000000                      10.000000   \n",
       "25%                      175.000000                      23.000000   \n",
       "50%                      209.000000                      35.000000   \n",
       "75%                      298.000000                      62.000000   \n",
       "max                     2110.000000                    2000.000000   \n",
       "\n",
       "       Sgot Aspartate Aminotransferase  Total Protiens   ALB Albumin  \\\n",
       "count                     16389.000000    16389.000000  16389.000000   \n",
       "mean                        111.367564        6.487705      3.136573   \n",
       "std                         280.665994        1.090549      0.794006   \n",
       "min                          10.000000        2.700000      0.900000   \n",
       "25%                          25.000000        5.800000      2.600000   \n",
       "50%                          42.000000        6.600000      3.100000   \n",
       "75%                          88.000000        7.200000      3.800000   \n",
       "max                        4929.000000        9.600000      5.500000   \n",
       "\n",
       "       A/G Ratio Albumin and Globulin Ratio        Result  \\\n",
       "count                          16389.000000  16389.000000   \n",
       "mean                               0.946612      1.283056   \n",
       "std                                0.323337      0.450497   \n",
       "min                                0.300000      1.000000   \n",
       "25%                                0.700000      1.000000   \n",
       "50%                                0.930000      1.000000   \n",
       "75%                                1.100000      2.000000   \n",
       "max                                2.800000      2.000000   \n",
       "\n",
       "       Gender of the patient  \n",
       "count           16389.000000  \n",
       "mean                0.696443  \n",
       "std                 0.459808  \n",
       "min                 0.000000  \n",
       "25%                 0.000000  \n",
       "50%                 1.000000  \n",
       "75%                 1.000000  \n",
       "max                 1.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4b72b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Fix alkaline phosphotase column\n",
    "df['Alkaline Phosphatase'] = df['Alkphos Alkaline Phosphotase']\n",
    "df.drop(columns=['Alkphos Alkaline Phosphotase'], inplace=True)\n",
    "\n",
    "# Convert Gender to numeric\n",
    "df['Gender of the patient'] = df['Gender of the patient'].str.strip().str.lower().map({\n",
    "    'male': 1,\n",
    "    'female': 0\n",
    "})\n",
    "\n",
    "# Features for heatmap\n",
    "features = [\n",
    "    'Age of the patient',\n",
    "    'Gender of the patient', 'Total Bilirubin', 'Direct Bilirubin',\n",
    "    'Alkaline Phosphatase', 'Sgpt Alamine Aminotransferase',\n",
    "    'Sgot Aspartate Aminotransferase', 'Total Protiens', 'ALB Albumin',\n",
    "    'A/G Ratio Albumin and Globulin Ratio', 'Result'\n",
    "]\n",
    "\n",
    "# Compute correlation matrix\n",
    "corr_matrix = df[features].corr()\n",
    "\n",
    "# Plot heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, fmt=\".2f\", cmap=\"coolwarm\", linewidths=0.5)\n",
    "plt.title(\"Feature Correlation Heatmap - Liver Disease Dataset\", fontsize=14, pad=15)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49e5aba",
   "metadata": {},
   "source": [
    "Deep learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f149f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rahat\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "c:\\Users\\rahat\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "c:\\Users\\rahat\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Training CNN-LSTM...\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step\n",
      "\n",
      "📊 Classification Report for CNN-LSTM:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.99      2341\n",
      "           1       0.96      0.98      0.97       937\n",
      "\n",
      "    accuracy                           0.98      3278\n",
      "   macro avg       0.97      0.98      0.98      3278\n",
      "weighted avg       0.98      0.98      0.98      3278\n",
      "\n",
      "🎯 ROC-AUC for CNN-LSTM: 0.9960\n",
      "\n",
      "🔍 Training LSTM...\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
      "\n",
      "📊 Classification Report for LSTM:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00      2341\n",
      "           1       0.99      0.99      0.99       937\n",
      "\n",
      "    accuracy                           0.99      3278\n",
      "   macro avg       0.99      0.99      0.99      3278\n",
      "weighted avg       0.99      0.99      0.99      3278\n",
      "\n",
      "🎯 ROC-AUC for LSTM: 0.9991\n",
      "\n",
      "🔍 Training GRU...\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\n",
      "📊 Classification Report for GRU:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.95      0.95      2341\n",
      "           1       0.88      0.85      0.86       937\n",
      "\n",
      "    accuracy                           0.92      3278\n",
      "   macro avg       0.91      0.90      0.91      3278\n",
      "weighted avg       0.92      0.92      0.92      3278\n",
      "\n",
      "🎯 ROC-AUC for GRU: 0.9781\n",
      "\n",
      "🔍 Training DNN...\n",
      "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\n",
      "📊 Classification Report for DNN:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.98      2341\n",
      "           1       0.99      0.92      0.95       937\n",
      "\n",
      "    accuracy                           0.97      3278\n",
      "   macro avg       0.98      0.96      0.97      3278\n",
      "weighted avg       0.97      0.97      0.97      3278\n",
      "\n",
      "🎯 ROC-AUC for DNN: 0.9972\n",
      "\n",
      "🔍 Training CatBoost...\n",
      "\n",
      "📊 Classification Report for CatBoost:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      1.00      0.96      2341\n",
      "           1       0.99      0.81      0.89       937\n",
      "\n",
      "    accuracy                           0.94      3278\n",
      "   macro avg       0.96      0.90      0.93      3278\n",
      "weighted avg       0.95      0.94      0.94      3278\n",
      "\n",
      "🎯 ROC-AUC for CatBoost: 0.9890\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import classification_report, roc_auc_score, roc_curve, auc\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, GRU, Dense, Conv1D, Flatten, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# -----------------------\n",
    "# Load & preprocess data\n",
    "# -----------------------\n",
    "encoder = LabelEncoder()\n",
    "df['Gender of the patient'] = encoder.fit_transform(df['Gender of the patient'])\n",
    "target_col = \"Result\"\n",
    "X = df.drop(columns=[target_col])\n",
    "y = df[target_col].map({1: 0, 2: 1})   # binary target\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split\n",
    "train_size = int(len(X_scaled) * 0.8)\n",
    "X_train, X_test = X_scaled[:train_size], X_scaled[train_size:]\n",
    "y_train, y_test = y[:train_size], y[train_size:]\n",
    "\n",
    "# One-hot encode for deep learning\n",
    "num_classes = len(np.unique(y_train))\n",
    "y_train_cat = to_categorical(y_train, num_classes)\n",
    "y_test_cat = to_categorical(y_test, num_classes)\n",
    "\n",
    "# Reshape for sequential models\n",
    "X_train_seq = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "X_test_seq = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "input_shape = (X_train_seq.shape[1], 1)\n",
    "\n",
    "# -----------------------\n",
    "# Define Models\n",
    "# -----------------------\n",
    "\n",
    "def build_cnn_lstm():\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=64, kernel_size=2, activation=\"tanh\", input_shape=input_shape))\n",
    "    model.add(LSTM(96, activation=\"tanh\"))\n",
    "    model.add(Dense(num_classes, activation=\"softmax\"))\n",
    "    model.compile(optimizer=Adam(learning_rate=0.01), loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "def build_lstm():\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(80, activation=\"tanh\", input_shape=input_shape))\n",
    "    model.add(Dense(num_classes, activation=\"softmax\"))\n",
    "    model.compile(optimizer=Adam(learning_rate=0.01), loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "def build_gru():\n",
    "    model = Sequential()\n",
    "    model.add(GRU(64, activation=\"relu\", input_shape=input_shape))\n",
    "    model.add(Dense(num_classes, activation=\"softmax\"))\n",
    "    model.compile(optimizer=Adam(learning_rate=0.01), loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "def build_dnn():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, activation=\"relu\", input_shape=(X_train.shape[1],)))\n",
    "    model.add(Dropout(0.0))   # as given\n",
    "    model.add(Dense(64, activation=\"relu\"))\n",
    "    model.add(Dense(num_classes, activation=\"softmax\"))\n",
    "    model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "# CatBoost\n",
    "cat_model = CatBoostClassifier(\n",
    "    iterations=100,\n",
    "    learning_rate=0.1,\n",
    "    depth=5,\n",
    "    loss_function='Logloss',\n",
    "    eval_metric='AUC',\n",
    "    verbose=0,\n",
    "    random_seed=42\n",
    ")\n",
    "\n",
    "# -----------------------\n",
    "# Train & Evaluate Models\n",
    "# -----------------------\n",
    "models = {\n",
    "    \"CNN-LSTM\": build_cnn_lstm(),\n",
    "    \"LSTM\": build_lstm(),\n",
    "    \"GRU\": build_gru(),\n",
    "    \"DNN\": build_dnn(),\n",
    "}\n",
    "\n",
    "results = {}\n",
    "roc_curves = {}\n",
    "\n",
    "# Train DL models\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n🔍 Training {name}...\")\n",
    "    if name == \"DNN\":\n",
    "        history = model.fit(X_train, y_train_cat, epochs=50, batch_size=16, verbose=0,\n",
    "                            validation_data=(X_test, y_test_cat))\n",
    "        y_pred = model.predict(X_test)\n",
    "    else:\n",
    "        history = model.fit(X_train_seq, y_train_cat, epochs=15, batch_size=32, verbose=0,\n",
    "                            validation_data=(X_test_seq, y_test_cat))\n",
    "        y_pred = model.predict(X_test_seq)\n",
    "\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "    print(f\"\\n📊 Classification Report for {name}:\\n\")\n",
    "    print(classification_report(y_test, y_pred_classes, zero_division=0))\n",
    "\n",
    "    roc_auc = roc_auc_score(y_test, y_pred[:, 1])\n",
    "    print(f\"🎯 ROC-AUC for {name}: {roc_auc:.4f}\")\n",
    "\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred[:, 1])\n",
    "    roc_curves[name] = (fpr, tpr, roc_auc)\n",
    "\n",
    "    results[name] = {\"report\": classification_report(y_test, y_pred_classes, output_dict=True),\n",
    "                     \"roc_auc\": roc_auc}\n",
    "\n",
    "# Train CatBoost\n",
    "print(\"\\n🔍 Training CatBoost...\")\n",
    "cat_model.fit(X_train, y_train)\n",
    "y_pred_cb = cat_model.predict_proba(X_test)[:, 1]\n",
    "y_pred_classes_cb = cat_model.predict(X_test)\n",
    "\n",
    "print(f\"\\n📊 Classification Report for CatBoost:\\n\")\n",
    "print(classification_report(y_test, y_pred_classes_cb, zero_division=0))\n",
    "\n",
    "roc_auc_cb = roc_auc_score(y_test, y_pred_cb)\n",
    "print(f\"🎯 ROC-AUC for CatBoost: {roc_auc_cb:.4f}\")\n",
    "\n",
    "fpr_cb, tpr_cb, _ = roc_curve(y_test, y_pred_cb)\n",
    "roc_curves[\"CatBoost\"] = (fpr_cb, tpr_cb, roc_auc_cb)\n",
    "\n",
    "# -----------------------\n",
    "# ROC Curve Plot\n",
    "# -----------------------\n",
    "plt.figure(figsize=(8, 6))\n",
    "for name, (fpr, tpr, roc_auc) in roc_curves.items():\n",
    "    plt.plot(fpr, tpr, lw=2, label=f\"{name} (AUC={roc_auc:.3f})\")\n",
    "\n",
    "plt.plot([0, 1], [0, 1], linestyle=\"--\", color=\"gray\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curves for Models\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18064305",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- > Fold 1 < ---------------\n",
      "Epoch 1/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6431 - loss: 0.6067\n",
      "Epoch 1: val_loss improved from inf to 0.52423, saving model to folds0.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 42ms/step - accuracy: 0.6463 - loss: 0.6041 - val_accuracy: 0.7120 - val_loss: 0.5242 - learning_rate: 0.0100\n",
      "Epoch 2/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7241 - loss: 0.5189  \n",
      "Epoch 2: val_loss improved from 0.52423 to 0.50355, saving model to folds0.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.7202 - loss: 0.5207 - val_accuracy: 0.7132 - val_loss: 0.5036 - learning_rate: 0.0100\n",
      "Epoch 3/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7239 - loss: 0.5037\n",
      "Epoch 3: val_loss improved from 0.50355 to 0.49194, saving model to folds0.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.7240 - loss: 0.5034 - val_accuracy: 0.7108 - val_loss: 0.4919 - learning_rate: 0.0100\n",
      "Epoch 4/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7188 - loss: 0.4933 \n",
      "Epoch 4: val_loss improved from 0.49194 to 0.47690, saving model to folds0.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.7204 - loss: 0.4932 - val_accuracy: 0.7474 - val_loss: 0.4769 - learning_rate: 0.0100\n",
      "Epoch 5/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7318 - loss: 0.4804\n",
      "Epoch 5: val_loss improved from 0.47690 to 0.47009, saving model to folds0.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.7316 - loss: 0.4804 - val_accuracy: 0.7492 - val_loss: 0.4701 - learning_rate: 0.0100\n",
      "Epoch 6/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7345 - loss: 0.4800 \n",
      "Epoch 6: val_loss improved from 0.47009 to 0.46588, saving model to folds0.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7332 - loss: 0.4805 - val_accuracy: 0.7413 - val_loss: 0.4659 - learning_rate: 0.0100\n",
      "Epoch 7/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7395 - loss: 0.4729 \n",
      "Epoch 7: val_loss did not improve from 0.46588\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7386 - loss: 0.4729 - val_accuracy: 0.7407 - val_loss: 0.4670 - learning_rate: 0.0100\n",
      "Epoch 8/300\n",
      "\u001b[1m12/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7363 - loss: 0.4692 \n",
      "Epoch 8: val_loss improved from 0.46588 to 0.46248, saving model to folds0.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7366 - loss: 0.4696 - val_accuracy: 0.7559 - val_loss: 0.4625 - learning_rate: 0.0100\n",
      "Epoch 9/300\n",
      "\u001b[1m12/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7359 - loss: 0.4699 \n",
      "Epoch 9: val_loss improved from 0.46248 to 0.45939, saving model to folds0.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7364 - loss: 0.4699 - val_accuracy: 0.7517 - val_loss: 0.4594 - learning_rate: 0.0100\n",
      "Epoch 10/300\n",
      "\u001b[1m12/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7388 - loss: 0.4708 \n",
      "Epoch 10: val_loss improved from 0.45939 to 0.45937, saving model to folds0.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7384 - loss: 0.4703 - val_accuracy: 0.7468 - val_loss: 0.4594 - learning_rate: 0.0100\n",
      "Epoch 11/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7414 - loss: 0.4667 \n",
      "Epoch 11: val_loss improved from 0.45937 to 0.45824, saving model to folds0.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7407 - loss: 0.4669 - val_accuracy: 0.7529 - val_loss: 0.4582 - learning_rate: 0.0100\n",
      "Epoch 12/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7517 - loss: 0.4574 \n",
      "Epoch 12: val_loss did not improve from 0.45824\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7502 - loss: 0.4578 - val_accuracy: 0.7291 - val_loss: 0.4588 - learning_rate: 0.0100\n",
      "Epoch 13/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7400 - loss: 0.4603 \n",
      "Epoch 13: val_loss improved from 0.45824 to 0.44866, saving model to folds0.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7424 - loss: 0.4595 - val_accuracy: 0.7498 - val_loss: 0.4487 - learning_rate: 0.0100\n",
      "Epoch 14/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7504 - loss: 0.4556 \n",
      "Epoch 14: val_loss did not improve from 0.44866\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7510 - loss: 0.4548 - val_accuracy: 0.7547 - val_loss: 0.4514 - learning_rate: 0.0100\n",
      "Epoch 15/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7389 - loss: 0.4589 \n",
      "Epoch 15: val_loss did not improve from 0.44866\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7412 - loss: 0.4585 - val_accuracy: 0.7498 - val_loss: 0.4537 - learning_rate: 0.0100\n",
      "Epoch 16/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7557 - loss: 0.4503 \n",
      "Epoch 16: val_loss improved from 0.44866 to 0.44601, saving model to folds0.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7555 - loss: 0.4508 - val_accuracy: 0.7566 - val_loss: 0.4460 - learning_rate: 0.0100\n",
      "Epoch 17/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7597 - loss: 0.4437 \n",
      "Epoch 17: val_loss improved from 0.44601 to 0.44537, saving model to folds0.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7581 - loss: 0.4445 - val_accuracy: 0.7535 - val_loss: 0.4454 - learning_rate: 0.0100\n",
      "Epoch 18/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7586 - loss: 0.4427 \n",
      "Epoch 18: val_loss improved from 0.44537 to 0.43983, saving model to folds0.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7583 - loss: 0.4430 - val_accuracy: 0.7645 - val_loss: 0.4398 - learning_rate: 0.0100\n",
      "Epoch 19/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7657 - loss: 0.4409 \n",
      "Epoch 19: val_loss did not improve from 0.43983\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7646 - loss: 0.4404 - val_accuracy: 0.7700 - val_loss: 0.4411 - learning_rate: 0.0100\n",
      "Epoch 20/300\n",
      "\u001b[1m12/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7595 - loss: 0.4469 \n",
      "Epoch 20: val_loss improved from 0.43983 to 0.43878, saving model to folds0.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7604 - loss: 0.4450 - val_accuracy: 0.7608 - val_loss: 0.4388 - learning_rate: 0.0100\n",
      "Epoch 21/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7606 - loss: 0.4381  \n",
      "Epoch 21: val_loss improved from 0.43878 to 0.43804, saving model to folds0.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7600 - loss: 0.4392 - val_accuracy: 0.7584 - val_loss: 0.4380 - learning_rate: 0.0100\n",
      "Epoch 22/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7594 - loss: 0.4367 \n",
      "Epoch 22: val_loss improved from 0.43804 to 0.43646, saving model to folds0.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7613 - loss: 0.4371 - val_accuracy: 0.7669 - val_loss: 0.4365 - learning_rate: 0.0100\n",
      "Epoch 23/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7697 - loss: 0.4344 \n",
      "Epoch 23: val_loss improved from 0.43646 to 0.43425, saving model to folds0.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7690 - loss: 0.4344 - val_accuracy: 0.7669 - val_loss: 0.4342 - learning_rate: 0.0100\n",
      "Epoch 24/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7728 - loss: 0.4279 \n",
      "Epoch 24: val_loss did not improve from 0.43425\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7690 - loss: 0.4291 - val_accuracy: 0.7492 - val_loss: 0.4383 - learning_rate: 0.0100\n",
      "Epoch 25/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7621 - loss: 0.4338 \n",
      "Epoch 25: val_loss improved from 0.43425 to 0.43172, saving model to folds0.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7635 - loss: 0.4325 - val_accuracy: 0.7663 - val_loss: 0.4317 - learning_rate: 0.0100\n",
      "Epoch 26/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7755 - loss: 0.4218 \n",
      "Epoch 26: val_loss improved from 0.43172 to 0.42756, saving model to folds0.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7736 - loss: 0.4237 - val_accuracy: 0.7651 - val_loss: 0.4276 - learning_rate: 0.0100\n",
      "Epoch 27/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7661 - loss: 0.4270 \n",
      "Epoch 27: val_loss did not improve from 0.42756\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7666 - loss: 0.4262 - val_accuracy: 0.7535 - val_loss: 0.4304 - learning_rate: 0.0100\n",
      "Epoch 28/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7635 - loss: 0.4308 \n",
      "Epoch 28: val_loss did not improve from 0.42756\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7650 - loss: 0.4287 - val_accuracy: 0.7608 - val_loss: 0.4329 - learning_rate: 0.0100\n",
      "Epoch 29/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7669 - loss: 0.4239 \n",
      "Epoch 29: val_loss improved from 0.42756 to 0.42463, saving model to folds0.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7673 - loss: 0.4235 - val_accuracy: 0.7675 - val_loss: 0.4246 - learning_rate: 0.0100\n",
      "Epoch 30/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7642 - loss: 0.4311 \n",
      "Epoch 30: val_loss did not improve from 0.42463\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7662 - loss: 0.4282 - val_accuracy: 0.7511 - val_loss: 0.4414 - learning_rate: 0.0100\n",
      "Epoch 31/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7715 - loss: 0.4185 \n",
      "Epoch 31: val_loss did not improve from 0.42463\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7708 - loss: 0.4200 - val_accuracy: 0.7688 - val_loss: 0.4310 - learning_rate: 0.0100\n",
      "Epoch 32/300\n",
      "\u001b[1m12/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7659 - loss: 0.4309 \n",
      "Epoch 32: val_loss did not improve from 0.42463\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7664 - loss: 0.4292 - val_accuracy: 0.7791 - val_loss: 0.4273 - learning_rate: 0.0100\n",
      "Epoch 33/300\n",
      "\u001b[1m12/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7735 - loss: 0.4204 \n",
      "Epoch 33: val_loss did not improve from 0.42463\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7731 - loss: 0.4205 - val_accuracy: 0.7779 - val_loss: 0.4251 - learning_rate: 0.0100\n",
      "Epoch 34/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7686 - loss: 0.4215 \n",
      "Epoch 34: val_loss did not improve from 0.42463\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7691 - loss: 0.4209 - val_accuracy: 0.7669 - val_loss: 0.4289 - learning_rate: 0.0100\n",
      "Epoch 35/300\n",
      "\u001b[1m12/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7717 - loss: 0.4228 \n",
      "Epoch 35: val_loss did not improve from 0.42463\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7725 - loss: 0.4211 - val_accuracy: 0.7602 - val_loss: 0.4259 - learning_rate: 0.0100\n",
      "Epoch 36/300\n",
      "\u001b[1m12/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7727 - loss: 0.4125 \n",
      "Epoch 36: val_loss improved from 0.42463 to 0.42324, saving model to folds0.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7732 - loss: 0.4128 - val_accuracy: 0.7688 - val_loss: 0.4232 - learning_rate: 0.0100\n",
      "Epoch 37/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7713 - loss: 0.4124 \n",
      "Epoch 37: val_loss did not improve from 0.42324\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7736 - loss: 0.4119 - val_accuracy: 0.7736 - val_loss: 0.4237 - learning_rate: 0.0100\n",
      "Epoch 38/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7712 - loss: 0.4149 \n",
      "Epoch 38: val_loss improved from 0.42324 to 0.41710, saving model to folds0.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7730 - loss: 0.4136 - val_accuracy: 0.7773 - val_loss: 0.4171 - learning_rate: 0.0100\n",
      "Epoch 39/300\n",
      "\u001b[1m12/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7766 - loss: 0.4072 \n",
      "Epoch 39: val_loss did not improve from 0.41710\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7755 - loss: 0.4078 - val_accuracy: 0.7633 - val_loss: 0.4190 - learning_rate: 0.0100\n",
      "Epoch 40/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7829 - loss: 0.3990 \n",
      "Epoch 40: val_loss improved from 0.41710 to 0.41587, saving model to folds0.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7813 - loss: 0.4012 - val_accuracy: 0.7682 - val_loss: 0.4159 - learning_rate: 0.0100\n",
      "Epoch 41/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7777 - loss: 0.4119 \n",
      "Epoch 41: val_loss did not improve from 0.41587\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7779 - loss: 0.4107 - val_accuracy: 0.7779 - val_loss: 0.4188 - learning_rate: 0.0100\n",
      "Epoch 42/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7765 - loss: 0.4040 \n",
      "Epoch 42: val_loss improved from 0.41587 to 0.41296, saving model to folds0.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7762 - loss: 0.4052 - val_accuracy: 0.7804 - val_loss: 0.4130 - learning_rate: 0.0100\n",
      "Epoch 43/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7728 - loss: 0.4122 \n",
      "Epoch 43: val_loss did not improve from 0.41296\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7732 - loss: 0.4123 - val_accuracy: 0.7627 - val_loss: 0.4240 - learning_rate: 0.0100\n",
      "Epoch 44/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7730 - loss: 0.4104  \n",
      "Epoch 44: val_loss did not improve from 0.41296\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7717 - loss: 0.4115 - val_accuracy: 0.7718 - val_loss: 0.4210 - learning_rate: 0.0100\n",
      "Epoch 45/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7798 - loss: 0.4083 \n",
      "Epoch 45: val_loss did not improve from 0.41296\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7793 - loss: 0.4081 - val_accuracy: 0.7718 - val_loss: 0.4138 - learning_rate: 0.0100\n",
      "Epoch 46/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7715 - loss: 0.4037 \n",
      "Epoch 46: val_loss improved from 0.41296 to 0.40662, saving model to folds0.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7736 - loss: 0.4025 - val_accuracy: 0.7804 - val_loss: 0.4066 - learning_rate: 0.0100\n",
      "Epoch 47/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7735 - loss: 0.4068 \n",
      "Epoch 47: val_loss did not improve from 0.40662\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7757 - loss: 0.4041 - val_accuracy: 0.7749 - val_loss: 0.4089 - learning_rate: 0.0100\n",
      "Epoch 48/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7824 - loss: 0.3934 \n",
      "Epoch 48: val_loss did not improve from 0.40662\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7806 - loss: 0.3954 - val_accuracy: 0.7773 - val_loss: 0.4086 - learning_rate: 0.0100\n",
      "Epoch 49/300\n",
      "\u001b[1m12/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7762 - loss: 0.4032 \n",
      "Epoch 49: val_loss did not improve from 0.40662\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7753 - loss: 0.4037 - val_accuracy: 0.7700 - val_loss: 0.4110 - learning_rate: 0.0100\n",
      "Epoch 50/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7725 - loss: 0.4024 \n",
      "Epoch 50: val_loss did not improve from 0.40662\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7728 - loss: 0.4036 - val_accuracy: 0.7791 - val_loss: 0.4095 - learning_rate: 0.0100\n",
      "Epoch 51/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7770 - loss: 0.4051 \n",
      "Epoch 51: val_loss did not improve from 0.40662\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7766 - loss: 0.4045 - val_accuracy: 0.7651 - val_loss: 0.4163 - learning_rate: 0.0100\n",
      "Epoch 52/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7659 - loss: 0.4090 \n",
      "Epoch 52: val_loss did not improve from 0.40662\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7684 - loss: 0.4080 - val_accuracy: 0.7584 - val_loss: 0.4200 - learning_rate: 0.0100\n",
      "Epoch 53/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7654 - loss: 0.4151 \n",
      "Epoch 53: val_loss did not improve from 0.40662\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7694 - loss: 0.4103 - val_accuracy: 0.7736 - val_loss: 0.4099 - learning_rate: 0.0100\n",
      "Epoch 54/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7711 - loss: 0.4058 \n",
      "Epoch 54: val_loss did not improve from 0.40662\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7716 - loss: 0.4053 - val_accuracy: 0.7822 - val_loss: 0.4104 - learning_rate: 0.0100\n",
      "Epoch 55/300\n",
      "\u001b[1m12/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7800 - loss: 0.4034 \n",
      "Epoch 55: val_loss did not improve from 0.40662\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7799 - loss: 0.4034 - val_accuracy: 0.7706 - val_loss: 0.4078 - learning_rate: 0.0100\n",
      "Epoch 56/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7807 - loss: 0.4008 \n",
      "Epoch 56: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
      "\n",
      "Epoch 56: val_loss did not improve from 0.40662\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7792 - loss: 0.4015 - val_accuracy: 0.7761 - val_loss: 0.4185 - learning_rate: 0.0100\n",
      "Epoch 57/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7759 - loss: 0.4095 \n",
      "Epoch 57: val_loss did not improve from 0.40662\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7774 - loss: 0.4068 - val_accuracy: 0.7852 - val_loss: 0.4081 - learning_rate: 0.0050\n",
      "Epoch 58/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7805 - loss: 0.3975 \n",
      "Epoch 58: val_loss improved from 0.40662 to 0.40562, saving model to folds0.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7811 - loss: 0.3974 - val_accuracy: 0.7852 - val_loss: 0.4056 - learning_rate: 0.0050\n",
      "Epoch 59/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7904 - loss: 0.3846 \n",
      "Epoch 59: val_loss improved from 0.40562 to 0.39826, saving model to folds0.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7873 - loss: 0.3883 - val_accuracy: 0.7785 - val_loss: 0.3983 - learning_rate: 0.0050\n",
      "Epoch 60/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7720 - loss: 0.3969 \n",
      "Epoch 60: val_loss did not improve from 0.39826\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7737 - loss: 0.3964 - val_accuracy: 0.7804 - val_loss: 0.4010 - learning_rate: 0.0050\n",
      "Epoch 61/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7800 - loss: 0.3929\n",
      "Epoch 61: val_loss did not improve from 0.39826\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.7801 - loss: 0.3930 - val_accuracy: 0.7755 - val_loss: 0.4007 - learning_rate: 0.0050\n",
      "Epoch 62/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7843 - loss: 0.3894\n",
      "Epoch 62: val_loss did not improve from 0.39826\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7840 - loss: 0.3899 - val_accuracy: 0.7755 - val_loss: 0.3996 - learning_rate: 0.0050\n",
      "Epoch 63/300\n",
      "\u001b[1m12/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7830 - loss: 0.3860\n",
      "Epoch 63: val_loss improved from 0.39826 to 0.39808, saving model to folds0.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.7825 - loss: 0.3876 - val_accuracy: 0.7871 - val_loss: 0.3981 - learning_rate: 0.0050\n",
      "Epoch 64/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7812 - loss: 0.3931\n",
      "Epoch 64: val_loss improved from 0.39808 to 0.39454, saving model to folds0.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.7813 - loss: 0.3927 - val_accuracy: 0.7919 - val_loss: 0.3945 - learning_rate: 0.0050\n",
      "Epoch 65/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7874 - loss: 0.3932\n",
      "Epoch 65: val_loss improved from 0.39454 to 0.39453, saving model to folds0.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.7872 - loss: 0.3930 - val_accuracy: 0.7883 - val_loss: 0.3945 - learning_rate: 0.0050\n",
      "Epoch 66/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7830 - loss: 0.3919\n",
      "Epoch 66: val_loss did not improve from 0.39453\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.7830 - loss: 0.3917 - val_accuracy: 0.7767 - val_loss: 0.3949 - learning_rate: 0.0050\n",
      "Epoch 67/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7866 - loss: 0.3877\n",
      "Epoch 67: val_loss improved from 0.39453 to 0.39327, saving model to folds0.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.7866 - loss: 0.3876 - val_accuracy: 0.7816 - val_loss: 0.3933 - learning_rate: 0.0050\n",
      "Epoch 68/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7881 - loss: 0.3900 \n",
      "Epoch 68: val_loss improved from 0.39327 to 0.39274, saving model to folds0.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.7872 - loss: 0.3891 - val_accuracy: 0.7858 - val_loss: 0.3927 - learning_rate: 0.0050\n",
      "Epoch 69/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7838 - loss: 0.3894\n",
      "Epoch 69: val_loss improved from 0.39274 to 0.39088, saving model to folds0.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.7840 - loss: 0.3890 - val_accuracy: 0.7846 - val_loss: 0.3909 - learning_rate: 0.0050\n",
      "Epoch 70/300\n",
      "\u001b[1m13/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7859 - loss: 0.3887\n",
      "Epoch 70: val_loss did not improve from 0.39088\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7859 - loss: 0.3881 - val_accuracy: 0.7810 - val_loss: 0.3929 - learning_rate: 0.0050\n",
      "Epoch 71/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7796 - loss: 0.3842\n",
      "Epoch 71: val_loss did not improve from 0.39088\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7800 - loss: 0.3844 - val_accuracy: 0.7743 - val_loss: 0.3950 - learning_rate: 0.0050\n",
      "Epoch 72/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7804 - loss: 0.3886\n",
      "Epoch 72: val_loss did not improve from 0.39088\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7808 - loss: 0.3883 - val_accuracy: 0.7852 - val_loss: 0.3917 - learning_rate: 0.0050\n",
      "Epoch 73/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7886 - loss: 0.3835 \n",
      "Epoch 73: val_loss did not improve from 0.39088\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7879 - loss: 0.3841 - val_accuracy: 0.7816 - val_loss: 0.3919 - learning_rate: 0.0050\n",
      "Epoch 74/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7805 - loss: 0.3907  \n",
      "Epoch 74: val_loss did not improve from 0.39088\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7816 - loss: 0.3901 - val_accuracy: 0.7883 - val_loss: 0.3953 - learning_rate: 0.0050\n",
      "Epoch 75/300\n",
      "\u001b[1m12/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7804 - loss: 0.3857 \n",
      "Epoch 75: val_loss did not improve from 0.39088\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7812 - loss: 0.3853 - val_accuracy: 0.7712 - val_loss: 0.3920 - learning_rate: 0.0050\n",
      "Epoch 76/300\n",
      "\u001b[1m12/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7873 - loss: 0.3802 \n",
      "Epoch 76: val_loss improved from 0.39088 to 0.39073, saving model to folds0.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7874 - loss: 0.3809 - val_accuracy: 0.7852 - val_loss: 0.3907 - learning_rate: 0.0050\n",
      "Epoch 77/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7850 - loss: 0.3847 \n",
      "Epoch 77: val_loss improved from 0.39073 to 0.38952, saving model to folds0.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7852 - loss: 0.3843 - val_accuracy: 0.7816 - val_loss: 0.3895 - learning_rate: 0.0050\n",
      "Epoch 78/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7952 - loss: 0.3715  \n",
      "Epoch 78: val_loss did not improve from 0.38952\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7926 - loss: 0.3745 - val_accuracy: 0.7865 - val_loss: 0.3898 - learning_rate: 0.0050\n",
      "Epoch 79/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7911 - loss: 0.3827 \n",
      "Epoch 79: val_loss did not improve from 0.38952\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7897 - loss: 0.3828 - val_accuracy: 0.7871 - val_loss: 0.3906 - learning_rate: 0.0050\n",
      "Epoch 80/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7889 - loss: 0.3832  \n",
      "Epoch 80: val_loss improved from 0.38952 to 0.38911, saving model to folds0.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7880 - loss: 0.3839 - val_accuracy: 0.7871 - val_loss: 0.3891 - learning_rate: 0.0050\n",
      "Epoch 81/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7856 - loss: 0.3849 \n",
      "Epoch 81: val_loss did not improve from 0.38911\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7856 - loss: 0.3845 - val_accuracy: 0.7865 - val_loss: 0.3951 - learning_rate: 0.0050\n",
      "Epoch 82/300\n",
      "\u001b[1m12/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7822 - loss: 0.3781 \n",
      "Epoch 82: val_loss did not improve from 0.38911\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7834 - loss: 0.3790 - val_accuracy: 0.7828 - val_loss: 0.3947 - learning_rate: 0.0050\n",
      "Epoch 83/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7903 - loss: 0.3781 \n",
      "Epoch 83: val_loss did not improve from 0.38911\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7884 - loss: 0.3791 - val_accuracy: 0.7822 - val_loss: 0.3987 - learning_rate: 0.0050\n",
      "Epoch 84/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7831 - loss: 0.3889 \n",
      "Epoch 84: val_loss did not improve from 0.38911\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7838 - loss: 0.3869 - val_accuracy: 0.7865 - val_loss: 0.3930 - learning_rate: 0.0050\n",
      "Epoch 85/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7892 - loss: 0.3811 \n",
      "Epoch 85: val_loss did not improve from 0.38911\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7897 - loss: 0.3809 - val_accuracy: 0.7810 - val_loss: 0.3911 - learning_rate: 0.0050\n",
      "Epoch 86/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8029 - loss: 0.3707 \n",
      "Epoch 86: val_loss did not improve from 0.38911\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7990 - loss: 0.3730 - val_accuracy: 0.7724 - val_loss: 0.3922 - learning_rate: 0.0050\n",
      "Epoch 87/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7862 - loss: 0.3798 \n",
      "Epoch 87: val_loss did not improve from 0.38911\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7877 - loss: 0.3794 - val_accuracy: 0.7791 - val_loss: 0.3960 - learning_rate: 0.0050\n",
      "Epoch 88/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7951 - loss: 0.3732 \n",
      "Epoch 88: val_loss did not improve from 0.38911\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7931 - loss: 0.3751 - val_accuracy: 0.7767 - val_loss: 0.3918 - learning_rate: 0.0050\n",
      "Epoch 89/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7951 - loss: 0.3712 \n",
      "Epoch 89: val_loss did not improve from 0.38911\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7933 - loss: 0.3726 - val_accuracy: 0.7834 - val_loss: 0.3929 - learning_rate: 0.0050\n",
      "Epoch 90/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7873 - loss: 0.3800 \n",
      "Epoch 90: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "\n",
      "Epoch 90: val_loss did not improve from 0.38911\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7862 - loss: 0.3793 - val_accuracy: 0.7858 - val_loss: 0.3955 - learning_rate: 0.0050\n",
      "Epoch 91/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7899 - loss: 0.3765 \n",
      "Epoch 91: val_loss did not improve from 0.38911\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7902 - loss: 0.3761 - val_accuracy: 0.7822 - val_loss: 0.3931 - learning_rate: 0.0025\n",
      "Epoch 92/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7864 - loss: 0.3808 \n",
      "Epoch 92: val_loss improved from 0.38911 to 0.38868, saving model to folds0.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7876 - loss: 0.3782 - val_accuracy: 0.7791 - val_loss: 0.3887 - learning_rate: 0.0025\n",
      "Epoch 93/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7902 - loss: 0.3696 \n",
      "Epoch 93: val_loss did not improve from 0.38868\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7899 - loss: 0.3708 - val_accuracy: 0.7797 - val_loss: 0.3898 - learning_rate: 0.0025\n",
      "Epoch 94/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7971 - loss: 0.3596 \n",
      "Epoch 94: val_loss did not improve from 0.38868\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7946 - loss: 0.3636 - val_accuracy: 0.7834 - val_loss: 0.3887 - learning_rate: 0.0025\n",
      "Epoch 95/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7870 - loss: 0.3756  \n",
      "Epoch 95: val_loss did not improve from 0.38868\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7887 - loss: 0.3741 - val_accuracy: 0.7846 - val_loss: 0.3890 - learning_rate: 0.0025\n",
      "Epoch 96/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7836 - loss: 0.3784 \n",
      "Epoch 96: val_loss improved from 0.38868 to 0.38622, saving model to folds0.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7859 - loss: 0.3761 - val_accuracy: 0.7834 - val_loss: 0.3862 - learning_rate: 0.0025\n",
      "Epoch 97/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7967 - loss: 0.3673 \n",
      "Epoch 97: val_loss did not improve from 0.38622\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7958 - loss: 0.3685 - val_accuracy: 0.7840 - val_loss: 0.3915 - learning_rate: 0.0025\n",
      "Epoch 98/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7869 - loss: 0.3768 \n",
      "Epoch 98: val_loss did not improve from 0.38622\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7887 - loss: 0.3753 - val_accuracy: 0.7865 - val_loss: 0.3922 - learning_rate: 0.0025\n",
      "Epoch 99/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8005 - loss: 0.3618  \n",
      "Epoch 99: val_loss did not improve from 0.38622\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7979 - loss: 0.3654 - val_accuracy: 0.7877 - val_loss: 0.3871 - learning_rate: 0.0025\n",
      "Epoch 100/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7894 - loss: 0.3688 \n",
      "Epoch 100: val_loss did not improve from 0.38622\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7902 - loss: 0.3700 - val_accuracy: 0.7785 - val_loss: 0.3877 - learning_rate: 0.0025\n",
      "Epoch 101/300\n",
      "\u001b[1m 7/15\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7924 - loss: 0.3709 \n",
      "Epoch 101: val_loss did not improve from 0.38622\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7930 - loss: 0.3719 - val_accuracy: 0.7865 - val_loss: 0.3878 - learning_rate: 0.0025\n",
      "Epoch 102/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7874 - loss: 0.3777 \n",
      "Epoch 102: val_loss improved from 0.38622 to 0.38555, saving model to folds0.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7890 - loss: 0.3760 - val_accuracy: 0.7840 - val_loss: 0.3855 - learning_rate: 0.0025\n",
      "Epoch 103/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7877 - loss: 0.3688 \n",
      "Epoch 103: val_loss did not improve from 0.38555\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7878 - loss: 0.3693 - val_accuracy: 0.7816 - val_loss: 0.3909 - learning_rate: 0.0025\n",
      "Epoch 104/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7936 - loss: 0.3767 \n",
      "Epoch 104: val_loss did not improve from 0.38555\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7941 - loss: 0.3752 - val_accuracy: 0.7804 - val_loss: 0.3879 - learning_rate: 0.0025\n",
      "Epoch 105/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7958 - loss: 0.3663 \n",
      "Epoch 105: val_loss improved from 0.38555 to 0.38511, saving model to folds0.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7958 - loss: 0.3674 - val_accuracy: 0.7810 - val_loss: 0.3851 - learning_rate: 0.0025\n",
      "Epoch 106/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7967 - loss: 0.3728 \n",
      "Epoch 106: val_loss did not improve from 0.38511\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7961 - loss: 0.3721 - val_accuracy: 0.7846 - val_loss: 0.3864 - learning_rate: 0.0025\n",
      "Epoch 107/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7930 - loss: 0.3663 \n",
      "Epoch 107: val_loss did not improve from 0.38511\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7936 - loss: 0.3673 - val_accuracy: 0.7913 - val_loss: 0.3851 - learning_rate: 0.0025\n",
      "Epoch 108/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8001 - loss: 0.3649 \n",
      "Epoch 108: val_loss did not improve from 0.38511\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7989 - loss: 0.3662 - val_accuracy: 0.7919 - val_loss: 0.3872 - learning_rate: 0.0025\n",
      "Epoch 109/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7976 - loss: 0.3699 \n",
      "Epoch 109: val_loss did not improve from 0.38511\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7981 - loss: 0.3694 - val_accuracy: 0.7974 - val_loss: 0.3854 - learning_rate: 0.0025\n",
      "Epoch 110/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8066 - loss: 0.3649  \n",
      "Epoch 110: val_loss did not improve from 0.38511\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8042 - loss: 0.3661 - val_accuracy: 0.7858 - val_loss: 0.3869 - learning_rate: 0.0025\n",
      "Epoch 111/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7973 - loss: 0.3721 \n",
      "Epoch 111: val_loss did not improve from 0.38511\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7967 - loss: 0.3711 - val_accuracy: 0.7883 - val_loss: 0.3866 - learning_rate: 0.0025\n",
      "Epoch 112/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7978 - loss: 0.3658 \n",
      "Epoch 112: val_loss did not improve from 0.38511\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7984 - loss: 0.3663 - val_accuracy: 0.7858 - val_loss: 0.3864 - learning_rate: 0.0025\n",
      "Epoch 113/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7996 - loss: 0.3689 \n",
      "Epoch 113: val_loss improved from 0.38511 to 0.38274, saving model to folds0.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7997 - loss: 0.3687 - val_accuracy: 0.7974 - val_loss: 0.3827 - learning_rate: 0.0025\n",
      "Epoch 114/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7950 - loss: 0.3690 \n",
      "Epoch 114: val_loss did not improve from 0.38274\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7959 - loss: 0.3683 - val_accuracy: 0.7974 - val_loss: 0.3833 - learning_rate: 0.0025\n",
      "Epoch 115/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8066 - loss: 0.3631 \n",
      "Epoch 115: val_loss did not improve from 0.38274\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8042 - loss: 0.3642 - val_accuracy: 0.7926 - val_loss: 0.3864 - learning_rate: 0.0025\n",
      "Epoch 116/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8047 - loss: 0.3633 \n",
      "Epoch 116: val_loss did not improve from 0.38274\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8028 - loss: 0.3646 - val_accuracy: 0.7919 - val_loss: 0.3843 - learning_rate: 0.0025\n",
      "Epoch 117/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7905 - loss: 0.3694 \n",
      "Epoch 117: val_loss improved from 0.38274 to 0.38226, saving model to folds0.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7907 - loss: 0.3694 - val_accuracy: 0.7956 - val_loss: 0.3823 - learning_rate: 0.0025\n",
      "Epoch 118/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8006 - loss: 0.3665  \n",
      "Epoch 118: val_loss did not improve from 0.38226\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8001 - loss: 0.3665 - val_accuracy: 0.7846 - val_loss: 0.3860 - learning_rate: 0.0025\n",
      "Epoch 119/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8037 - loss: 0.3618 \n",
      "Epoch 119: val_loss did not improve from 0.38226\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8016 - loss: 0.3634 - val_accuracy: 0.7865 - val_loss: 0.3830 - learning_rate: 0.0025\n",
      "Epoch 120/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7923 - loss: 0.3703 \n",
      "Epoch 120: val_loss did not improve from 0.38226\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7946 - loss: 0.3692 - val_accuracy: 0.7791 - val_loss: 0.3844 - learning_rate: 0.0025\n",
      "Epoch 121/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7963 - loss: 0.3693 \n",
      "Epoch 121: val_loss improved from 0.38226 to 0.38135, saving model to folds0.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7975 - loss: 0.3682 - val_accuracy: 0.7999 - val_loss: 0.3814 - learning_rate: 0.0025\n",
      "Epoch 122/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8034 - loss: 0.3645 \n",
      "Epoch 122: val_loss improved from 0.38135 to 0.38087, saving model to folds0.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8033 - loss: 0.3648 - val_accuracy: 0.8035 - val_loss: 0.3809 - learning_rate: 0.0025\n",
      "Epoch 123/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8024 - loss: 0.3650 \n",
      "Epoch 123: val_loss did not improve from 0.38087\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8015 - loss: 0.3655 - val_accuracy: 0.7901 - val_loss: 0.3858 - learning_rate: 0.0025\n",
      "Epoch 124/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7970 - loss: 0.3739 \n",
      "Epoch 124: val_loss improved from 0.38087 to 0.38066, saving model to folds0.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7981 - loss: 0.3710 - val_accuracy: 0.7858 - val_loss: 0.3807 - learning_rate: 0.0025\n",
      "Epoch 125/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8036 - loss: 0.3645\n",
      "Epoch 125: val_loss improved from 0.38066 to 0.37912, saving model to folds0.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.8035 - loss: 0.3646 - val_accuracy: 0.7944 - val_loss: 0.3791 - learning_rate: 0.0025\n",
      "Epoch 126/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7966 - loss: 0.3652\n",
      "Epoch 126: val_loss did not improve from 0.37912\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7967 - loss: 0.3652 - val_accuracy: 0.7932 - val_loss: 0.3822 - learning_rate: 0.0025\n",
      "Epoch 127/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8024 - loss: 0.3603\n",
      "Epoch 127: val_loss did not improve from 0.37912\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8024 - loss: 0.3606 - val_accuracy: 0.7944 - val_loss: 0.3828 - learning_rate: 0.0025\n",
      "Epoch 128/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8036 - loss: 0.3602 \n",
      "Epoch 128: val_loss did not improve from 0.37912\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8028 - loss: 0.3619 - val_accuracy: 0.7895 - val_loss: 0.3802 - learning_rate: 0.0025\n",
      "Epoch 129/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8043 - loss: 0.3613\n",
      "Epoch 129: val_loss did not improve from 0.37912\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8042 - loss: 0.3616 - val_accuracy: 0.8005 - val_loss: 0.3810 - learning_rate: 0.0025\n",
      "Epoch 130/300\n",
      "\u001b[1m13/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8015 - loss: 0.3671\n",
      "Epoch 130: val_loss improved from 0.37912 to 0.37737, saving model to folds0.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8019 - loss: 0.3665 - val_accuracy: 0.7938 - val_loss: 0.3774 - learning_rate: 0.0025\n",
      "Epoch 131/300\n",
      "\u001b[1m12/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8041 - loss: 0.3655\n",
      "Epoch 131: val_loss did not improve from 0.37737\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8041 - loss: 0.3651 - val_accuracy: 0.7980 - val_loss: 0.3797 - learning_rate: 0.0025\n",
      "Epoch 132/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7999 - loss: 0.3624\n",
      "Epoch 132: val_loss did not improve from 0.37737\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8000 - loss: 0.3625 - val_accuracy: 0.7913 - val_loss: 0.3881 - learning_rate: 0.0025\n",
      "Epoch 133/300\n",
      "\u001b[1m13/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7973 - loss: 0.3717\n",
      "Epoch 133: val_loss did not improve from 0.37737\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7981 - loss: 0.3705 - val_accuracy: 0.7877 - val_loss: 0.3831 - learning_rate: 0.0025\n",
      "Epoch 134/300\n",
      "\u001b[1m13/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8009 - loss: 0.3630\n",
      "Epoch 134: val_loss did not improve from 0.37737\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8004 - loss: 0.3635 - val_accuracy: 0.7956 - val_loss: 0.3786 - learning_rate: 0.0025\n",
      "Epoch 135/300\n",
      "\u001b[1m12/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8073 - loss: 0.3626\n",
      "Epoch 135: val_loss did not improve from 0.37737\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8064 - loss: 0.3629 - val_accuracy: 0.7980 - val_loss: 0.3779 - learning_rate: 0.0025\n",
      "Epoch 136/300\n",
      "\u001b[1m13/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8103 - loss: 0.3518\n",
      "Epoch 136: val_loss did not improve from 0.37737\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8092 - loss: 0.3537 - val_accuracy: 0.7956 - val_loss: 0.3832 - learning_rate: 0.0025\n",
      "Epoch 137/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7979 - loss: 0.3654 \n",
      "Epoch 137: val_loss did not improve from 0.37737\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7995 - loss: 0.3643 - val_accuracy: 0.7889 - val_loss: 0.3811 - learning_rate: 0.0025\n",
      "Epoch 138/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8061 - loss: 0.3603 \n",
      "Epoch 138: val_loss did not improve from 0.37737\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8062 - loss: 0.3605 - val_accuracy: 0.7889 - val_loss: 0.3795 - learning_rate: 0.0025\n",
      "Epoch 139/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8103 - loss: 0.3511 \n",
      "Epoch 139: val_loss did not improve from 0.37737\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8087 - loss: 0.3546 - val_accuracy: 0.7865 - val_loss: 0.3780 - learning_rate: 0.0025\n",
      "Epoch 140/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8067 - loss: 0.3591 \n",
      "Epoch 140: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
      "\n",
      "Epoch 140: val_loss did not improve from 0.37737\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8057 - loss: 0.3598 - val_accuracy: 0.7871 - val_loss: 0.3792 - learning_rate: 0.0025\n",
      "Epoch 141/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8024 - loss: 0.3583 \n",
      "Epoch 141: val_loss improved from 0.37737 to 0.37574, saving model to folds0.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8032 - loss: 0.3587 - val_accuracy: 0.7993 - val_loss: 0.3757 - learning_rate: 0.0012\n",
      "Epoch 142/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8039 - loss: 0.3632 \n",
      "Epoch 142: val_loss did not improve from 0.37574\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8046 - loss: 0.3618 - val_accuracy: 0.7950 - val_loss: 0.3770 - learning_rate: 0.0012\n",
      "Epoch 143/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8091 - loss: 0.3537 \n",
      "Epoch 143: val_loss did not improve from 0.37574\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8086 - loss: 0.3549 - val_accuracy: 0.7962 - val_loss: 0.3770 - learning_rate: 0.0012\n",
      "Epoch 144/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8138 - loss: 0.3513 \n",
      "Epoch 144: val_loss did not improve from 0.37574\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8124 - loss: 0.3529 - val_accuracy: 0.7932 - val_loss: 0.3779 - learning_rate: 0.0012\n",
      "Epoch 145/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8013 - loss: 0.3620 \n",
      "Epoch 145: val_loss did not improve from 0.37574\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8038 - loss: 0.3602 - val_accuracy: 0.7932 - val_loss: 0.3780 - learning_rate: 0.0012\n",
      "Epoch 146/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8144 - loss: 0.3559 \n",
      "Epoch 146: val_loss did not improve from 0.37574\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8121 - loss: 0.3565 - val_accuracy: 0.7987 - val_loss: 0.3759 - learning_rate: 0.0012\n",
      "Epoch 147/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8039 - loss: 0.3634 \n",
      "Epoch 147: val_loss improved from 0.37574 to 0.37453, saving model to folds0.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8053 - loss: 0.3611 - val_accuracy: 0.7993 - val_loss: 0.3745 - learning_rate: 0.0012\n",
      "Epoch 148/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8077 - loss: 0.3498  \n",
      "Epoch 148: val_loss improved from 0.37453 to 0.37335, saving model to folds0.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8079 - loss: 0.3525 - val_accuracy: 0.8017 - val_loss: 0.3734 - learning_rate: 0.0012\n",
      "Epoch 149/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8021 - loss: 0.3600 \n",
      "Epoch 149: val_loss did not improve from 0.37335\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8051 - loss: 0.3589 - val_accuracy: 0.7999 - val_loss: 0.3741 - learning_rate: 0.0012\n",
      "Epoch 150/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8121 - loss: 0.3533 \n",
      "Epoch 150: val_loss did not improve from 0.37335\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8120 - loss: 0.3541 - val_accuracy: 0.8035 - val_loss: 0.3739 - learning_rate: 0.0012\n",
      "Epoch 151/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8140 - loss: 0.3566 \n",
      "Epoch 151: val_loss improved from 0.37335 to 0.37326, saving model to folds0.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8127 - loss: 0.3562 - val_accuracy: 0.7987 - val_loss: 0.3733 - learning_rate: 0.0012\n",
      "Epoch 152/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8145 - loss: 0.3475 \n",
      "Epoch 152: val_loss improved from 0.37326 to 0.37224, saving model to folds0.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8140 - loss: 0.3502 - val_accuracy: 0.8041 - val_loss: 0.3722 - learning_rate: 0.0012\n",
      "Epoch 153/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8114 - loss: 0.3546 \n",
      "Epoch 153: val_loss did not improve from 0.37224\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8101 - loss: 0.3550 - val_accuracy: 0.7987 - val_loss: 0.3723 - learning_rate: 0.0012\n",
      "Epoch 154/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8124 - loss: 0.3560 \n",
      "Epoch 154: val_loss did not improve from 0.37224\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8131 - loss: 0.3553 - val_accuracy: 0.7938 - val_loss: 0.3734 - learning_rate: 0.0012\n",
      "Epoch 155/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8098 - loss: 0.3540 \n",
      "Epoch 155: val_loss did not improve from 0.37224\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8103 - loss: 0.3548 - val_accuracy: 0.8023 - val_loss: 0.3724 - learning_rate: 0.0012\n",
      "Epoch 156/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8171 - loss: 0.3455 \n",
      "Epoch 156: val_loss did not improve from 0.37224\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8156 - loss: 0.3487 - val_accuracy: 0.8041 - val_loss: 0.3746 - learning_rate: 0.0012\n",
      "Epoch 157/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8161 - loss: 0.3471  \n",
      "Epoch 157: val_loss improved from 0.37224 to 0.37164, saving model to folds0.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8140 - loss: 0.3501 - val_accuracy: 0.7987 - val_loss: 0.3716 - learning_rate: 0.0012\n",
      "Epoch 158/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8139 - loss: 0.3474 \n",
      "Epoch 158: val_loss did not improve from 0.37164\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8135 - loss: 0.3497 - val_accuracy: 0.8035 - val_loss: 0.3730 - learning_rate: 0.0012\n",
      "Epoch 159/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8122 - loss: 0.3560 \n",
      "Epoch 159: val_loss improved from 0.37164 to 0.37143, saving model to folds0.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8120 - loss: 0.3558 - val_accuracy: 0.7980 - val_loss: 0.3714 - learning_rate: 0.0012\n",
      "Epoch 160/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8082 - loss: 0.3554 \n",
      "Epoch 160: val_loss did not improve from 0.37143\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8093 - loss: 0.3549 - val_accuracy: 0.7999 - val_loss: 0.3721 - learning_rate: 0.0012\n",
      "Epoch 161/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8103 - loss: 0.3538 \n",
      "Epoch 161: val_loss improved from 0.37143 to 0.36929, saving model to folds0.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8112 - loss: 0.3533 - val_accuracy: 0.7999 - val_loss: 0.3693 - learning_rate: 0.0012\n",
      "Epoch 162/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8113 - loss: 0.3535 \n",
      "Epoch 162: val_loss did not improve from 0.36929\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8114 - loss: 0.3536 - val_accuracy: 0.8029 - val_loss: 0.3695 - learning_rate: 0.0012\n",
      "Epoch 163/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8111 - loss: 0.3558 \n",
      "Epoch 163: val_loss did not improve from 0.36929\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8129 - loss: 0.3551 - val_accuracy: 0.7926 - val_loss: 0.3714 - learning_rate: 0.0012\n",
      "Epoch 164/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8049 - loss: 0.3607 \n",
      "Epoch 164: val_loss improved from 0.36929 to 0.36914, saving model to folds0.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8070 - loss: 0.3581 - val_accuracy: 0.8029 - val_loss: 0.3691 - learning_rate: 0.0012\n",
      "Epoch 165/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8135 - loss: 0.3528 \n",
      "Epoch 165: val_loss did not improve from 0.36914\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8131 - loss: 0.3527 - val_accuracy: 0.7993 - val_loss: 0.3696 - learning_rate: 0.0012\n",
      "Epoch 166/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8059 - loss: 0.3570 \n",
      "Epoch 166: val_loss did not improve from 0.36914\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8079 - loss: 0.3556 - val_accuracy: 0.8011 - val_loss: 0.3698 - learning_rate: 0.0012\n",
      "Epoch 167/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8095 - loss: 0.3547 \n",
      "Epoch 167: val_loss improved from 0.36914 to 0.36895, saving model to folds0.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8108 - loss: 0.3534 - val_accuracy: 0.7987 - val_loss: 0.3689 - learning_rate: 0.0012\n",
      "Epoch 168/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8146 - loss: 0.3494 \n",
      "Epoch 168: val_loss improved from 0.36895 to 0.36882, saving model to folds0.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8140 - loss: 0.3503 - val_accuracy: 0.8048 - val_loss: 0.3688 - learning_rate: 0.0012\n",
      "Epoch 169/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8170 - loss: 0.3494 \n",
      "Epoch 169: val_loss improved from 0.36882 to 0.36729, saving model to folds0.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8151 - loss: 0.3505 - val_accuracy: 0.8017 - val_loss: 0.3673 - learning_rate: 0.0012\n",
      "Epoch 170/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8103 - loss: 0.3554 \n",
      "Epoch 170: val_loss improved from 0.36729 to 0.36689, saving model to folds0.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8110 - loss: 0.3540 - val_accuracy: 0.8084 - val_loss: 0.3669 - learning_rate: 0.0012\n",
      "Epoch 171/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8185 - loss: 0.3448 \n",
      "Epoch 171: val_loss did not improve from 0.36689\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8166 - loss: 0.3471 - val_accuracy: 0.8090 - val_loss: 0.3679 - learning_rate: 0.0012\n",
      "Epoch 172/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8126 - loss: 0.3483  \n",
      "Epoch 172: val_loss did not improve from 0.36689\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8126 - loss: 0.3493 - val_accuracy: 0.8035 - val_loss: 0.3682 - learning_rate: 0.0012\n",
      "Epoch 173/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8136 - loss: 0.3563 \n",
      "Epoch 173: val_loss did not improve from 0.36689\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8139 - loss: 0.3547 - val_accuracy: 0.8005 - val_loss: 0.3684 - learning_rate: 0.0012\n",
      "Epoch 174/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8152 - loss: 0.3488 \n",
      "Epoch 174: val_loss did not improve from 0.36689\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8141 - loss: 0.3491 - val_accuracy: 0.7974 - val_loss: 0.3679 - learning_rate: 0.0012\n",
      "Epoch 175/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8150 - loss: 0.3518 \n",
      "Epoch 175: val_loss did not improve from 0.36689\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8149 - loss: 0.3514 - val_accuracy: 0.8005 - val_loss: 0.3673 - learning_rate: 0.0012\n",
      "Epoch 176/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8201 - loss: 0.3413 \n",
      "Epoch 176: val_loss improved from 0.36689 to 0.36592, saving model to folds0.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8184 - loss: 0.3438 - val_accuracy: 0.8017 - val_loss: 0.3659 - learning_rate: 0.0012\n",
      "Epoch 177/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8166 - loss: 0.3471\n",
      "Epoch 177: val_loss did not improve from 0.36592\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8163 - loss: 0.3473 - val_accuracy: 0.8005 - val_loss: 0.3706 - learning_rate: 0.0012\n",
      "Epoch 178/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8112 - loss: 0.3516 \n",
      "Epoch 178: val_loss did not improve from 0.36592\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8117 - loss: 0.3512 - val_accuracy: 0.7950 - val_loss: 0.3679 - learning_rate: 0.0012\n",
      "Epoch 179/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8068 - loss: 0.3491 \n",
      "Epoch 179: val_loss did not improve from 0.36592\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8089 - loss: 0.3488 - val_accuracy: 0.8054 - val_loss: 0.3674 - learning_rate: 0.0012\n",
      "Epoch 180/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8185 - loss: 0.3446 \n",
      "Epoch 180: val_loss did not improve from 0.36592\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8169 - loss: 0.3458 - val_accuracy: 0.8078 - val_loss: 0.3660 - learning_rate: 0.0012\n",
      "Epoch 181/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8110 - loss: 0.3536 \n",
      "Epoch 181: val_loss did not improve from 0.36592\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8132 - loss: 0.3512 - val_accuracy: 0.8054 - val_loss: 0.3663 - learning_rate: 0.0012\n",
      "Epoch 182/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8172 - loss: 0.3443 \n",
      "Epoch 182: val_loss did not improve from 0.36592\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8159 - loss: 0.3460 - val_accuracy: 0.7968 - val_loss: 0.3674 - learning_rate: 0.0012\n",
      "Epoch 183/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8084 - loss: 0.3511 \n",
      "Epoch 183: val_loss did not improve from 0.36592\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8101 - loss: 0.3506 - val_accuracy: 0.7950 - val_loss: 0.3677 - learning_rate: 0.0012\n",
      "Epoch 184/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8112 - loss: 0.3514 \n",
      "Epoch 184: val_loss did not improve from 0.36592\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8124 - loss: 0.3502 - val_accuracy: 0.8048 - val_loss: 0.3662 - learning_rate: 0.0012\n",
      "Epoch 185/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8094 - loss: 0.3504 \n",
      "Epoch 185: val_loss did not improve from 0.36592\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8101 - loss: 0.3496 - val_accuracy: 0.7956 - val_loss: 0.3687 - learning_rate: 0.0012\n",
      "Epoch 186/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8127 - loss: 0.3496 \n",
      "Epoch 186: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "\n",
      "Epoch 186: val_loss improved from 0.36592 to 0.36583, saving model to folds0.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8126 - loss: 0.3491 - val_accuracy: 0.7999 - val_loss: 0.3658 - learning_rate: 0.0012\n",
      "Epoch 187/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8086 - loss: 0.3539 \n",
      "Epoch 187: val_loss improved from 0.36583 to 0.36360, saving model to folds0.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8106 - loss: 0.3511 - val_accuracy: 0.8011 - val_loss: 0.3636 - learning_rate: 6.2500e-04\n",
      "Epoch 188/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8160 - loss: 0.3409 \n",
      "Epoch 188: val_loss did not improve from 0.36360\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8150 - loss: 0.3431 - val_accuracy: 0.8041 - val_loss: 0.3639 - learning_rate: 6.2500e-04\n",
      "Epoch 189/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8188 - loss: 0.3473 \n",
      "Epoch 189: val_loss improved from 0.36360 to 0.36342, saving model to folds0.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8180 - loss: 0.3467 - val_accuracy: 0.7999 - val_loss: 0.3634 - learning_rate: 6.2500e-04\n",
      "Epoch 190/300\n",
      "\u001b[1m13/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8208 - loss: 0.3427\n",
      "Epoch 190: val_loss improved from 0.36342 to 0.36339, saving model to folds0.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8197 - loss: 0.3434 - val_accuracy: 0.7987 - val_loss: 0.3634 - learning_rate: 6.2500e-04\n",
      "Epoch 191/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8179 - loss: 0.3429\n",
      "Epoch 191: val_loss did not improve from 0.36339\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8177 - loss: 0.3433 - val_accuracy: 0.8005 - val_loss: 0.3636 - learning_rate: 6.2500e-04\n",
      "Epoch 192/300\n",
      "\u001b[1m12/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8211 - loss: 0.3396\n",
      "Epoch 192: val_loss did not improve from 0.36339\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8194 - loss: 0.3412 - val_accuracy: 0.8011 - val_loss: 0.3636 - learning_rate: 6.2500e-04\n",
      "Epoch 193/300\n",
      "\u001b[1m13/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8135 - loss: 0.3409\n",
      "Epoch 193: val_loss did not improve from 0.36339\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8137 - loss: 0.3418 - val_accuracy: 0.7999 - val_loss: 0.3636 - learning_rate: 6.2500e-04\n",
      "Epoch 194/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8161 - loss: 0.3427\n",
      "Epoch 194: val_loss improved from 0.36339 to 0.36309, saving model to folds0.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8160 - loss: 0.3428 - val_accuracy: 0.7987 - val_loss: 0.3631 - learning_rate: 6.2500e-04\n",
      "Epoch 195/300\n",
      "\u001b[1m13/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8105 - loss: 0.3487\n",
      "Epoch 195: val_loss improved from 0.36309 to 0.36298, saving model to folds0.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.8112 - loss: 0.3482 - val_accuracy: 0.8029 - val_loss: 0.3630 - learning_rate: 6.2500e-04\n",
      "Epoch 196/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8158 - loss: 0.3439\n",
      "Epoch 196: val_loss improved from 0.36298 to 0.36282, saving model to folds0.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8159 - loss: 0.3441 - val_accuracy: 0.7993 - val_loss: 0.3628 - learning_rate: 6.2500e-04\n",
      "Epoch 197/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8171 - loss: 0.3477\n",
      "Epoch 197: val_loss did not improve from 0.36282\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8170 - loss: 0.3475 - val_accuracy: 0.7987 - val_loss: 0.3634 - learning_rate: 6.2500e-04\n",
      "Epoch 198/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8183 - loss: 0.3454\n",
      "Epoch 198: val_loss improved from 0.36282 to 0.36241, saving model to folds0.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.8179 - loss: 0.3455 - val_accuracy: 0.8041 - val_loss: 0.3624 - learning_rate: 6.2500e-04\n",
      "Epoch 199/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8104 - loss: 0.3440\n",
      "Epoch 199: val_loss did not improve from 0.36241\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8110 - loss: 0.3442 - val_accuracy: 0.8029 - val_loss: 0.3628 - learning_rate: 6.2500e-04\n",
      "Epoch 200/300\n",
      "\u001b[1m13/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8192 - loss: 0.3428\n",
      "Epoch 200: val_loss did not improve from 0.36241\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8185 - loss: 0.3433 - val_accuracy: 0.7993 - val_loss: 0.3627 - learning_rate: 6.2500e-04\n",
      "Epoch 201/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8161 - loss: 0.3424\n",
      "Epoch 201: val_loss improved from 0.36241 to 0.36211, saving model to folds0.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.8160 - loss: 0.3426 - val_accuracy: 0.8011 - val_loss: 0.3621 - learning_rate: 6.2500e-04\n",
      "Epoch 202/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8187 - loss: 0.3379  \n",
      "Epoch 202: val_loss did not improve from 0.36211\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8169 - loss: 0.3412 - val_accuracy: 0.8011 - val_loss: 0.3627 - learning_rate: 6.2500e-04\n",
      "Epoch 203/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8170 - loss: 0.3430 \n",
      "Epoch 203: val_loss did not improve from 0.36211\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8164 - loss: 0.3437 - val_accuracy: 0.8029 - val_loss: 0.3631 - learning_rate: 6.2500e-04\n",
      "Epoch 204/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8205 - loss: 0.3376 \n",
      "Epoch 204: val_loss did not improve from 0.36211\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8188 - loss: 0.3400 - val_accuracy: 0.8054 - val_loss: 0.3621 - learning_rate: 6.2500e-04\n",
      "Epoch 205/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8155 - loss: 0.3439 \n",
      "Epoch 205: val_loss improved from 0.36211 to 0.36186, saving model to folds0.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8158 - loss: 0.3441 - val_accuracy: 0.8054 - val_loss: 0.3619 - learning_rate: 6.2500e-04\n",
      "Epoch 206/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8148 - loss: 0.3452 \n",
      "Epoch 206: val_loss did not improve from 0.36186\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8152 - loss: 0.3450 - val_accuracy: 0.7987 - val_loss: 0.3646 - learning_rate: 6.2500e-04\n",
      "Epoch 207/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8094 - loss: 0.3440 \n",
      "Epoch 207: val_loss did not improve from 0.36186\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8106 - loss: 0.3443 - val_accuracy: 0.8011 - val_loss: 0.3621 - learning_rate: 6.2500e-04\n",
      "Epoch 208/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8117 - loss: 0.3458 \n",
      "Epoch 208: val_loss improved from 0.36186 to 0.36179, saving model to folds0.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8126 - loss: 0.3450 - val_accuracy: 0.7999 - val_loss: 0.3618 - learning_rate: 6.2500e-04\n",
      "Epoch 209/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8120 - loss: 0.3485 \n",
      "Epoch 209: val_loss did not improve from 0.36179\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8130 - loss: 0.3470 - val_accuracy: 0.8005 - val_loss: 0.3632 - learning_rate: 6.2500e-04\n",
      "Epoch 210/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8074 - loss: 0.3485 \n",
      "Epoch 210: val_loss did not improve from 0.36179\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8100 - loss: 0.3468 - val_accuracy: 0.8066 - val_loss: 0.3623 - learning_rate: 6.2500e-04\n",
      "Epoch 211/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8191 - loss: 0.3466 \n",
      "Epoch 211: val_loss did not improve from 0.36179\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8178 - loss: 0.3463 - val_accuracy: 0.7980 - val_loss: 0.3622 - learning_rate: 6.2500e-04\n",
      "Epoch 212/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8142 - loss: 0.3467 \n",
      "Epoch 212: val_loss improved from 0.36179 to 0.36135, saving model to folds0.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8139 - loss: 0.3459 - val_accuracy: 0.8011 - val_loss: 0.3614 - learning_rate: 6.2500e-04\n",
      "Epoch 213/300\n",
      "\u001b[1m 7/15\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8187 - loss: 0.3444 \n",
      "Epoch 213: val_loss improved from 0.36135 to 0.36055, saving model to folds0.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8177 - loss: 0.3431 - val_accuracy: 0.8023 - val_loss: 0.3606 - learning_rate: 6.2500e-04\n",
      "Epoch 214/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8098 - loss: 0.3514 \n",
      "Epoch 214: val_loss did not improve from 0.36055\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8117 - loss: 0.3489 - val_accuracy: 0.8011 - val_loss: 0.3626 - learning_rate: 6.2500e-04\n",
      "Epoch 215/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8138 - loss: 0.3460 \n",
      "Epoch 215: val_loss improved from 0.36055 to 0.36017, saving model to folds0.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8145 - loss: 0.3453 - val_accuracy: 0.8017 - val_loss: 0.3602 - learning_rate: 6.2500e-04\n",
      "Epoch 216/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8149 - loss: 0.3397 \n",
      "Epoch 216: val_loss did not improve from 0.36017\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8149 - loss: 0.3410 - val_accuracy: 0.7999 - val_loss: 0.3606 - learning_rate: 6.2500e-04\n",
      "Epoch 217/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8187 - loss: 0.3372 \n",
      "Epoch 217: val_loss did not improve from 0.36017\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8173 - loss: 0.3395 - val_accuracy: 0.8005 - val_loss: 0.3612 - learning_rate: 6.2500e-04\n",
      "Epoch 218/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8123 - loss: 0.3461 \n",
      "Epoch 218: val_loss did not improve from 0.36017\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8132 - loss: 0.3452 - val_accuracy: 0.8054 - val_loss: 0.3608 - learning_rate: 6.2500e-04\n",
      "Epoch 219/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8195 - loss: 0.3448 \n",
      "Epoch 219: val_loss did not improve from 0.36017\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8187 - loss: 0.3442 - val_accuracy: 0.8005 - val_loss: 0.3615 - learning_rate: 6.2500e-04\n",
      "Epoch 220/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8154 - loss: 0.3389 \n",
      "Epoch 220: val_loss improved from 0.36017 to 0.35937, saving model to folds0.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8146 - loss: 0.3403 - val_accuracy: 0.8029 - val_loss: 0.3594 - learning_rate: 6.2500e-04\n",
      "Epoch 221/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8148 - loss: 0.3430 \n",
      "Epoch 221: val_loss did not improve from 0.35937\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8146 - loss: 0.3432 - val_accuracy: 0.8005 - val_loss: 0.3612 - learning_rate: 6.2500e-04\n",
      "Epoch 222/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8119 - loss: 0.3420 \n",
      "Epoch 222: val_loss did not improve from 0.35937\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8128 - loss: 0.3422 - val_accuracy: 0.8041 - val_loss: 0.3607 - learning_rate: 6.2500e-04\n",
      "Epoch 223/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8154 - loss: 0.3410 \n",
      "Epoch 223: val_loss did not improve from 0.35937\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8158 - loss: 0.3419 - val_accuracy: 0.8023 - val_loss: 0.3605 - learning_rate: 6.2500e-04\n",
      "Epoch 224/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8130 - loss: 0.3429 \n",
      "Epoch 224: val_loss did not improve from 0.35937\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8130 - loss: 0.3429 - val_accuracy: 0.8035 - val_loss: 0.3609 - learning_rate: 6.2500e-04\n",
      "Epoch 225/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8069 - loss: 0.3525 \n",
      "Epoch 225: val_loss did not improve from 0.35937\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8114 - loss: 0.3490 - val_accuracy: 0.8005 - val_loss: 0.3613 - learning_rate: 6.2500e-04\n",
      "Epoch 226/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8086 - loss: 0.3450 \n",
      "Epoch 226: val_loss did not improve from 0.35937\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8107 - loss: 0.3442 - val_accuracy: 0.8017 - val_loss: 0.3607 - learning_rate: 6.2500e-04\n",
      "Epoch 227/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8172 - loss: 0.3438 \n",
      "Epoch 227: val_loss did not improve from 0.35937\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8165 - loss: 0.3439 - val_accuracy: 0.8023 - val_loss: 0.3599 - learning_rate: 6.2500e-04\n",
      "Epoch 228/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8149 - loss: 0.3426 \n",
      "Epoch 228: val_loss did not improve from 0.35937\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8144 - loss: 0.3426 - val_accuracy: 0.7993 - val_loss: 0.3595 - learning_rate: 6.2500e-04\n",
      "Epoch 229/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8148 - loss: 0.3404 \n",
      "Epoch 229: val_loss did not improve from 0.35937\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8150 - loss: 0.3406 - val_accuracy: 0.7999 - val_loss: 0.3604 - learning_rate: 6.2500e-04\n",
      "Epoch 230/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8106 - loss: 0.3495 \n",
      "Epoch 230: ReduceLROnPlateau reducing learning rate to 0.0003124999930150807.\n",
      "\n",
      "Epoch 230: val_loss did not improve from 0.35937\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8131 - loss: 0.3456 - val_accuracy: 0.8017 - val_loss: 0.3604 - learning_rate: 6.2500e-04\n",
      "Epoch 231/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8161 - loss: 0.3363 \n",
      "Epoch 231: val_loss improved from 0.35937 to 0.35904, saving model to folds0.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8154 - loss: 0.3382 - val_accuracy: 0.8017 - val_loss: 0.3590 - learning_rate: 3.1250e-04\n",
      "Epoch 232/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8165 - loss: 0.3408 \n",
      "Epoch 232: val_loss did not improve from 0.35904\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8162 - loss: 0.3412 - val_accuracy: 0.8029 - val_loss: 0.3596 - learning_rate: 3.1250e-04\n",
      "Epoch 233/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8167 - loss: 0.3363 \n",
      "Epoch 233: val_loss did not improve from 0.35904\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8157 - loss: 0.3383 - val_accuracy: 0.8035 - val_loss: 0.3598 - learning_rate: 3.1250e-04\n",
      "Epoch 234/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8150 - loss: 0.3381 \n",
      "Epoch 234: val_loss did not improve from 0.35904\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8157 - loss: 0.3386 - val_accuracy: 0.7999 - val_loss: 0.3596 - learning_rate: 3.1250e-04\n",
      "Epoch 235/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8214 - loss: 0.3351 \n",
      "Epoch 235: val_loss did not improve from 0.35904\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8195 - loss: 0.3374 - val_accuracy: 0.8005 - val_loss: 0.3594 - learning_rate: 3.1250e-04\n",
      "Epoch 236/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8088 - loss: 0.3374 \n",
      "Epoch 236: val_loss did not improve from 0.35904\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8105 - loss: 0.3390 - val_accuracy: 0.7999 - val_loss: 0.3592 - learning_rate: 3.1250e-04\n",
      "Epoch 237/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8176 - loss: 0.3381 \n",
      "Epoch 237: val_loss did not improve from 0.35904\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8176 - loss: 0.3391 - val_accuracy: 0.8011 - val_loss: 0.3594 - learning_rate: 3.1250e-04\n",
      "Epoch 238/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8154 - loss: 0.3339 \n",
      "Epoch 238: val_loss improved from 0.35904 to 0.35895, saving model to folds0.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8148 - loss: 0.3365 - val_accuracy: 0.8017 - val_loss: 0.3589 - learning_rate: 3.1250e-04\n",
      "Epoch 239/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8171 - loss: 0.3454 \n",
      "Epoch 239: val_loss did not improve from 0.35895\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8158 - loss: 0.3445 - val_accuracy: 0.8029 - val_loss: 0.3596 - learning_rate: 3.1250e-04\n",
      "Epoch 240/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8166 - loss: 0.3376 \n",
      "Epoch 240: val_loss did not improve from 0.35895\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8162 - loss: 0.3386 - val_accuracy: 0.8011 - val_loss: 0.3593 - learning_rate: 3.1250e-04\n",
      "Epoch 241/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8111 - loss: 0.3411 \n",
      "Epoch 241: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "\n",
      "Epoch 241: val_loss did not improve from 0.35895\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8123 - loss: 0.3411 - val_accuracy: 0.8017 - val_loss: 0.3592 - learning_rate: 3.1250e-04\n",
      "Epoch 242/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8133 - loss: 0.3428 \n",
      "Epoch 242: val_loss did not improve from 0.35895\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8139 - loss: 0.3423 - val_accuracy: 0.8005 - val_loss: 0.3591 - learning_rate: 1.5625e-04\n",
      "Epoch 243/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8107 - loss: 0.3453 \n",
      "Epoch 243: val_loss did not improve from 0.35895\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8117 - loss: 0.3443 - val_accuracy: 0.8017 - val_loss: 0.3591 - learning_rate: 1.5625e-04\n",
      "Epoch 244/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8219 - loss: 0.3327 \n",
      "Epoch 244: val_loss did not improve from 0.35895\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8182 - loss: 0.3364 - val_accuracy: 0.8017 - val_loss: 0.3590 - learning_rate: 1.5625e-04\n",
      "Epoch 245/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8156 - loss: 0.3383 \n",
      "Epoch 245: val_loss did not improve from 0.35895\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8151 - loss: 0.3394 - val_accuracy: 0.8005 - val_loss: 0.3592 - learning_rate: 1.5625e-04\n",
      "Epoch 246/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8112 - loss: 0.3444 \n",
      "Epoch 246: val_loss did not improve from 0.35895\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8125 - loss: 0.3429 - val_accuracy: 0.8023 - val_loss: 0.3590 - learning_rate: 1.5625e-04\n",
      "Epoch 247/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8183 - loss: 0.3370 \n",
      "Epoch 247: val_loss did not improve from 0.35895\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8172 - loss: 0.3383 - val_accuracy: 0.8017 - val_loss: 0.3591 - learning_rate: 1.5625e-04\n",
      "Epoch 248/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8136 - loss: 0.3420 \n",
      "Epoch 248: val_loss did not improve from 0.35895\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8145 - loss: 0.3410 - val_accuracy: 0.8011 - val_loss: 0.3590 - learning_rate: 1.5625e-04\n",
      "Epoch 249/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8150 - loss: 0.3361 \n",
      "Epoch 249: val_loss improved from 0.35895 to 0.35876, saving model to folds0.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8149 - loss: 0.3376 - val_accuracy: 0.7999 - val_loss: 0.3588 - learning_rate: 1.5625e-04\n",
      "Epoch 250/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8140 - loss: 0.3402 \n",
      "Epoch 250: val_loss did not improve from 0.35876\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8142 - loss: 0.3404 - val_accuracy: 0.8005 - val_loss: 0.3590 - learning_rate: 1.5625e-04\n",
      "Epoch 251/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8119 - loss: 0.3454 \n",
      "Epoch 251: val_loss did not improve from 0.35876\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8125 - loss: 0.3439 - val_accuracy: 0.8029 - val_loss: 0.3589 - learning_rate: 1.5625e-04\n",
      "Epoch 252/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8078 - loss: 0.3440  \n",
      "Epoch 252: val_loss did not improve from 0.35876\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8101 - loss: 0.3433 - val_accuracy: 0.8029 - val_loss: 0.3588 - learning_rate: 1.5625e-04\n",
      "Epoch 253/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8159 - loss: 0.3407\n",
      "Epoch 253: val_loss improved from 0.35876 to 0.35876, saving model to folds0.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.8157 - loss: 0.3407 - val_accuracy: 0.8011 - val_loss: 0.3588 - learning_rate: 1.5625e-04\n",
      "Epoch 254/300\n",
      "\u001b[1m13/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8141 - loss: 0.3410\n",
      "Epoch 254: val_loss improved from 0.35876 to 0.35861, saving model to folds0.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8144 - loss: 0.3410 - val_accuracy: 0.8023 - val_loss: 0.3586 - learning_rate: 1.5625e-04\n",
      "Epoch 255/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8138 - loss: 0.3423\n",
      "Epoch 255: val_loss did not improve from 0.35861\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8139 - loss: 0.3422 - val_accuracy: 0.8023 - val_loss: 0.3587 - learning_rate: 1.5625e-04\n",
      "Epoch 256/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8124 - loss: 0.3441\n",
      "Epoch 256: val_loss did not improve from 0.35861\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8126 - loss: 0.3439 - val_accuracy: 0.8005 - val_loss: 0.3587 - learning_rate: 1.5625e-04\n",
      "Epoch 257/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8189 - loss: 0.3390\n",
      "Epoch 257: val_loss improved from 0.35861 to 0.35850, saving model to folds0.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.8182 - loss: 0.3393 - val_accuracy: 0.8011 - val_loss: 0.3585 - learning_rate: 1.5625e-04\n",
      "Epoch 258/300\n",
      "\u001b[1m13/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8174 - loss: 0.3371 \n",
      "Epoch 258: val_loss did not improve from 0.35850\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8169 - loss: 0.3378 - val_accuracy: 0.8011 - val_loss: 0.3588 - learning_rate: 1.5625e-04\n",
      "Epoch 259/300\n",
      "\u001b[1m12/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8119 - loss: 0.3490\n",
      "Epoch 259: val_loss did not improve from 0.35850\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8126 - loss: 0.3469 - val_accuracy: 0.8017 - val_loss: 0.3585 - learning_rate: 1.5625e-04\n",
      "Epoch 260/300\n",
      "\u001b[1m12/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8129 - loss: 0.3402\n",
      "Epoch 260: val_loss improved from 0.35850 to 0.35818, saving model to folds0.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.8134 - loss: 0.3403 - val_accuracy: 0.8017 - val_loss: 0.3582 - learning_rate: 1.5625e-04\n",
      "Epoch 261/300\n",
      "\u001b[1m12/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8165 - loss: 0.3413\n",
      "Epoch 261: val_loss did not improve from 0.35818\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.8160 - loss: 0.3411 - val_accuracy: 0.8005 - val_loss: 0.3588 - learning_rate: 1.5625e-04\n",
      "Epoch 262/300\n",
      "\u001b[1m12/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8115 - loss: 0.3434\n",
      "Epoch 262: val_loss did not improve from 0.35818\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8123 - loss: 0.3428 - val_accuracy: 0.8017 - val_loss: 0.3583 - learning_rate: 1.5625e-04\n",
      "Epoch 263/300\n",
      "\u001b[1m13/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8146 - loss: 0.3422 \n",
      "Epoch 263: val_loss did not improve from 0.35818\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.8147 - loss: 0.3419 - val_accuracy: 0.8017 - val_loss: 0.3588 - learning_rate: 1.5625e-04\n",
      "Epoch 264/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8165 - loss: 0.3408 \n",
      "Epoch 264: val_loss did not improve from 0.35818\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8160 - loss: 0.3412 - val_accuracy: 0.8017 - val_loss: 0.3582 - learning_rate: 1.5625e-04\n",
      "Epoch 265/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8150 - loss: 0.3403 \n",
      "Epoch 265: val_loss did not improve from 0.35818\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8150 - loss: 0.3405 - val_accuracy: 0.8011 - val_loss: 0.3585 - learning_rate: 1.5625e-04\n",
      "Epoch 266/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8139 - loss: 0.3429 \n",
      "Epoch 266: val_loss did not improve from 0.35818\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8146 - loss: 0.3416 - val_accuracy: 0.7999 - val_loss: 0.3584 - learning_rate: 1.5625e-04\n",
      "Epoch 267/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8171 - loss: 0.3364 \n",
      "Epoch 267: val_loss did not improve from 0.35818\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8167 - loss: 0.3374 - val_accuracy: 0.8011 - val_loss: 0.3583 - learning_rate: 1.5625e-04\n",
      "Epoch 268/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8120 - loss: 0.3450 \n",
      "Epoch 268: val_loss did not improve from 0.35818\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8133 - loss: 0.3430 - val_accuracy: 0.7999 - val_loss: 0.3587 - learning_rate: 1.5625e-04\n",
      "Epoch 269/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8111 - loss: 0.3409 \n",
      "Epoch 269: val_loss did not improve from 0.35818\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8127 - loss: 0.3403 - val_accuracy: 0.8017 - val_loss: 0.3583 - learning_rate: 1.5625e-04\n",
      "Epoch 270/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8180 - loss: 0.3406 \n",
      "Epoch 270: ReduceLROnPlateau reducing learning rate to 7.812499825377017e-05.\n",
      "\n",
      "Epoch 270: val_loss did not improve from 0.35818\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8170 - loss: 0.3405 - val_accuracy: 0.8011 - val_loss: 0.3583 - learning_rate: 1.5625e-04\n",
      "Epoch 271/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8085 - loss: 0.3457 \n",
      "Epoch 271: val_loss did not improve from 0.35818\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8114 - loss: 0.3435 - val_accuracy: 0.7999 - val_loss: 0.3583 - learning_rate: 7.8125e-05\n",
      "Epoch 272/300\n",
      "\u001b[1m 7/15\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8181 - loss: 0.3411 \n",
      "Epoch 272: val_loss did not improve from 0.35818\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8167 - loss: 0.3403 - val_accuracy: 0.7999 - val_loss: 0.3583 - learning_rate: 7.8125e-05\n",
      "Epoch 273/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8148 - loss: 0.3369 \n",
      "Epoch 273: val_loss did not improve from 0.35818\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8148 - loss: 0.3379 - val_accuracy: 0.8005 - val_loss: 0.3582 - learning_rate: 7.8125e-05\n",
      "Epoch 274/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8171 - loss: 0.3370 \n",
      "Epoch 274: val_loss improved from 0.35818 to 0.35802, saving model to folds0.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8169 - loss: 0.3377 - val_accuracy: 0.8011 - val_loss: 0.3580 - learning_rate: 7.8125e-05\n",
      "Epoch 275/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8151 - loss: 0.3413 \n",
      "Epoch 275: val_loss did not improve from 0.35802\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8156 - loss: 0.3405 - val_accuracy: 0.8005 - val_loss: 0.3582 - learning_rate: 7.8125e-05\n",
      "Epoch 276/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8158 - loss: 0.3396 \n",
      "Epoch 276: val_loss did not improve from 0.35802\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8158 - loss: 0.3400 - val_accuracy: 0.8011 - val_loss: 0.3583 - learning_rate: 7.8125e-05\n",
      "Epoch 277/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8181 - loss: 0.3391 \n",
      "Epoch 277: val_loss did not improve from 0.35802\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8170 - loss: 0.3394 - val_accuracy: 0.8017 - val_loss: 0.3582 - learning_rate: 7.8125e-05\n",
      "Epoch 278/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8115 - loss: 0.3403 \n",
      "Epoch 278: val_loss did not improve from 0.35802\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8129 - loss: 0.3402 - val_accuracy: 0.8017 - val_loss: 0.3583 - learning_rate: 7.8125e-05\n",
      "Epoch 279/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8161 - loss: 0.3416 \n",
      "Epoch 279: val_loss did not improve from 0.35802\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8160 - loss: 0.3411 - val_accuracy: 0.8005 - val_loss: 0.3581 - learning_rate: 7.8125e-05\n",
      "Epoch 280/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8133 - loss: 0.3422 \n",
      "Epoch 280: val_loss did not improve from 0.35802\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8136 - loss: 0.3418 - val_accuracy: 0.7999 - val_loss: 0.3582 - learning_rate: 7.8125e-05\n",
      "Epoch 281/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8123 - loss: 0.3451 \n",
      "Epoch 281: val_loss did not improve from 0.35802\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8132 - loss: 0.3435 - val_accuracy: 0.8017 - val_loss: 0.3583 - learning_rate: 7.8125e-05\n",
      "Epoch 282/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8134 - loss: 0.3422 \n",
      "Epoch 282: val_loss improved from 0.35802 to 0.35795, saving model to folds0.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8141 - loss: 0.3417 - val_accuracy: 0.7993 - val_loss: 0.3579 - learning_rate: 7.8125e-05\n",
      "Epoch 283/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8203 - loss: 0.3348 \n",
      "Epoch 283: val_loss did not improve from 0.35795\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8189 - loss: 0.3365 - val_accuracy: 0.8023 - val_loss: 0.3582 - learning_rate: 7.8125e-05\n",
      "Epoch 284/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8165 - loss: 0.3413 \n",
      "Epoch 284: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "\n",
      "Epoch 284: val_loss did not improve from 0.35795\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8157 - loss: 0.3410 - val_accuracy: 0.8017 - val_loss: 0.3582 - learning_rate: 7.8125e-05\n",
      "Epoch 285/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8218 - loss: 0.3356 \n",
      "Epoch 285: val_loss did not improve from 0.35795\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8195 - loss: 0.3373 - val_accuracy: 0.8005 - val_loss: 0.3581 - learning_rate: 3.9062e-05\n",
      "Epoch 286/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8165 - loss: 0.3412 \n",
      "Epoch 286: val_loss did not improve from 0.35795\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8157 - loss: 0.3410 - val_accuracy: 0.8005 - val_loss: 0.3581 - learning_rate: 3.9062e-05\n",
      "Epoch 287/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8107 - loss: 0.3416 \n",
      "Epoch 287: val_loss improved from 0.35795 to 0.35794, saving model to folds0.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8126 - loss: 0.3410 - val_accuracy: 0.8011 - val_loss: 0.3579 - learning_rate: 3.9062e-05\n",
      "Epoch 288/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8220 - loss: 0.3400 \n",
      "Epoch 288: val_loss did not improve from 0.35794\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8193 - loss: 0.3398 - val_accuracy: 0.8011 - val_loss: 0.3580 - learning_rate: 3.9062e-05\n",
      "Epoch 289/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8157 - loss: 0.3355 \n",
      "Epoch 289: val_loss did not improve from 0.35794\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8155 - loss: 0.3374 - val_accuracy: 0.8005 - val_loss: 0.3581 - learning_rate: 3.9062e-05\n",
      "Epoch 290/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8124 - loss: 0.3441 \n",
      "Epoch 290: val_loss did not improve from 0.35794\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8138 - loss: 0.3421 - val_accuracy: 0.8017 - val_loss: 0.3582 - learning_rate: 3.9062e-05\n",
      "Epoch 291/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8166 - loss: 0.3403 \n",
      "Epoch 291: val_loss did not improve from 0.35794\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8161 - loss: 0.3401 - val_accuracy: 0.8011 - val_loss: 0.3581 - learning_rate: 3.9062e-05\n",
      "Epoch 292/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8086 - loss: 0.3427 \n",
      "Epoch 292: val_loss did not improve from 0.35794\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8109 - loss: 0.3422 - val_accuracy: 0.8005 - val_loss: 0.3580 - learning_rate: 3.9062e-05\n",
      "Epoch 293/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8171 - loss: 0.3351 \n",
      "Epoch 293: val_loss did not improve from 0.35794\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8168 - loss: 0.3365 - val_accuracy: 0.8017 - val_loss: 0.3580 - learning_rate: 3.9062e-05\n",
      "Epoch 294/300\n",
      "\u001b[1m 7/15\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8206 - loss: 0.3378 \n",
      "Epoch 294: ReduceLROnPlateau reducing learning rate to 1.9531249563442543e-05.\n",
      "\n",
      "Epoch 294: val_loss did not improve from 0.35794\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8170 - loss: 0.3388 - val_accuracy: 0.8005 - val_loss: 0.3581 - learning_rate: 3.9062e-05\n",
      "Epoch 295/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8131 - loss: 0.3448  \n",
      "Epoch 295: val_loss did not improve from 0.35794\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8139 - loss: 0.3434 - val_accuracy: 0.8011 - val_loss: 0.3580 - learning_rate: 1.9531e-05\n",
      "Epoch 296/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8124 - loss: 0.3387 \n",
      "Epoch 296: val_loss did not improve from 0.35794\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8134 - loss: 0.3391 - val_accuracy: 0.8011 - val_loss: 0.3580 - learning_rate: 1.9531e-05\n",
      "Epoch 297/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8100 - loss: 0.3441 \n",
      "Epoch 297: val_loss did not improve from 0.35794\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8120 - loss: 0.3429 - val_accuracy: 0.8011 - val_loss: 0.3580 - learning_rate: 1.9531e-05\n",
      "Epoch 298/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8147 - loss: 0.3365 \n",
      "Epoch 298: val_loss did not improve from 0.35794\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8152 - loss: 0.3376 - val_accuracy: 0.8017 - val_loss: 0.3580 - learning_rate: 1.9531e-05\n",
      "Epoch 299/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8196 - loss: 0.3398 \n",
      "Epoch 299: val_loss did not improve from 0.35794\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8184 - loss: 0.3398 - val_accuracy: 0.8005 - val_loss: 0.3580 - learning_rate: 1.9531e-05\n",
      "Epoch 300/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8203 - loss: 0.3369 \n",
      "Epoch 300: val_loss did not improve from 0.35794\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8183 - loss: 0.3380 - val_accuracy: 0.8005 - val_loss: 0.3580 - learning_rate: 1.9531e-05\n",
      "Restoring model weights from the end of the best epoch: 287.\n",
      " Computing LSTM feature importance...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:01<00:00,  6.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- > Fold 2 < ---------------\n",
      "Epoch 1/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6963 - loss: 0.5712\n",
      "Epoch 1: val_loss improved from inf to 0.51558, saving model to folds1.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 43ms/step - accuracy: 0.6976 - loss: 0.5684 - val_accuracy: 0.7285 - val_loss: 0.5156 - learning_rate: 0.0100\n",
      "Epoch 2/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7148 - loss: 0.5234 \n",
      "Epoch 2: val_loss improved from 0.51558 to 0.49337, saving model to folds1.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.7140 - loss: 0.5213 - val_accuracy: 0.7236 - val_loss: 0.4934 - learning_rate: 0.0100\n",
      "Epoch 3/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7206 - loss: 0.4975\n",
      "Epoch 3: val_loss improved from 0.49337 to 0.48443, saving model to folds1.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.7208 - loss: 0.4975 - val_accuracy: 0.7425 - val_loss: 0.4844 - learning_rate: 0.0100\n",
      "Epoch 4/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7312 - loss: 0.4894\n",
      "Epoch 4: val_loss improved from 0.48443 to 0.46894, saving model to folds1.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.7311 - loss: 0.4891 - val_accuracy: 0.7474 - val_loss: 0.4689 - learning_rate: 0.0100\n",
      "Epoch 5/300\n",
      "\u001b[1m12/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7333 - loss: 0.4822\n",
      "Epoch 5: val_loss did not improve from 0.46894\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.7327 - loss: 0.4827 - val_accuracy: 0.7376 - val_loss: 0.4708 - learning_rate: 0.0100\n",
      "Epoch 6/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7279 - loss: 0.4842 \n",
      "Epoch 6: val_loss did not improve from 0.46894\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7296 - loss: 0.4838 - val_accuracy: 0.7395 - val_loss: 0.4703 - learning_rate: 0.0100\n",
      "Epoch 7/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7333 - loss: 0.4789 \n",
      "Epoch 7: val_loss did not improve from 0.46894\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7326 - loss: 0.4804 - val_accuracy: 0.7279 - val_loss: 0.4834 - learning_rate: 0.0100\n",
      "Epoch 8/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7381 - loss: 0.4784 \n",
      "Epoch 8: val_loss improved from 0.46894 to 0.46397, saving model to folds1.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7384 - loss: 0.4786 - val_accuracy: 0.7413 - val_loss: 0.4640 - learning_rate: 0.0100\n",
      "Epoch 9/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7324 - loss: 0.4740 \n",
      "Epoch 9: val_loss improved from 0.46397 to 0.45973, saving model to folds1.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7342 - loss: 0.4731 - val_accuracy: 0.7517 - val_loss: 0.4597 - learning_rate: 0.0100\n",
      "Epoch 10/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7295 - loss: 0.4677 \n",
      "Epoch 10: val_loss did not improve from 0.45973\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7316 - loss: 0.4689 - val_accuracy: 0.7474 - val_loss: 0.4680 - learning_rate: 0.0100\n",
      "Epoch 11/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7434 - loss: 0.4687 \n",
      "Epoch 11: val_loss did not improve from 0.45973\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7416 - loss: 0.4694 - val_accuracy: 0.7364 - val_loss: 0.4684 - learning_rate: 0.0100\n",
      "Epoch 12/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7273 - loss: 0.4751 \n",
      "Epoch 12: val_loss did not improve from 0.45973\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7297 - loss: 0.4731 - val_accuracy: 0.7462 - val_loss: 0.4607 - learning_rate: 0.0100\n",
      "Epoch 13/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7396 - loss: 0.4700  \n",
      "Epoch 13: val_loss improved from 0.45973 to 0.45789, saving model to folds1.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7403 - loss: 0.4688 - val_accuracy: 0.7498 - val_loss: 0.4579 - learning_rate: 0.0100\n",
      "Epoch 14/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7475 - loss: 0.4633 \n",
      "Epoch 14: val_loss did not improve from 0.45789\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7467 - loss: 0.4634 - val_accuracy: 0.7523 - val_loss: 0.4583 - learning_rate: 0.0100\n",
      "Epoch 15/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7425 - loss: 0.4686 \n",
      "Epoch 15: val_loss improved from 0.45789 to 0.45700, saving model to folds1.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7428 - loss: 0.4668 - val_accuracy: 0.7566 - val_loss: 0.4570 - learning_rate: 0.0100\n",
      "Epoch 16/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7464 - loss: 0.4606 \n",
      "Epoch 16: val_loss did not improve from 0.45700\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7434 - loss: 0.4623 - val_accuracy: 0.7590 - val_loss: 0.4583 - learning_rate: 0.0100\n",
      "Epoch 17/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7586 - loss: 0.4503 \n",
      "Epoch 17: val_loss improved from 0.45700 to 0.45357, saving model to folds1.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7539 - loss: 0.4536 - val_accuracy: 0.7596 - val_loss: 0.4536 - learning_rate: 0.0100\n",
      "Epoch 18/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7414 - loss: 0.4657 \n",
      "Epoch 18: val_loss did not improve from 0.45357\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7428 - loss: 0.4633 - val_accuracy: 0.7425 - val_loss: 0.4570 - learning_rate: 0.0100\n",
      "Epoch 19/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7486 - loss: 0.4550 \n",
      "Epoch 19: val_loss did not improve from 0.45357\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7472 - loss: 0.4560 - val_accuracy: 0.7547 - val_loss: 0.4539 - learning_rate: 0.0100\n",
      "Epoch 20/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7480 - loss: 0.4638 \n",
      "Epoch 20: val_loss did not improve from 0.45357\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7474 - loss: 0.4632 - val_accuracy: 0.7559 - val_loss: 0.4560 - learning_rate: 0.0100\n",
      "Epoch 21/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7527 - loss: 0.4564 \n",
      "Epoch 21: val_loss improved from 0.45357 to 0.45109, saving model to folds1.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7516 - loss: 0.4571 - val_accuracy: 0.7566 - val_loss: 0.4511 - learning_rate: 0.0100\n",
      "Epoch 22/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7527 - loss: 0.4530 \n",
      "Epoch 22: val_loss improved from 0.45109 to 0.44782, saving model to folds1.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7530 - loss: 0.4526 - val_accuracy: 0.7657 - val_loss: 0.4478 - learning_rate: 0.0100\n",
      "Epoch 23/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7548 - loss: 0.4511 \n",
      "Epoch 23: val_loss did not improve from 0.44782\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7532 - loss: 0.4522 - val_accuracy: 0.7578 - val_loss: 0.4489 - learning_rate: 0.0100\n",
      "Epoch 24/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7549 - loss: 0.4396 \n",
      "Epoch 24: val_loss improved from 0.44782 to 0.43852, saving model to folds1.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7540 - loss: 0.4423 - val_accuracy: 0.7688 - val_loss: 0.4385 - learning_rate: 0.0100\n",
      "Epoch 25/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7580 - loss: 0.4423 \n",
      "Epoch 25: val_loss improved from 0.43852 to 0.43698, saving model to folds1.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7572 - loss: 0.4432 - val_accuracy: 0.7724 - val_loss: 0.4370 - learning_rate: 0.0100\n",
      "Epoch 26/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7731 - loss: 0.4337 \n",
      "Epoch 26: val_loss did not improve from 0.43698\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7693 - loss: 0.4376 - val_accuracy: 0.7529 - val_loss: 0.4446 - learning_rate: 0.0100\n",
      "Epoch 27/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7561 - loss: 0.4441 \n",
      "Epoch 27: val_loss did not improve from 0.43698\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7558 - loss: 0.4451 - val_accuracy: 0.7572 - val_loss: 0.4449 - learning_rate: 0.0100\n",
      "Epoch 28/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7535 - loss: 0.4443  \n",
      "Epoch 28: val_loss did not improve from 0.43698\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7554 - loss: 0.4436 - val_accuracy: 0.7682 - val_loss: 0.4387 - learning_rate: 0.0100\n",
      "Epoch 29/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7582 - loss: 0.4430 \n",
      "Epoch 29: val_loss improved from 0.43698 to 0.43571, saving model to folds1.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7585 - loss: 0.4423 - val_accuracy: 0.7621 - val_loss: 0.4357 - learning_rate: 0.0100\n",
      "Epoch 30/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7654 - loss: 0.4341 \n",
      "Epoch 30: val_loss did not improve from 0.43571\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7630 - loss: 0.4357 - val_accuracy: 0.7657 - val_loss: 0.4403 - learning_rate: 0.0100\n",
      "Epoch 31/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7627 - loss: 0.4384  \n",
      "Epoch 31: val_loss did not improve from 0.43571\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7618 - loss: 0.4392 - val_accuracy: 0.7633 - val_loss: 0.4377 - learning_rate: 0.0100\n",
      "Epoch 32/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7665 - loss: 0.4427 \n",
      "Epoch 32: val_loss improved from 0.43571 to 0.42803, saving model to folds1.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7664 - loss: 0.4410 - val_accuracy: 0.7755 - val_loss: 0.4280 - learning_rate: 0.0100\n",
      "Epoch 33/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7522 - loss: 0.4412 \n",
      "Epoch 33: val_loss did not improve from 0.42803\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7551 - loss: 0.4391 - val_accuracy: 0.7700 - val_loss: 0.4309 - learning_rate: 0.0100\n",
      "Epoch 34/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7638 - loss: 0.4344  \n",
      "Epoch 34: val_loss did not improve from 0.42803\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7622 - loss: 0.4360 - val_accuracy: 0.7694 - val_loss: 0.4325 - learning_rate: 0.0100\n",
      "Epoch 35/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7685 - loss: 0.4382 \n",
      "Epoch 35: val_loss did not improve from 0.42803\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7663 - loss: 0.4376 - val_accuracy: 0.7480 - val_loss: 0.4388 - learning_rate: 0.0100\n",
      "Epoch 36/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7440 - loss: 0.4464 \n",
      "Epoch 36: val_loss did not improve from 0.42803\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7471 - loss: 0.4441 - val_accuracy: 0.7712 - val_loss: 0.4282 - learning_rate: 0.0100\n",
      "Epoch 37/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7661 - loss: 0.4334 \n",
      "Epoch 37: val_loss improved from 0.42803 to 0.42259, saving model to folds1.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7660 - loss: 0.4323 - val_accuracy: 0.7767 - val_loss: 0.4226 - learning_rate: 0.0100\n",
      "Epoch 38/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7592 - loss: 0.4345 \n",
      "Epoch 38: val_loss improved from 0.42259 to 0.41881, saving model to folds1.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7618 - loss: 0.4319 - val_accuracy: 0.7761 - val_loss: 0.4188 - learning_rate: 0.0100\n",
      "Epoch 39/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7667 - loss: 0.4256 \n",
      "Epoch 39: val_loss did not improve from 0.41881\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7681 - loss: 0.4250 - val_accuracy: 0.7791 - val_loss: 0.4209 - learning_rate: 0.0100\n",
      "Epoch 40/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7523 - loss: 0.4362 \n",
      "Epoch 40: val_loss improved from 0.41881 to 0.41705, saving model to folds1.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7559 - loss: 0.4331 - val_accuracy: 0.7645 - val_loss: 0.4171 - learning_rate: 0.0100\n",
      "Epoch 41/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7811 - loss: 0.4111 \n",
      "Epoch 41: val_loss did not improve from 0.41705\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7756 - loss: 0.4151 - val_accuracy: 0.7627 - val_loss: 0.4192 - learning_rate: 0.0100\n",
      "Epoch 42/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7648 - loss: 0.4149 \n",
      "Epoch 42: val_loss improved from 0.41705 to 0.41370, saving model to folds1.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7647 - loss: 0.4175 - val_accuracy: 0.7858 - val_loss: 0.4137 - learning_rate: 0.0100\n",
      "Epoch 43/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7756 - loss: 0.4140 \n",
      "Epoch 43: val_loss improved from 0.41370 to 0.40984, saving model to folds1.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7749 - loss: 0.4147 - val_accuracy: 0.7816 - val_loss: 0.4098 - learning_rate: 0.0100\n",
      "Epoch 44/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7660 - loss: 0.4216 \n",
      "Epoch 44: val_loss improved from 0.40984 to 0.40952, saving model to folds1.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7675 - loss: 0.4204 - val_accuracy: 0.7694 - val_loss: 0.4095 - learning_rate: 0.0100\n",
      "Epoch 45/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7677 - loss: 0.4157 \n",
      "Epoch 45: val_loss did not improve from 0.40952\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7686 - loss: 0.4153 - val_accuracy: 0.7797 - val_loss: 0.4106 - learning_rate: 0.0100\n",
      "Epoch 46/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7685 - loss: 0.4170 \n",
      "Epoch 46: val_loss did not improve from 0.40952\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7706 - loss: 0.4152 - val_accuracy: 0.7773 - val_loss: 0.4119 - learning_rate: 0.0100\n",
      "Epoch 47/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7779 - loss: 0.4168  \n",
      "Epoch 47: val_loss improved from 0.40952 to 0.40778, saving model to folds1.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7762 - loss: 0.4158 - val_accuracy: 0.7730 - val_loss: 0.4078 - learning_rate: 0.0100\n",
      "Epoch 48/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7691 - loss: 0.4085 \n",
      "Epoch 48: val_loss improved from 0.40778 to 0.40613, saving model to folds1.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7693 - loss: 0.4097 - val_accuracy: 0.7810 - val_loss: 0.4061 - learning_rate: 0.0100\n",
      "Epoch 49/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7700 - loss: 0.4144 \n",
      "Epoch 49: val_loss improved from 0.40613 to 0.40359, saving model to folds1.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7705 - loss: 0.4142 - val_accuracy: 0.7846 - val_loss: 0.4036 - learning_rate: 0.0100\n",
      "Epoch 50/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7720 - loss: 0.4167 \n",
      "Epoch 50: val_loss did not improve from 0.40359\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7723 - loss: 0.4157 - val_accuracy: 0.7797 - val_loss: 0.4052 - learning_rate: 0.0100\n",
      "Epoch 51/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7679 - loss: 0.4172 \n",
      "Epoch 51: val_loss improved from 0.40359 to 0.40099, saving model to folds1.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7712 - loss: 0.4138 - val_accuracy: 0.7962 - val_loss: 0.4010 - learning_rate: 0.0100\n",
      "Epoch 52/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7732 - loss: 0.4146 \n",
      "Epoch 52: val_loss improved from 0.40099 to 0.39641, saving model to folds1.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7747 - loss: 0.4126 - val_accuracy: 0.7901 - val_loss: 0.3964 - learning_rate: 0.0100\n",
      "Epoch 53/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7860 - loss: 0.3948  \n",
      "Epoch 53: val_loss did not improve from 0.39641\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7838 - loss: 0.3985 - val_accuracy: 0.7706 - val_loss: 0.4119 - learning_rate: 0.0100\n",
      "Epoch 54/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7784 - loss: 0.4034 \n",
      "Epoch 54: val_loss did not improve from 0.39641\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7782 - loss: 0.4035 - val_accuracy: 0.7871 - val_loss: 0.4023 - learning_rate: 0.0100\n",
      "Epoch 55/300\n",
      "\u001b[1m13/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7819 - loss: 0.4023\n",
      "Epoch 55: val_loss improved from 0.39641 to 0.39456, saving model to folds1.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.7820 - loss: 0.4027 - val_accuracy: 0.7950 - val_loss: 0.3946 - learning_rate: 0.0100\n",
      "Epoch 56/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7753 - loss: 0.4045\n",
      "Epoch 56: val_loss did not improve from 0.39456\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.7756 - loss: 0.4043 - val_accuracy: 0.7932 - val_loss: 0.3999 - learning_rate: 0.0100\n",
      "Epoch 57/300\n",
      "\u001b[1m13/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7828 - loss: 0.4088\n",
      "Epoch 57: val_loss did not improve from 0.39456\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7820 - loss: 0.4089 - val_accuracy: 0.7730 - val_loss: 0.4029 - learning_rate: 0.0100\n",
      "Epoch 58/300\n",
      "\u001b[1m12/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7745 - loss: 0.4071\n",
      "Epoch 58: val_loss did not improve from 0.39456\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.7753 - loss: 0.4070 - val_accuracy: 0.7889 - val_loss: 0.3962 - learning_rate: 0.0100\n",
      "Epoch 59/300\n",
      "\u001b[1m13/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7740 - loss: 0.4086\n",
      "Epoch 59: val_loss improved from 0.39456 to 0.38908, saving model to folds1.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.7752 - loss: 0.4073 - val_accuracy: 0.7932 - val_loss: 0.3891 - learning_rate: 0.0100\n",
      "Epoch 60/300\n",
      "\u001b[1m13/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7834 - loss: 0.3941\n",
      "Epoch 60: val_loss did not improve from 0.38908\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.7831 - loss: 0.3945 - val_accuracy: 0.7877 - val_loss: 0.3936 - learning_rate: 0.0100\n",
      "Epoch 61/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7808 - loss: 0.3985\n",
      "Epoch 61: val_loss did not improve from 0.38908\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7804 - loss: 0.3989 - val_accuracy: 0.7761 - val_loss: 0.4045 - learning_rate: 0.0100\n",
      "Epoch 62/300\n",
      "\u001b[1m12/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7789 - loss: 0.3970\n",
      "Epoch 62: val_loss did not improve from 0.38908\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.7796 - loss: 0.3971 - val_accuracy: 0.7822 - val_loss: 0.3983 - learning_rate: 0.0100\n",
      "Epoch 63/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7800 - loss: 0.4101 \n",
      "Epoch 63: val_loss did not improve from 0.38908\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7788 - loss: 0.4091 - val_accuracy: 0.7846 - val_loss: 0.3918 - learning_rate: 0.0100\n",
      "Epoch 64/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7821 - loss: 0.3940 \n",
      "Epoch 64: val_loss did not improve from 0.38908\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7816 - loss: 0.3953 - val_accuracy: 0.7938 - val_loss: 0.3962 - learning_rate: 0.0100\n",
      "Epoch 65/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7810 - loss: 0.4044 \n",
      "Epoch 65: val_loss did not improve from 0.38908\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7824 - loss: 0.4030 - val_accuracy: 0.7785 - val_loss: 0.3947 - learning_rate: 0.0100\n",
      "Epoch 66/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7810 - loss: 0.3949 \n",
      "Epoch 66: val_loss did not improve from 0.38908\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7812 - loss: 0.3954 - val_accuracy: 0.7938 - val_loss: 0.3983 - learning_rate: 0.0100\n",
      "Epoch 67/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7844 - loss: 0.4055 \n",
      "Epoch 67: val_loss improved from 0.38908 to 0.38232, saving model to folds1.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7841 - loss: 0.4028 - val_accuracy: 0.7999 - val_loss: 0.3823 - learning_rate: 0.0100\n",
      "Epoch 68/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7757 - loss: 0.4042  \n",
      "Epoch 68: val_loss did not improve from 0.38232\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7789 - loss: 0.4014 - val_accuracy: 0.7968 - val_loss: 0.3906 - learning_rate: 0.0100\n",
      "Epoch 69/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7812 - loss: 0.3956 \n",
      "Epoch 69: val_loss did not improve from 0.38232\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7812 - loss: 0.3944 - val_accuracy: 0.8090 - val_loss: 0.3849 - learning_rate: 0.0100\n",
      "Epoch 70/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7918 - loss: 0.3877  \n",
      "Epoch 70: val_loss improved from 0.38232 to 0.38223, saving model to folds1.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7901 - loss: 0.3880 - val_accuracy: 0.7944 - val_loss: 0.3822 - learning_rate: 0.0100\n",
      "Epoch 71/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7935 - loss: 0.3882 \n",
      "Epoch 71: val_loss did not improve from 0.38223\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7927 - loss: 0.3888 - val_accuracy: 0.7871 - val_loss: 0.3911 - learning_rate: 0.0100\n",
      "Epoch 72/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7801 - loss: 0.3960 \n",
      "Epoch 72: val_loss did not improve from 0.38223\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7798 - loss: 0.3957 - val_accuracy: 0.7834 - val_loss: 0.3913 - learning_rate: 0.0100\n",
      "Epoch 73/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7801 - loss: 0.4033 \n",
      "Epoch 73: val_loss did not improve from 0.38223\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7811 - loss: 0.4017 - val_accuracy: 0.7932 - val_loss: 0.3913 - learning_rate: 0.0100\n",
      "Epoch 74/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7893 - loss: 0.3930  \n",
      "Epoch 74: val_loss improved from 0.38223 to 0.38209, saving model to folds1.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7895 - loss: 0.3926 - val_accuracy: 0.7822 - val_loss: 0.3821 - learning_rate: 0.0100\n",
      "Epoch 75/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7869 - loss: 0.3902 \n",
      "Epoch 75: val_loss improved from 0.38209 to 0.38000, saving model to folds1.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7870 - loss: 0.3900 - val_accuracy: 0.7980 - val_loss: 0.3800 - learning_rate: 0.0100\n",
      "Epoch 76/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7896 - loss: 0.3862  \n",
      "Epoch 76: val_loss did not improve from 0.38000\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7883 - loss: 0.3886 - val_accuracy: 0.7932 - val_loss: 0.3979 - learning_rate: 0.0100\n",
      "Epoch 77/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7774 - loss: 0.3978 \n",
      "Epoch 77: val_loss improved from 0.38000 to 0.37690, saving model to folds1.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7777 - loss: 0.3971 - val_accuracy: 0.7907 - val_loss: 0.3769 - learning_rate: 0.0100\n",
      "Epoch 78/300\n",
      "\u001b[1m12/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.7932 - loss: 0.3800\n",
      "Epoch 78: val_loss did not improve from 0.37690\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.7928 - loss: 0.3810 - val_accuracy: 0.7938 - val_loss: 0.3827 - learning_rate: 0.0100\n",
      "Epoch 79/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7883 - loss: 0.3862  \n",
      "Epoch 79: val_loss did not improve from 0.37690\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7888 - loss: 0.3852 - val_accuracy: 0.7950 - val_loss: 0.3797 - learning_rate: 0.0100\n",
      "Epoch 80/300\n",
      "\u001b[1m13/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.7888 - loss: 0.3892\n",
      "Epoch 80: val_loss improved from 0.37690 to 0.37008, saving model to folds1.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.7891 - loss: 0.3884 - val_accuracy: 0.8035 - val_loss: 0.3701 - learning_rate: 0.0100\n",
      "Epoch 81/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7893 - loss: 0.3849 \n",
      "Epoch 81: val_loss did not improve from 0.37008\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7890 - loss: 0.3838 - val_accuracy: 0.8023 - val_loss: 0.3756 - learning_rate: 0.0100\n",
      "Epoch 82/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7864 - loss: 0.3845 \n",
      "Epoch 82: val_loss did not improve from 0.37008\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7884 - loss: 0.3836 - val_accuracy: 0.7877 - val_loss: 0.3791 - learning_rate: 0.0100\n",
      "Epoch 83/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.7870 - loss: 0.3839\n",
      "Epoch 83: val_loss did not improve from 0.37008\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.7870 - loss: 0.3839 - val_accuracy: 0.8041 - val_loss: 0.3720 - learning_rate: 0.0100\n",
      "Epoch 84/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7880 - loss: 0.3826  \n",
      "Epoch 84: val_loss improved from 0.37008 to 0.36954, saving model to folds1.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7891 - loss: 0.3817 - val_accuracy: 0.8084 - val_loss: 0.3695 - learning_rate: 0.0100\n",
      "Epoch 85/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7907 - loss: 0.3779 \n",
      "Epoch 85: val_loss did not improve from 0.36954\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7917 - loss: 0.3778 - val_accuracy: 0.7944 - val_loss: 0.3715 - learning_rate: 0.0100\n",
      "Epoch 86/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7907 - loss: 0.3756 \n",
      "Epoch 86: val_loss improved from 0.36954 to 0.36588, saving model to folds1.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7908 - loss: 0.3757 - val_accuracy: 0.8084 - val_loss: 0.3659 - learning_rate: 0.0100\n",
      "Epoch 87/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7993 - loss: 0.3778 \n",
      "Epoch 87: val_loss did not improve from 0.36588\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7963 - loss: 0.3779 - val_accuracy: 0.7956 - val_loss: 0.3681 - learning_rate: 0.0100\n",
      "Epoch 88/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7806 - loss: 0.3772 \n",
      "Epoch 88: val_loss did not improve from 0.36588\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7837 - loss: 0.3766 - val_accuracy: 0.8048 - val_loss: 0.3779 - learning_rate: 0.0100\n",
      "Epoch 89/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7842 - loss: 0.3885 \n",
      "Epoch 89: val_loss did not improve from 0.36588\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7868 - loss: 0.3853 - val_accuracy: 0.8011 - val_loss: 0.3683 - learning_rate: 0.0100\n",
      "Epoch 90/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7919 - loss: 0.3728 \n",
      "Epoch 90: val_loss did not improve from 0.36588\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7910 - loss: 0.3748 - val_accuracy: 0.7993 - val_loss: 0.3680 - learning_rate: 0.0100\n",
      "Epoch 91/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7900 - loss: 0.3758 \n",
      "Epoch 91: val_loss improved from 0.36588 to 0.36440, saving model to folds1.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7909 - loss: 0.3766 - val_accuracy: 0.8115 - val_loss: 0.3644 - learning_rate: 0.0100\n",
      "Epoch 92/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7824 - loss: 0.3827 \n",
      "Epoch 92: val_loss improved from 0.36440 to 0.36146, saving model to folds1.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7837 - loss: 0.3816 - val_accuracy: 0.8023 - val_loss: 0.3615 - learning_rate: 0.0100\n",
      "Epoch 93/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7864 - loss: 0.3735 \n",
      "Epoch 93: val_loss did not improve from 0.36146\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7888 - loss: 0.3741 - val_accuracy: 0.7962 - val_loss: 0.3629 - learning_rate: 0.0100\n",
      "Epoch 94/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7952 - loss: 0.3742 \n",
      "Epoch 94: val_loss did not improve from 0.36146\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7946 - loss: 0.3748 - val_accuracy: 0.8023 - val_loss: 0.3666 - learning_rate: 0.0100\n",
      "Epoch 95/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7893 - loss: 0.3804 \n",
      "Epoch 95: val_loss improved from 0.36146 to 0.36032, saving model to folds1.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7919 - loss: 0.3774 - val_accuracy: 0.7974 - val_loss: 0.3603 - learning_rate: 0.0100\n",
      "Epoch 96/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8053 - loss: 0.3656 \n",
      "Epoch 96: val_loss improved from 0.36032 to 0.35919, saving model to folds1.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8028 - loss: 0.3668 - val_accuracy: 0.8023 - val_loss: 0.3592 - learning_rate: 0.0100\n",
      "Epoch 97/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7973 - loss: 0.3687 \n",
      "Epoch 97: val_loss did not improve from 0.35919\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7972 - loss: 0.3684 - val_accuracy: 0.7956 - val_loss: 0.3620 - learning_rate: 0.0100\n",
      "Epoch 98/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7987 - loss: 0.3656 \n",
      "Epoch 98: val_loss did not improve from 0.35919\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7957 - loss: 0.3694 - val_accuracy: 0.7944 - val_loss: 0.3699 - learning_rate: 0.0100\n",
      "Epoch 99/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7850 - loss: 0.3808  \n",
      "Epoch 99: val_loss did not improve from 0.35919\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7846 - loss: 0.3822 - val_accuracy: 0.7974 - val_loss: 0.3689 - learning_rate: 0.0100\n",
      "Epoch 100/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7822 - loss: 0.3906 \n",
      "Epoch 100: val_loss did not improve from 0.35919\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7838 - loss: 0.3879 - val_accuracy: 0.7999 - val_loss: 0.3684 - learning_rate: 0.0100\n",
      "Epoch 101/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7981 - loss: 0.3813 \n",
      "Epoch 101: val_loss did not improve from 0.35919\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7967 - loss: 0.3805 - val_accuracy: 0.7926 - val_loss: 0.3660 - learning_rate: 0.0100\n",
      "Epoch 102/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7848 - loss: 0.3841 \n",
      "Epoch 102: val_loss did not improve from 0.35919\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7874 - loss: 0.3815 - val_accuracy: 0.8017 - val_loss: 0.3610 - learning_rate: 0.0100\n",
      "Epoch 103/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7963 - loss: 0.3745 \n",
      "Epoch 103: val_loss did not improve from 0.35919\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7971 - loss: 0.3715 - val_accuracy: 0.8054 - val_loss: 0.3626 - learning_rate: 0.0100\n",
      "Epoch 104/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7985 - loss: 0.3620  \n",
      "Epoch 104: val_loss did not improve from 0.35919\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7969 - loss: 0.3646 - val_accuracy: 0.8157 - val_loss: 0.3609 - learning_rate: 0.0100\n",
      "Epoch 105/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8039 - loss: 0.3695 \n",
      "Epoch 105: val_loss improved from 0.35919 to 0.35822, saving model to folds1.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8018 - loss: 0.3684 - val_accuracy: 0.8103 - val_loss: 0.3582 - learning_rate: 0.0100\n",
      "Epoch 106/300\n",
      "\u001b[1m13/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7960 - loss: 0.3663\n",
      "Epoch 106: val_loss did not improve from 0.35822\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7962 - loss: 0.3665 - val_accuracy: 0.7993 - val_loss: 0.3678 - learning_rate: 0.0100\n",
      "Epoch 107/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7946 - loss: 0.3681\n",
      "Epoch 107: val_loss did not improve from 0.35822\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7946 - loss: 0.3680 - val_accuracy: 0.8078 - val_loss: 0.3670 - learning_rate: 0.0100\n",
      "Epoch 108/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7959 - loss: 0.3697\n",
      "Epoch 108: val_loss improved from 0.35822 to 0.35779, saving model to folds1.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.7956 - loss: 0.3698 - val_accuracy: 0.8127 - val_loss: 0.3578 - learning_rate: 0.0100\n",
      "Epoch 109/300\n",
      "\u001b[1m13/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7957 - loss: 0.3677\n",
      "Epoch 109: val_loss did not improve from 0.35779\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.7960 - loss: 0.3678 - val_accuracy: 0.8005 - val_loss: 0.3580 - learning_rate: 0.0100\n",
      "Epoch 110/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7970 - loss: 0.3652\n",
      "Epoch 110: val_loss did not improve from 0.35779\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7971 - loss: 0.3653 - val_accuracy: 0.8066 - val_loss: 0.3617 - learning_rate: 0.0100\n",
      "Epoch 111/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7899 - loss: 0.3694\n",
      "Epoch 111: val_loss did not improve from 0.35779\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7906 - loss: 0.3687 - val_accuracy: 0.8029 - val_loss: 0.3682 - learning_rate: 0.0100\n",
      "Epoch 112/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7837 - loss: 0.3788\n",
      "Epoch 112: val_loss did not improve from 0.35779\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7840 - loss: 0.3786 - val_accuracy: 0.7913 - val_loss: 0.3727 - learning_rate: 0.0100\n",
      "Epoch 113/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7954 - loss: 0.3731\n",
      "Epoch 113: val_loss did not improve from 0.35779\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7951 - loss: 0.3731 - val_accuracy: 0.7987 - val_loss: 0.3746 - learning_rate: 0.0100\n",
      "Epoch 114/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7955 - loss: 0.3715\n",
      "Epoch 114: val_loss did not improve from 0.35779\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7954 - loss: 0.3714 - val_accuracy: 0.8048 - val_loss: 0.3596 - learning_rate: 0.0100\n",
      "Epoch 115/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8054 - loss: 0.3622 \n",
      "Epoch 115: val_loss did not improve from 0.35779\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8026 - loss: 0.3639 - val_accuracy: 0.8005 - val_loss: 0.3601 - learning_rate: 0.0100\n",
      "Epoch 116/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7988 - loss: 0.3645\n",
      "Epoch 116: val_loss did not improve from 0.35779\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.7990 - loss: 0.3646 - val_accuracy: 0.7944 - val_loss: 0.3628 - learning_rate: 0.0100\n",
      "Epoch 117/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7930 - loss: 0.3682\n",
      "Epoch 117: val_loss improved from 0.35779 to 0.35596, saving model to folds1.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.7933 - loss: 0.3680 - val_accuracy: 0.8066 - val_loss: 0.3560 - learning_rate: 0.0100\n",
      "Epoch 118/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7987 - loss: 0.3621\n",
      "Epoch 118: val_loss improved from 0.35596 to 0.35476, saving model to folds1.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.7986 - loss: 0.3620 - val_accuracy: 0.8072 - val_loss: 0.3548 - learning_rate: 0.0100\n",
      "Epoch 119/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8051 - loss: 0.3646 \n",
      "Epoch 119: val_loss improved from 0.35476 to 0.35221, saving model to folds1.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8042 - loss: 0.3631 - val_accuracy: 0.8041 - val_loss: 0.3522 - learning_rate: 0.0100\n",
      "Epoch 120/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7928 - loss: 0.3683 \n",
      "Epoch 120: val_loss did not improve from 0.35221\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7954 - loss: 0.3660 - val_accuracy: 0.8005 - val_loss: 0.3549 - learning_rate: 0.0100\n",
      "Epoch 121/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7920 - loss: 0.3665  \n",
      "Epoch 121: val_loss did not improve from 0.35221\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7927 - loss: 0.3688 - val_accuracy: 0.8023 - val_loss: 0.3663 - learning_rate: 0.0100\n",
      "Epoch 122/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7973 - loss: 0.3735 \n",
      "Epoch 122: val_loss improved from 0.35221 to 0.35075, saving model to folds1.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7954 - loss: 0.3738 - val_accuracy: 0.7993 - val_loss: 0.3507 - learning_rate: 0.0100\n",
      "Epoch 123/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7980 - loss: 0.3606 \n",
      "Epoch 123: val_loss did not improve from 0.35075\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7990 - loss: 0.3605 - val_accuracy: 0.7999 - val_loss: 0.3560 - learning_rate: 0.0100\n",
      "Epoch 124/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7944 - loss: 0.3672 \n",
      "Epoch 124: val_loss improved from 0.35075 to 0.34908, saving model to folds1.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7970 - loss: 0.3647 - val_accuracy: 0.8084 - val_loss: 0.3491 - learning_rate: 0.0100\n",
      "Epoch 125/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8080 - loss: 0.3487 \n",
      "Epoch 125: val_loss did not improve from 0.34908\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8045 - loss: 0.3524 - val_accuracy: 0.8188 - val_loss: 0.3546 - learning_rate: 0.0100\n",
      "Epoch 126/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8040 - loss: 0.3550 \n",
      "Epoch 126: val_loss did not improve from 0.34908\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8025 - loss: 0.3562 - val_accuracy: 0.8096 - val_loss: 0.3507 - learning_rate: 0.0100\n",
      "Epoch 127/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8064 - loss: 0.3646  \n",
      "Epoch 127: val_loss did not improve from 0.34908\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8054 - loss: 0.3621 - val_accuracy: 0.8072 - val_loss: 0.3526 - learning_rate: 0.0100\n",
      "Epoch 128/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7976 - loss: 0.3644 \n",
      "Epoch 128: val_loss did not improve from 0.34908\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7982 - loss: 0.3633 - val_accuracy: 0.7956 - val_loss: 0.3501 - learning_rate: 0.0100\n",
      "Epoch 129/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7991 - loss: 0.3614 \n",
      "Epoch 129: val_loss did not improve from 0.34908\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7996 - loss: 0.3615 - val_accuracy: 0.8011 - val_loss: 0.3508 - learning_rate: 0.0100\n",
      "Epoch 130/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7950 - loss: 0.3682 \n",
      "Epoch 130: val_loss did not improve from 0.34908\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7958 - loss: 0.3661 - val_accuracy: 0.8279 - val_loss: 0.3575 - learning_rate: 0.0100\n",
      "Epoch 131/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8007 - loss: 0.3609  \n",
      "Epoch 131: val_loss did not improve from 0.34908\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7987 - loss: 0.3612 - val_accuracy: 0.8115 - val_loss: 0.3530 - learning_rate: 0.0100\n",
      "Epoch 132/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7977 - loss: 0.3664 \n",
      "Epoch 132: val_loss did not improve from 0.34908\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7987 - loss: 0.3638 - val_accuracy: 0.8072 - val_loss: 0.3593 - learning_rate: 0.0100\n",
      "Epoch 133/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7917 - loss: 0.3697 \n",
      "Epoch 133: val_loss improved from 0.34908 to 0.34899, saving model to folds1.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7925 - loss: 0.3679 - val_accuracy: 0.8066 - val_loss: 0.3490 - learning_rate: 0.0100\n",
      "Epoch 134/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7913 - loss: 0.3616 \n",
      "Epoch 134: val_loss improved from 0.34899 to 0.34697, saving model to folds1.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7935 - loss: 0.3599 - val_accuracy: 0.8121 - val_loss: 0.3470 - learning_rate: 0.0100\n",
      "Epoch 135/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8077 - loss: 0.3584 \n",
      "Epoch 135: val_loss did not improve from 0.34697\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8066 - loss: 0.3578 - val_accuracy: 0.8084 - val_loss: 0.3486 - learning_rate: 0.0100\n",
      "Epoch 136/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7931 - loss: 0.3634  \n",
      "Epoch 136: val_loss did not improve from 0.34697\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7947 - loss: 0.3625 - val_accuracy: 0.8096 - val_loss: 0.3545 - learning_rate: 0.0100\n",
      "Epoch 137/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7979 - loss: 0.3666 \n",
      "Epoch 137: val_loss did not improve from 0.34697\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7976 - loss: 0.3652 - val_accuracy: 0.8011 - val_loss: 0.3620 - learning_rate: 0.0100\n",
      "Epoch 138/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8075 - loss: 0.3465 \n",
      "Epoch 138: val_loss did not improve from 0.34697\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8056 - loss: 0.3508 - val_accuracy: 0.7950 - val_loss: 0.3673 - learning_rate: 0.0100\n",
      "Epoch 139/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7993 - loss: 0.3659 \n",
      "Epoch 139: val_loss did not improve from 0.34697\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7987 - loss: 0.3646 - val_accuracy: 0.8005 - val_loss: 0.3527 - learning_rate: 0.0100\n",
      "Epoch 140/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7904 - loss: 0.3691 \n",
      "Epoch 140: val_loss did not improve from 0.34697\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7921 - loss: 0.3672 - val_accuracy: 0.7932 - val_loss: 0.3605 - learning_rate: 0.0100\n",
      "Epoch 141/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7867 - loss: 0.3658 \n",
      "Epoch 141: val_loss did not improve from 0.34697\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7892 - loss: 0.3649 - val_accuracy: 0.8005 - val_loss: 0.3526 - learning_rate: 0.0100\n",
      "Epoch 142/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8011 - loss: 0.3522 \n",
      "Epoch 142: val_loss did not improve from 0.34697\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8014 - loss: 0.3530 - val_accuracy: 0.8066 - val_loss: 0.3500 - learning_rate: 0.0100\n",
      "Epoch 143/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8004 - loss: 0.3568 \n",
      "Epoch 143: val_loss improved from 0.34697 to 0.34555, saving model to folds1.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8000 - loss: 0.3554 - val_accuracy: 0.8151 - val_loss: 0.3456 - learning_rate: 0.0100\n",
      "Epoch 144/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7932 - loss: 0.3627  \n",
      "Epoch 144: val_loss did not improve from 0.34555\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7953 - loss: 0.3611 - val_accuracy: 0.8121 - val_loss: 0.3649 - learning_rate: 0.0100\n",
      "Epoch 145/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8081 - loss: 0.3626 \n",
      "Epoch 145: val_loss did not improve from 0.34555\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8052 - loss: 0.3619 - val_accuracy: 0.8072 - val_loss: 0.3520 - learning_rate: 0.0100\n",
      "Epoch 146/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7995 - loss: 0.3551 \n",
      "Epoch 146: val_loss improved from 0.34555 to 0.34458, saving model to folds1.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7998 - loss: 0.3545 - val_accuracy: 0.8023 - val_loss: 0.3446 - learning_rate: 0.0100\n",
      "Epoch 147/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8054 - loss: 0.3501 \n",
      "Epoch 147: val_loss did not improve from 0.34458\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8042 - loss: 0.3518 - val_accuracy: 0.8011 - val_loss: 0.3575 - learning_rate: 0.0100\n",
      "Epoch 148/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7934 - loss: 0.3600 \n",
      "Epoch 148: val_loss did not improve from 0.34458\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7942 - loss: 0.3587 - val_accuracy: 0.8145 - val_loss: 0.3449 - learning_rate: 0.0100\n",
      "Epoch 149/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7997 - loss: 0.3515 \n",
      "Epoch 149: val_loss did not improve from 0.34458\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8003 - loss: 0.3523 - val_accuracy: 0.8005 - val_loss: 0.3473 - learning_rate: 0.0100\n",
      "Epoch 150/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8041 - loss: 0.3521 \n",
      "Epoch 150: val_loss did not improve from 0.34458\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8026 - loss: 0.3535 - val_accuracy: 0.8048 - val_loss: 0.3457 - learning_rate: 0.0100\n",
      "Epoch 151/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7931 - loss: 0.3568  \n",
      "Epoch 151: val_loss improved from 0.34458 to 0.34439, saving model to folds1.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7944 - loss: 0.3569 - val_accuracy: 0.8017 - val_loss: 0.3444 - learning_rate: 0.0100\n",
      "Epoch 152/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7971 - loss: 0.3598 \n",
      "Epoch 152: val_loss did not improve from 0.34439\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7977 - loss: 0.3585 - val_accuracy: 0.7956 - val_loss: 0.3593 - learning_rate: 0.0100\n",
      "Epoch 153/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7989 - loss: 0.3597 \n",
      "Epoch 153: val_loss did not improve from 0.34439\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7983 - loss: 0.3594 - val_accuracy: 0.8103 - val_loss: 0.3449 - learning_rate: 0.0100\n",
      "Epoch 154/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7975 - loss: 0.3558 \n",
      "Epoch 154: val_loss improved from 0.34439 to 0.34403, saving model to folds1.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7990 - loss: 0.3550 - val_accuracy: 0.8133 - val_loss: 0.3440 - learning_rate: 0.0100\n",
      "Epoch 155/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8013 - loss: 0.3512 \n",
      "Epoch 155: val_loss did not improve from 0.34403\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8006 - loss: 0.3527 - val_accuracy: 0.8121 - val_loss: 0.3488 - learning_rate: 0.0100\n",
      "Epoch 156/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7946 - loss: 0.3615 \n",
      "Epoch 156: val_loss improved from 0.34403 to 0.34301, saving model to folds1.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7965 - loss: 0.3583 - val_accuracy: 0.8072 - val_loss: 0.3430 - learning_rate: 0.0100\n",
      "Epoch 157/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8045 - loss: 0.3538 \n",
      "Epoch 157: val_loss improved from 0.34301 to 0.33782, saving model to folds1.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8042 - loss: 0.3535 - val_accuracy: 0.8176 - val_loss: 0.3378 - learning_rate: 0.0100\n",
      "Epoch 158/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7970 - loss: 0.3545  \n",
      "Epoch 158: val_loss did not improve from 0.33782\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7993 - loss: 0.3529 - val_accuracy: 0.8115 - val_loss: 0.3539 - learning_rate: 0.0100\n",
      "Epoch 159/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7980 - loss: 0.3574 \n",
      "Epoch 159: val_loss did not improve from 0.33782\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7984 - loss: 0.3568 - val_accuracy: 0.8139 - val_loss: 0.3510 - learning_rate: 0.0100\n",
      "Epoch 160/300\n",
      "\u001b[1m 7/15\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8003 - loss: 0.3538 \n",
      "Epoch 160: val_loss did not improve from 0.33782\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8022 - loss: 0.3530 - val_accuracy: 0.8072 - val_loss: 0.3460 - learning_rate: 0.0100\n",
      "Epoch 161/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7971 - loss: 0.3551  \n",
      "Epoch 161: val_loss did not improve from 0.33782\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7967 - loss: 0.3553 - val_accuracy: 0.8048 - val_loss: 0.3527 - learning_rate: 0.0100\n",
      "Epoch 162/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7936 - loss: 0.3578  \n",
      "Epoch 162: val_loss did not improve from 0.33782\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7961 - loss: 0.3563 - val_accuracy: 0.8151 - val_loss: 0.3408 - learning_rate: 0.0100\n",
      "Epoch 163/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8037 - loss: 0.3487 \n",
      "Epoch 163: val_loss did not improve from 0.33782\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8018 - loss: 0.3503 - val_accuracy: 0.8145 - val_loss: 0.3422 - learning_rate: 0.0100\n",
      "Epoch 164/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7852 - loss: 0.3645 \n",
      "Epoch 164: val_loss did not improve from 0.33782\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7906 - loss: 0.3596 - val_accuracy: 0.7999 - val_loss: 0.3386 - learning_rate: 0.0100\n",
      "Epoch 165/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8026 - loss: 0.3462 \n",
      "Epoch 165: val_loss did not improve from 0.33782\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8028 - loss: 0.3474 - val_accuracy: 0.8072 - val_loss: 0.3388 - learning_rate: 0.0100\n",
      "Epoch 166/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7999 - loss: 0.3506 \n",
      "Epoch 166: val_loss did not improve from 0.33782\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8011 - loss: 0.3503 - val_accuracy: 0.8206 - val_loss: 0.3405 - learning_rate: 0.0100\n",
      "Epoch 167/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7986 - loss: 0.3552\n",
      "Epoch 167: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
      "\n",
      "Epoch 167: val_loss did not improve from 0.33782\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7989 - loss: 0.3549 - val_accuracy: 0.8017 - val_loss: 0.3436 - learning_rate: 0.0100\n",
      "Epoch 168/300\n",
      "\u001b[1m13/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8068 - loss: 0.3415 \n",
      "Epoch 168: val_loss improved from 0.33782 to 0.33745, saving model to folds1.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.8063 - loss: 0.3425 - val_accuracy: 0.8054 - val_loss: 0.3374 - learning_rate: 0.0050\n",
      "Epoch 169/300\n",
      "\u001b[1m13/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8017 - loss: 0.3488\n",
      "Epoch 169: val_loss did not improve from 0.33745\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.8018 - loss: 0.3482 - val_accuracy: 0.8164 - val_loss: 0.3379 - learning_rate: 0.0050\n",
      "Epoch 170/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8090 - loss: 0.3421\n",
      "Epoch 170: val_loss improved from 0.33745 to 0.33502, saving model to folds1.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.8087 - loss: 0.3424 - val_accuracy: 0.8109 - val_loss: 0.3350 - learning_rate: 0.0050\n",
      "Epoch 171/300\n",
      "\u001b[1m12/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8017 - loss: 0.3492\n",
      "Epoch 171: val_loss did not improve from 0.33502\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.8020 - loss: 0.3479 - val_accuracy: 0.8151 - val_loss: 0.3393 - learning_rate: 0.0050\n",
      "Epoch 172/300\n",
      "\u001b[1m13/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8082 - loss: 0.3450\n",
      "Epoch 172: val_loss did not improve from 0.33502\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.8082 - loss: 0.3451 - val_accuracy: 0.8060 - val_loss: 0.3373 - learning_rate: 0.0050\n",
      "Epoch 173/300\n",
      "\u001b[1m12/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8013 - loss: 0.3423\n",
      "Epoch 173: val_loss did not improve from 0.33502\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.8019 - loss: 0.3432 - val_accuracy: 0.8115 - val_loss: 0.3374 - learning_rate: 0.0050\n",
      "Epoch 174/300\n",
      "\u001b[1m13/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8055 - loss: 0.3467 \n",
      "Epoch 174: val_loss did not improve from 0.33502\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8051 - loss: 0.3467 - val_accuracy: 0.8029 - val_loss: 0.3357 - learning_rate: 0.0050\n",
      "Epoch 175/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7978 - loss: 0.3524 \n",
      "Epoch 175: val_loss did not improve from 0.33502\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7994 - loss: 0.3504 - val_accuracy: 0.8005 - val_loss: 0.3371 - learning_rate: 0.0050\n",
      "Epoch 176/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8019 - loss: 0.3432 \n",
      "Epoch 176: val_loss did not improve from 0.33502\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8037 - loss: 0.3441 - val_accuracy: 0.8109 - val_loss: 0.3364 - learning_rate: 0.0050\n",
      "Epoch 177/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8003 - loss: 0.3475 \n",
      "Epoch 177: val_loss did not improve from 0.33502\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8016 - loss: 0.3461 - val_accuracy: 0.8090 - val_loss: 0.3354 - learning_rate: 0.0050\n",
      "Epoch 178/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8024 - loss: 0.3427 \n",
      "Epoch 178: val_loss improved from 0.33502 to 0.33497, saving model to folds1.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8041 - loss: 0.3430 - val_accuracy: 0.8103 - val_loss: 0.3350 - learning_rate: 0.0050\n",
      "Epoch 179/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8048 - loss: 0.3495 \n",
      "Epoch 179: val_loss did not improve from 0.33497\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8062 - loss: 0.3475 - val_accuracy: 0.8109 - val_loss: 0.3365 - learning_rate: 0.0050\n",
      "Epoch 180/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8002 - loss: 0.3470 \n",
      "Epoch 180: val_loss improved from 0.33497 to 0.33481, saving model to folds1.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8024 - loss: 0.3459 - val_accuracy: 0.8157 - val_loss: 0.3348 - learning_rate: 0.0050\n",
      "Epoch 181/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8108 - loss: 0.3374 \n",
      "Epoch 181: val_loss did not improve from 0.33481\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8093 - loss: 0.3396 - val_accuracy: 0.8096 - val_loss: 0.3358 - learning_rate: 0.0050\n",
      "Epoch 182/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8106 - loss: 0.3373 \n",
      "Epoch 182: val_loss did not improve from 0.33481\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8083 - loss: 0.3394 - val_accuracy: 0.8090 - val_loss: 0.3399 - learning_rate: 0.0050\n",
      "Epoch 183/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7960 - loss: 0.3540 \n",
      "Epoch 183: val_loss did not improve from 0.33481\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8003 - loss: 0.3509 - val_accuracy: 0.8078 - val_loss: 0.3352 - learning_rate: 0.0050\n",
      "Epoch 184/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8060 - loss: 0.3415  \n",
      "Epoch 184: val_loss did not improve from 0.33481\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8064 - loss: 0.3422 - val_accuracy: 0.8176 - val_loss: 0.3391 - learning_rate: 0.0050\n",
      "Epoch 185/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8061 - loss: 0.3438 \n",
      "Epoch 185: val_loss improved from 0.33481 to 0.33392, saving model to folds1.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8061 - loss: 0.3437 - val_accuracy: 0.8096 - val_loss: 0.3339 - learning_rate: 0.0050\n",
      "Epoch 186/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8128 - loss: 0.3377 \n",
      "Epoch 186: val_loss did not improve from 0.33392\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8103 - loss: 0.3398 - val_accuracy: 0.8109 - val_loss: 0.3341 - learning_rate: 0.0050\n",
      "Epoch 187/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7987 - loss: 0.3534 \n",
      "Epoch 187: val_loss did not improve from 0.33392\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8002 - loss: 0.3497 - val_accuracy: 0.8170 - val_loss: 0.3348 - learning_rate: 0.0050\n",
      "Epoch 188/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8119 - loss: 0.3432 \n",
      "Epoch 188: val_loss did not improve from 0.33392\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8106 - loss: 0.3433 - val_accuracy: 0.8139 - val_loss: 0.3371 - learning_rate: 0.0050\n",
      "Epoch 189/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8059 - loss: 0.3432 \n",
      "Epoch 189: val_loss did not improve from 0.33392\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8058 - loss: 0.3434 - val_accuracy: 0.8072 - val_loss: 0.3351 - learning_rate: 0.0050\n",
      "Epoch 190/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8093 - loss: 0.3390 \n",
      "Epoch 190: val_loss improved from 0.33392 to 0.33339, saving model to folds1.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8089 - loss: 0.3400 - val_accuracy: 0.8145 - val_loss: 0.3334 - learning_rate: 0.0050\n",
      "Epoch 191/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8073 - loss: 0.3396 \n",
      "Epoch 191: val_loss did not improve from 0.33339\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8060 - loss: 0.3408 - val_accuracy: 0.8072 - val_loss: 0.3348 - learning_rate: 0.0050\n",
      "Epoch 192/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8003 - loss: 0.3469 \n",
      "Epoch 192: val_loss did not improve from 0.33339\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8017 - loss: 0.3457 - val_accuracy: 0.8035 - val_loss: 0.3362 - learning_rate: 0.0050\n",
      "Epoch 193/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8050 - loss: 0.3442 \n",
      "Epoch 193: val_loss did not improve from 0.33339\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8049 - loss: 0.3436 - val_accuracy: 0.8115 - val_loss: 0.3370 - learning_rate: 0.0050\n",
      "Epoch 194/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8003 - loss: 0.3479 \n",
      "Epoch 194: val_loss did not improve from 0.33339\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8010 - loss: 0.3459 - val_accuracy: 0.8176 - val_loss: 0.3340 - learning_rate: 0.0050\n",
      "Epoch 195/300\n",
      "\u001b[1m 7/15\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8051 - loss: 0.3464 \n",
      "Epoch 195: val_loss improved from 0.33339 to 0.33286, saving model to folds1.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8052 - loss: 0.3446 - val_accuracy: 0.8054 - val_loss: 0.3329 - learning_rate: 0.0050\n",
      "Epoch 196/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8072 - loss: 0.3388 \n",
      "Epoch 196: val_loss did not improve from 0.33286\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8065 - loss: 0.3396 - val_accuracy: 0.8115 - val_loss: 0.3344 - learning_rate: 0.0050\n",
      "Epoch 197/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8111 - loss: 0.3377 \n",
      "Epoch 197: val_loss did not improve from 0.33286\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8092 - loss: 0.3397 - val_accuracy: 0.8084 - val_loss: 0.3330 - learning_rate: 0.0050\n",
      "Epoch 198/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8052 - loss: 0.3448  \n",
      "Epoch 198: val_loss did not improve from 0.33286\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8057 - loss: 0.3441 - val_accuracy: 0.8157 - val_loss: 0.3412 - learning_rate: 0.0050\n",
      "Epoch 199/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8074 - loss: 0.3450 \n",
      "Epoch 199: val_loss did not improve from 0.33286\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8066 - loss: 0.3451 - val_accuracy: 0.8133 - val_loss: 0.3349 - learning_rate: 0.0050\n",
      "Epoch 200/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8009 - loss: 0.3534 \n",
      "Epoch 200: val_loss did not improve from 0.33286\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8016 - loss: 0.3509 - val_accuracy: 0.8151 - val_loss: 0.3379 - learning_rate: 0.0050\n",
      "Epoch 201/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8034 - loss: 0.3457 \n",
      "Epoch 201: val_loss did not improve from 0.33286\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8035 - loss: 0.3446 - val_accuracy: 0.8139 - val_loss: 0.3334 - learning_rate: 0.0050\n",
      "Epoch 202/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8034 - loss: 0.3421 \n",
      "Epoch 202: val_loss did not improve from 0.33286\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8047 - loss: 0.3421 - val_accuracy: 0.8164 - val_loss: 0.3365 - learning_rate: 0.0050\n",
      "Epoch 203/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8090 - loss: 0.3460  \n",
      "Epoch 203: val_loss did not improve from 0.33286\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8074 - loss: 0.3461 - val_accuracy: 0.8145 - val_loss: 0.3348 - learning_rate: 0.0050\n",
      "Epoch 204/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8084 - loss: 0.3418 \n",
      "Epoch 204: val_loss did not improve from 0.33286\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8080 - loss: 0.3413 - val_accuracy: 0.8139 - val_loss: 0.3340 - learning_rate: 0.0050\n",
      "Epoch 205/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8026 - loss: 0.3428  \n",
      "Epoch 205: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "\n",
      "Epoch 205: val_loss did not improve from 0.33286\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8031 - loss: 0.3430 - val_accuracy: 0.8206 - val_loss: 0.3369 - learning_rate: 0.0050\n",
      "Epoch 206/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8139 - loss: 0.3359 \n",
      "Epoch 206: val_loss did not improve from 0.33286\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8116 - loss: 0.3379 - val_accuracy: 0.8176 - val_loss: 0.3338 - learning_rate: 0.0025\n",
      "Epoch 207/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8116 - loss: 0.3420 \n",
      "Epoch 207: val_loss improved from 0.33286 to 0.33104, saving model to folds1.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8099 - loss: 0.3409 - val_accuracy: 0.8096 - val_loss: 0.3310 - learning_rate: 0.0025\n",
      "Epoch 208/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7999 - loss: 0.3442 \n",
      "Epoch 208: val_loss did not improve from 0.33104\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8035 - loss: 0.3425 - val_accuracy: 0.8225 - val_loss: 0.3327 - learning_rate: 0.0025\n",
      "Epoch 209/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8055 - loss: 0.3400 \n",
      "Epoch 209: val_loss did not improve from 0.33104\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8050 - loss: 0.3407 - val_accuracy: 0.8273 - val_loss: 0.3369 - learning_rate: 0.0025\n",
      "Epoch 210/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8083 - loss: 0.3447 \n",
      "Epoch 210: val_loss did not improve from 0.33104\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8089 - loss: 0.3427 - val_accuracy: 0.8164 - val_loss: 0.3323 - learning_rate: 0.0025\n",
      "Epoch 211/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8125 - loss: 0.3412 \n",
      "Epoch 211: val_loss did not improve from 0.33104\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8117 - loss: 0.3409 - val_accuracy: 0.8096 - val_loss: 0.3315 - learning_rate: 0.0025\n",
      "Epoch 212/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8053 - loss: 0.3357 \n",
      "Epoch 212: val_loss improved from 0.33104 to 0.33027, saving model to folds1.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8066 - loss: 0.3361 - val_accuracy: 0.8096 - val_loss: 0.3303 - learning_rate: 0.0025\n",
      "Epoch 213/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8063 - loss: 0.3356  \n",
      "Epoch 213: val_loss did not improve from 0.33027\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8081 - loss: 0.3364 - val_accuracy: 0.8164 - val_loss: 0.3312 - learning_rate: 0.0025\n",
      "Epoch 214/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8054 - loss: 0.3422  \n",
      "Epoch 214: val_loss did not improve from 0.33027\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8070 - loss: 0.3405 - val_accuracy: 0.8060 - val_loss: 0.3314 - learning_rate: 0.0025\n",
      "Epoch 215/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8105 - loss: 0.3344  \n",
      "Epoch 215: val_loss improved from 0.33027 to 0.32994, saving model to folds1.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8095 - loss: 0.3363 - val_accuracy: 0.8206 - val_loss: 0.3299 - learning_rate: 0.0025\n",
      "Epoch 216/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8141 - loss: 0.3416 \n",
      "Epoch 216: val_loss did not improve from 0.32994\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8123 - loss: 0.3407 - val_accuracy: 0.8188 - val_loss: 0.3316 - learning_rate: 0.0025\n",
      "Epoch 217/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8088 - loss: 0.3393 \n",
      "Epoch 217: val_loss did not improve from 0.32994\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8088 - loss: 0.3392 - val_accuracy: 0.8145 - val_loss: 0.3301 - learning_rate: 0.0025\n",
      "Epoch 218/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8046 - loss: 0.3405 \n",
      "Epoch 218: val_loss did not improve from 0.32994\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8065 - loss: 0.3398 - val_accuracy: 0.8072 - val_loss: 0.3310 - learning_rate: 0.0025\n",
      "Epoch 219/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8042 - loss: 0.3400 \n",
      "Epoch 219: val_loss did not improve from 0.32994\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8056 - loss: 0.3394 - val_accuracy: 0.8145 - val_loss: 0.3307 - learning_rate: 0.0025\n",
      "Epoch 220/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8071 - loss: 0.3372  \n",
      "Epoch 220: val_loss did not improve from 0.32994\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8080 - loss: 0.3379 - val_accuracy: 0.8218 - val_loss: 0.3313 - learning_rate: 0.0025\n",
      "Epoch 221/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8172 - loss: 0.3324 \n",
      "Epoch 221: val_loss improved from 0.32994 to 0.32945, saving model to folds1.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8152 - loss: 0.3346 - val_accuracy: 0.8103 - val_loss: 0.3294 - learning_rate: 0.0025\n",
      "Epoch 222/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8126 - loss: 0.3347\n",
      "Epoch 222: val_loss did not improve from 0.32945\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.8120 - loss: 0.3356 - val_accuracy: 0.8206 - val_loss: 0.3315 - learning_rate: 0.0025\n",
      "Epoch 223/300\n",
      "\u001b[1m13/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8058 - loss: 0.3455\n",
      "Epoch 223: val_loss did not improve from 0.32945\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8066 - loss: 0.3442 - val_accuracy: 0.8218 - val_loss: 0.3318 - learning_rate: 0.0025\n",
      "Epoch 224/300\n",
      "\u001b[1m12/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8040 - loss: 0.3417\n",
      "Epoch 224: val_loss did not improve from 0.32945\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8052 - loss: 0.3408 - val_accuracy: 0.8267 - val_loss: 0.3327 - learning_rate: 0.0025\n",
      "Epoch 225/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8094 - loss: 0.3387\n",
      "Epoch 225: val_loss did not improve from 0.32945\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8092 - loss: 0.3387 - val_accuracy: 0.8164 - val_loss: 0.3322 - learning_rate: 0.0025\n",
      "Epoch 226/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8129 - loss: 0.3414\n",
      "Epoch 226: val_loss did not improve from 0.32945\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8116 - loss: 0.3407 - val_accuracy: 0.8243 - val_loss: 0.3322 - learning_rate: 0.0025\n",
      "Epoch 227/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8048 - loss: 0.3426\n",
      "Epoch 227: val_loss did not improve from 0.32945\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.8050 - loss: 0.3417 - val_accuracy: 0.8212 - val_loss: 0.3351 - learning_rate: 0.0025\n",
      "Epoch 228/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8120 - loss: 0.3395\n",
      "Epoch 228: val_loss did not improve from 0.32945\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8119 - loss: 0.3395 - val_accuracy: 0.8151 - val_loss: 0.3320 - learning_rate: 0.0025\n",
      "Epoch 229/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8120 - loss: 0.3400\n",
      "Epoch 229: val_loss improved from 0.32945 to 0.32914, saving model to folds1.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.8122 - loss: 0.3394 - val_accuracy: 0.8133 - val_loss: 0.3291 - learning_rate: 0.0025\n",
      "Epoch 230/300\n",
      "\u001b[1m13/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8045 - loss: 0.3357\n",
      "Epoch 230: val_loss did not improve from 0.32914\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8053 - loss: 0.3359 - val_accuracy: 0.8170 - val_loss: 0.3292 - learning_rate: 0.0025\n",
      "Epoch 231/300\n",
      "\u001b[1m13/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8194 - loss: 0.3311\n",
      "Epoch 231: val_loss did not improve from 0.32914\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8178 - loss: 0.3324 - val_accuracy: 0.8164 - val_loss: 0.3311 - learning_rate: 0.0025\n",
      "Epoch 232/300\n",
      "\u001b[1m13/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8130 - loss: 0.3381\n",
      "Epoch 232: val_loss did not improve from 0.32914\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8123 - loss: 0.3381 - val_accuracy: 0.8194 - val_loss: 0.3297 - learning_rate: 0.0025\n",
      "Epoch 233/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8141 - loss: 0.3361\n",
      "Epoch 233: val_loss did not improve from 0.32914\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8132 - loss: 0.3367 - val_accuracy: 0.8212 - val_loss: 0.3312 - learning_rate: 0.0025\n",
      "Epoch 234/300\n",
      "\u001b[1m12/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8117 - loss: 0.3471\n",
      "Epoch 234: val_loss did not improve from 0.32914\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8112 - loss: 0.3456 - val_accuracy: 0.8206 - val_loss: 0.3311 - learning_rate: 0.0025\n",
      "Epoch 235/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8043 - loss: 0.3383 \n",
      "Epoch 235: val_loss did not improve from 0.32914\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8060 - loss: 0.3375 - val_accuracy: 0.8145 - val_loss: 0.3297 - learning_rate: 0.0025\n",
      "Epoch 236/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8114 - loss: 0.3314 \n",
      "Epoch 236: val_loss improved from 0.32914 to 0.32875, saving model to folds1.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8112 - loss: 0.3336 - val_accuracy: 0.8139 - val_loss: 0.3288 - learning_rate: 0.0025\n",
      "Epoch 237/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8012 - loss: 0.3516 \n",
      "Epoch 237: val_loss did not improve from 0.32875\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8049 - loss: 0.3452 - val_accuracy: 0.8151 - val_loss: 0.3303 - learning_rate: 0.0025\n",
      "Epoch 238/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8142 - loss: 0.3290 \n",
      "Epoch 238: val_loss improved from 0.32875 to 0.32873, saving model to folds1.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8115 - loss: 0.3322 - val_accuracy: 0.8188 - val_loss: 0.3287 - learning_rate: 0.0025\n",
      "Epoch 239/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8130 - loss: 0.3365 \n",
      "Epoch 239: val_loss did not improve from 0.32873\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8122 - loss: 0.3367 - val_accuracy: 0.8164 - val_loss: 0.3301 - learning_rate: 0.0025\n",
      "Epoch 240/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8100 - loss: 0.3379 \n",
      "Epoch 240: val_loss did not improve from 0.32873\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8103 - loss: 0.3378 - val_accuracy: 0.8090 - val_loss: 0.3295 - learning_rate: 0.0025\n",
      "Epoch 241/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8004 - loss: 0.3398 \n",
      "Epoch 241: val_loss did not improve from 0.32873\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8041 - loss: 0.3388 - val_accuracy: 0.8200 - val_loss: 0.3301 - learning_rate: 0.0025\n",
      "Epoch 242/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8114 - loss: 0.3401 \n",
      "Epoch 242: val_loss improved from 0.32873 to 0.32821, saving model to folds1.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8113 - loss: 0.3387 - val_accuracy: 0.8127 - val_loss: 0.3282 - learning_rate: 0.0025\n",
      "Epoch 243/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8099 - loss: 0.3377 \n",
      "Epoch 243: val_loss did not improve from 0.32821\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8100 - loss: 0.3368 - val_accuracy: 0.8243 - val_loss: 0.3296 - learning_rate: 0.0025\n",
      "Epoch 244/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8155 - loss: 0.3359 \n",
      "Epoch 244: val_loss did not improve from 0.32821\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8133 - loss: 0.3365 - val_accuracy: 0.8176 - val_loss: 0.3286 - learning_rate: 0.0025\n",
      "Epoch 245/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8096 - loss: 0.3330 \n",
      "Epoch 245: val_loss did not improve from 0.32821\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8090 - loss: 0.3343 - val_accuracy: 0.8212 - val_loss: 0.3294 - learning_rate: 0.0025\n",
      "Epoch 246/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8131 - loss: 0.3315  \n",
      "Epoch 246: val_loss improved from 0.32821 to 0.32808, saving model to folds1.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8137 - loss: 0.3332 - val_accuracy: 0.8121 - val_loss: 0.3281 - learning_rate: 0.0025\n",
      "Epoch 247/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8148 - loss: 0.3236 \n",
      "Epoch 247: val_loss did not improve from 0.32808\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8125 - loss: 0.3298 - val_accuracy: 0.8139 - val_loss: 0.3294 - learning_rate: 0.0025\n",
      "Epoch 248/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8086 - loss: 0.3344 \n",
      "Epoch 248: val_loss did not improve from 0.32808\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8077 - loss: 0.3362 - val_accuracy: 0.8231 - val_loss: 0.3281 - learning_rate: 0.0025\n",
      "Epoch 249/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8147 - loss: 0.3376 \n",
      "Epoch 249: val_loss did not improve from 0.32808\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8124 - loss: 0.3375 - val_accuracy: 0.8157 - val_loss: 0.3295 - learning_rate: 0.0025\n",
      "Epoch 250/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8095 - loss: 0.3427 \n",
      "Epoch 250: val_loss did not improve from 0.32808\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8087 - loss: 0.3414 - val_accuracy: 0.8243 - val_loss: 0.3293 - learning_rate: 0.0025\n",
      "Epoch 251/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8165 - loss: 0.3323 \n",
      "Epoch 251: val_loss did not improve from 0.32808\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8143 - loss: 0.3339 - val_accuracy: 0.8109 - val_loss: 0.3282 - learning_rate: 0.0025\n",
      "Epoch 252/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8116 - loss: 0.3346 \n",
      "Epoch 252: val_loss improved from 0.32808 to 0.32722, saving model to folds1.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8116 - loss: 0.3351 - val_accuracy: 0.8139 - val_loss: 0.3272 - learning_rate: 0.0025\n",
      "Epoch 253/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8028 - loss: 0.3396 \n",
      "Epoch 253: val_loss did not improve from 0.32722\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8059 - loss: 0.3380 - val_accuracy: 0.8164 - val_loss: 0.3280 - learning_rate: 0.0025\n",
      "Epoch 254/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8069 - loss: 0.3418 \n",
      "Epoch 254: val_loss did not improve from 0.32722\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8069 - loss: 0.3404 - val_accuracy: 0.8225 - val_loss: 0.3292 - learning_rate: 0.0025\n",
      "Epoch 255/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8071 - loss: 0.3375 \n",
      "Epoch 255: val_loss did not improve from 0.32722\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8078 - loss: 0.3376 - val_accuracy: 0.8261 - val_loss: 0.3301 - learning_rate: 0.0025\n",
      "Epoch 256/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8164 - loss: 0.3376 \n",
      "Epoch 256: val_loss did not improve from 0.32722\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8151 - loss: 0.3370 - val_accuracy: 0.8121 - val_loss: 0.3285 - learning_rate: 0.0025\n",
      "Epoch 257/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8134 - loss: 0.3372 \n",
      "Epoch 257: val_loss did not improve from 0.32722\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8129 - loss: 0.3373 - val_accuracy: 0.8084 - val_loss: 0.3293 - learning_rate: 0.0025\n",
      "Epoch 258/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8038 - loss: 0.3336 \n",
      "Epoch 258: val_loss did not improve from 0.32722\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8049 - loss: 0.3355 - val_accuracy: 0.8115 - val_loss: 0.3312 - learning_rate: 0.0025\n",
      "Epoch 259/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8110 - loss: 0.3382 \n",
      "Epoch 259: val_loss did not improve from 0.32722\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8101 - loss: 0.3376 - val_accuracy: 0.8188 - val_loss: 0.3309 - learning_rate: 0.0025\n",
      "Epoch 260/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8134 - loss: 0.3329  \n",
      "Epoch 260: val_loss did not improve from 0.32722\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8112 - loss: 0.3349 - val_accuracy: 0.8200 - val_loss: 0.3315 - learning_rate: 0.0025\n",
      "Epoch 261/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8098 - loss: 0.3283  \n",
      "Epoch 261: val_loss did not improve from 0.32722\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8077 - loss: 0.3334 - val_accuracy: 0.8121 - val_loss: 0.3304 - learning_rate: 0.0025\n",
      "Epoch 262/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7972 - loss: 0.3457 \n",
      "Epoch 262: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
      "\n",
      "Epoch 262: val_loss did not improve from 0.32722\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8002 - loss: 0.3433 - val_accuracy: 0.8218 - val_loss: 0.3286 - learning_rate: 0.0025\n",
      "Epoch 263/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8185 - loss: 0.3302 \n",
      "Epoch 263: val_loss did not improve from 0.32722\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8154 - loss: 0.3323 - val_accuracy: 0.8225 - val_loss: 0.3279 - learning_rate: 0.0012\n",
      "Epoch 264/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8122 - loss: 0.3342 \n",
      "Epoch 264: val_loss did not improve from 0.32722\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8116 - loss: 0.3347 - val_accuracy: 0.8237 - val_loss: 0.3304 - learning_rate: 0.0012\n",
      "Epoch 265/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8097 - loss: 0.3397 \n",
      "Epoch 265: val_loss did not improve from 0.32722\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8099 - loss: 0.3378 - val_accuracy: 0.8225 - val_loss: 0.3298 - learning_rate: 0.0012\n",
      "Epoch 266/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8127 - loss: 0.3372  \n",
      "Epoch 266: val_loss improved from 0.32722 to 0.32702, saving model to folds1.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8132 - loss: 0.3363 - val_accuracy: 0.8176 - val_loss: 0.3270 - learning_rate: 0.0012\n",
      "Epoch 267/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8095 - loss: 0.3407 \n",
      "Epoch 267: val_loss did not improve from 0.32702\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8095 - loss: 0.3376 - val_accuracy: 0.8182 - val_loss: 0.3272 - learning_rate: 0.0012\n",
      "Epoch 268/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8159 - loss: 0.3361  \n",
      "Epoch 268: val_loss improved from 0.32702 to 0.32638, saving model to folds1.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8149 - loss: 0.3357 - val_accuracy: 0.8145 - val_loss: 0.3264 - learning_rate: 0.0012\n",
      "Epoch 269/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8072 - loss: 0.3395 \n",
      "Epoch 269: val_loss did not improve from 0.32638\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8088 - loss: 0.3378 - val_accuracy: 0.8090 - val_loss: 0.3273 - learning_rate: 0.0012\n",
      "Epoch 270/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8121 - loss: 0.3330 \n",
      "Epoch 270: val_loss improved from 0.32638 to 0.32622, saving model to folds1.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8115 - loss: 0.3341 - val_accuracy: 0.8182 - val_loss: 0.3262 - learning_rate: 0.0012\n",
      "Epoch 271/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8097 - loss: 0.3296 \n",
      "Epoch 271: val_loss did not improve from 0.32622\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8109 - loss: 0.3317 - val_accuracy: 0.8151 - val_loss: 0.3270 - learning_rate: 0.0012\n",
      "Epoch 272/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8098 - loss: 0.3352 \n",
      "Epoch 272: val_loss did not improve from 0.32622\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8100 - loss: 0.3352 - val_accuracy: 0.8200 - val_loss: 0.3266 - learning_rate: 0.0012\n",
      "Epoch 273/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8126 - loss: 0.3320 \n",
      "Epoch 273: val_loss did not improve from 0.32622\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8127 - loss: 0.3330 - val_accuracy: 0.8139 - val_loss: 0.3264 - learning_rate: 0.0012\n",
      "Epoch 274/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8028 - loss: 0.3403 \n",
      "Epoch 274: val_loss did not improve from 0.32622\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8048 - loss: 0.3384 - val_accuracy: 0.8121 - val_loss: 0.3273 - learning_rate: 0.0012\n",
      "Epoch 275/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8128 - loss: 0.3337 \n",
      "Epoch 275: val_loss did not improve from 0.32622\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8124 - loss: 0.3342 - val_accuracy: 0.8133 - val_loss: 0.3264 - learning_rate: 0.0012\n",
      "Epoch 276/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8088 - loss: 0.3375 \n",
      "Epoch 276: val_loss did not improve from 0.32622\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8110 - loss: 0.3364 - val_accuracy: 0.8127 - val_loss: 0.3264 - learning_rate: 0.0012\n",
      "Epoch 277/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8074 - loss: 0.3356\n",
      "Epoch 277: val_loss did not improve from 0.32622\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8076 - loss: 0.3356 - val_accuracy: 0.8218 - val_loss: 0.3276 - learning_rate: 0.0012\n",
      "Epoch 278/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8107 - loss: 0.3315 \n",
      "Epoch 278: val_loss did not improve from 0.32622\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8109 - loss: 0.3327 - val_accuracy: 0.8231 - val_loss: 0.3274 - learning_rate: 0.0012\n",
      "Epoch 279/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8077 - loss: 0.3400  \n",
      "Epoch 279: val_loss did not improve from 0.32622\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8101 - loss: 0.3385 - val_accuracy: 0.8182 - val_loss: 0.3263 - learning_rate: 0.0012\n",
      "Epoch 280/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8112 - loss: 0.3316 \n",
      "Epoch 280: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "\n",
      "Epoch 280: val_loss did not improve from 0.32622\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8112 - loss: 0.3327 - val_accuracy: 0.8182 - val_loss: 0.3274 - learning_rate: 0.0012\n",
      "Epoch 281/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8091 - loss: 0.3420 \n",
      "Epoch 281: val_loss did not improve from 0.32622\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8101 - loss: 0.3391 - val_accuracy: 0.8194 - val_loss: 0.3266 - learning_rate: 6.2500e-04\n",
      "Epoch 282/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8166 - loss: 0.3289\n",
      "Epoch 282: val_loss did not improve from 0.32622\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8148 - loss: 0.3304 - val_accuracy: 0.8212 - val_loss: 0.3265 - learning_rate: 6.2500e-04\n",
      "Epoch 283/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8150 - loss: 0.3367\n",
      "Epoch 283: val_loss improved from 0.32622 to 0.32594, saving model to folds1.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.8146 - loss: 0.3360 - val_accuracy: 0.8164 - val_loss: 0.3259 - learning_rate: 6.2500e-04\n",
      "Epoch 284/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8150 - loss: 0.3311\n",
      "Epoch 284: val_loss did not improve from 0.32594\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.8149 - loss: 0.3320 - val_accuracy: 0.8176 - val_loss: 0.3262 - learning_rate: 6.2500e-04\n",
      "Epoch 285/300\n",
      "\u001b[1m12/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8170 - loss: 0.3280\n",
      "Epoch 285: val_loss did not improve from 0.32594\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.8154 - loss: 0.3295 - val_accuracy: 0.8200 - val_loss: 0.3261 - learning_rate: 6.2500e-04\n",
      "Epoch 286/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8083 - loss: 0.3380\n",
      "Epoch 286: val_loss did not improve from 0.32594\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.8086 - loss: 0.3377 - val_accuracy: 0.8151 - val_loss: 0.3260 - learning_rate: 6.2500e-04\n",
      "Epoch 287/300\n",
      "\u001b[1m12/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8134 - loss: 0.3286\n",
      "Epoch 287: val_loss did not improve from 0.32594\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.8132 - loss: 0.3298 - val_accuracy: 0.8206 - val_loss: 0.3267 - learning_rate: 6.2500e-04\n",
      "Epoch 288/300\n",
      "\u001b[1m12/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8120 - loss: 0.3307\n",
      "Epoch 288: val_loss did not improve from 0.32594\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.8117 - loss: 0.3315 - val_accuracy: 0.8231 - val_loss: 0.3262 - learning_rate: 6.2500e-04\n",
      "Epoch 289/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8176 - loss: 0.3307 \n",
      "Epoch 289: val_loss did not improve from 0.32594\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8147 - loss: 0.3324 - val_accuracy: 0.8206 - val_loss: 0.3263 - learning_rate: 6.2500e-04\n",
      "Epoch 290/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8134 - loss: 0.3284 \n",
      "Epoch 290: val_loss did not improve from 0.32594\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8133 - loss: 0.3309 - val_accuracy: 0.8206 - val_loss: 0.3261 - learning_rate: 6.2500e-04\n",
      "Epoch 291/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8107 - loss: 0.3378 \n",
      "Epoch 291: val_loss did not improve from 0.32594\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8104 - loss: 0.3369 - val_accuracy: 0.8231 - val_loss: 0.3269 - learning_rate: 6.2500e-04\n",
      "Epoch 292/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8116 - loss: 0.3374 \n",
      "Epoch 292: val_loss improved from 0.32594 to 0.32592, saving model to folds1.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8128 - loss: 0.3365 - val_accuracy: 0.8157 - val_loss: 0.3259 - learning_rate: 6.2500e-04\n",
      "Epoch 293/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8187 - loss: 0.3315 \n",
      "Epoch 293: ReduceLROnPlateau reducing learning rate to 0.0003124999930150807.\n",
      "\n",
      "Epoch 293: val_loss did not improve from 0.32592\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8157 - loss: 0.3324 - val_accuracy: 0.8212 - val_loss: 0.3263 - learning_rate: 6.2500e-04\n",
      "Epoch 294/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8166 - loss: 0.3317\n",
      "Epoch 294: val_loss did not improve from 0.32592\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8164 - loss: 0.3318 - val_accuracy: 0.8182 - val_loss: 0.3261 - learning_rate: 3.1250e-04\n",
      "Epoch 295/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8128 - loss: 0.3348 \n",
      "Epoch 295: val_loss did not improve from 0.32592\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8129 - loss: 0.3343 - val_accuracy: 0.8212 - val_loss: 0.3261 - learning_rate: 3.1250e-04\n",
      "Epoch 296/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8186 - loss: 0.3226 \n",
      "Epoch 296: val_loss improved from 0.32592 to 0.32592, saving model to folds1.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8170 - loss: 0.3263 - val_accuracy: 0.8194 - val_loss: 0.3259 - learning_rate: 3.1250e-04\n",
      "Epoch 297/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8094 - loss: 0.3375 \n",
      "Epoch 297: val_loss did not improve from 0.32592\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8101 - loss: 0.3360 - val_accuracy: 0.8182 - val_loss: 0.3260 - learning_rate: 3.1250e-04\n",
      "Epoch 298/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8170 - loss: 0.3282 \n",
      "Epoch 298: val_loss did not improve from 0.32592\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8143 - loss: 0.3306 - val_accuracy: 0.8206 - val_loss: 0.3262 - learning_rate: 3.1250e-04\n",
      "Epoch 299/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8194 - loss: 0.3292 \n",
      "Epoch 299: val_loss did not improve from 0.32592\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8182 - loss: 0.3309 - val_accuracy: 0.8231 - val_loss: 0.3261 - learning_rate: 3.1250e-04\n",
      "Epoch 300/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8120 - loss: 0.3296\n",
      "Epoch 300: val_loss did not improve from 0.32592\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8118 - loss: 0.3301 - val_accuracy: 0.8212 - val_loss: 0.3260 - learning_rate: 3.1250e-04\n",
      "Restoring model weights from the end of the best epoch: 296.\n",
      " Computing LSTM feature importance...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:01<00:00,  6.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- > Fold 3 < ---------------\n",
      "Epoch 1/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6765 - loss: 0.6026\n",
      "Epoch 1: val_loss improved from inf to 0.51895, saving model to folds2.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - accuracy: 0.6862 - loss: 0.5857 - val_accuracy: 0.7273 - val_loss: 0.5190 - learning_rate: 0.0100\n",
      "Epoch 2/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7114 - loss: 0.5224 \n",
      "Epoch 2: val_loss improved from 0.51895 to 0.48285, saving model to folds2.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7146 - loss: 0.5195 - val_accuracy: 0.7407 - val_loss: 0.4829 - learning_rate: 0.0100\n",
      "Epoch 3/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7206 - loss: 0.4985  \n",
      "Epoch 3: val_loss improved from 0.48285 to 0.47692, saving model to folds2.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7214 - loss: 0.4982 - val_accuracy: 0.7334 - val_loss: 0.4769 - learning_rate: 0.0100\n",
      "Epoch 4/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7160 - loss: 0.4879 \n",
      "Epoch 4: val_loss improved from 0.47692 to 0.46978, saving model to folds2.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7179 - loss: 0.4878 - val_accuracy: 0.7450 - val_loss: 0.4698 - learning_rate: 0.0100\n",
      "Epoch 5/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7163 - loss: 0.4961 \n",
      "Epoch 5: val_loss improved from 0.46978 to 0.46751, saving model to folds2.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7162 - loss: 0.4959 - val_accuracy: 0.7505 - val_loss: 0.4675 - learning_rate: 0.0100\n",
      "Epoch 6/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7257 - loss: 0.4793 \n",
      "Epoch 6: val_loss improved from 0.46751 to 0.46059, saving model to folds2.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7263 - loss: 0.4799 - val_accuracy: 0.7633 - val_loss: 0.4606 - learning_rate: 0.0100\n",
      "Epoch 7/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7346 - loss: 0.4746 \n",
      "Epoch 7: val_loss improved from 0.46059 to 0.45171, saving model to folds2.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7319 - loss: 0.4762 - val_accuracy: 0.7486 - val_loss: 0.4517 - learning_rate: 0.0100\n",
      "Epoch 8/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7341 - loss: 0.4733 \n",
      "Epoch 8: val_loss did not improve from 0.45171\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7336 - loss: 0.4728 - val_accuracy: 0.7407 - val_loss: 0.4535 - learning_rate: 0.0100\n",
      "Epoch 9/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7288 - loss: 0.4734 \n",
      "Epoch 9: val_loss improved from 0.45171 to 0.44864, saving model to folds2.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7292 - loss: 0.4734 - val_accuracy: 0.7383 - val_loss: 0.4486 - learning_rate: 0.0100\n",
      "Epoch 10/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7315 - loss: 0.4740 \n",
      "Epoch 10: val_loss did not improve from 0.44864\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7317 - loss: 0.4738 - val_accuracy: 0.7584 - val_loss: 0.4504 - learning_rate: 0.0100\n",
      "Epoch 11/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7356 - loss: 0.4668 \n",
      "Epoch 11: val_loss did not improve from 0.44864\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7346 - loss: 0.4687 - val_accuracy: 0.7431 - val_loss: 0.4536 - learning_rate: 0.0100\n",
      "Epoch 12/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7306 - loss: 0.4719 \n",
      "Epoch 12: val_loss improved from 0.44864 to 0.44308, saving model to folds2.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7319 - loss: 0.4706 - val_accuracy: 0.7566 - val_loss: 0.4431 - learning_rate: 0.0100\n",
      "Epoch 13/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7394 - loss: 0.4633 \n",
      "Epoch 13: val_loss improved from 0.44308 to 0.44074, saving model to folds2.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7386 - loss: 0.4644 - val_accuracy: 0.7651 - val_loss: 0.4407 - learning_rate: 0.0100\n",
      "Epoch 14/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7332 - loss: 0.4637 \n",
      "Epoch 14: val_loss did not improve from 0.44074\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7342 - loss: 0.4629 - val_accuracy: 0.7633 - val_loss: 0.4409 - learning_rate: 0.0100\n",
      "Epoch 15/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7357 - loss: 0.4648 \n",
      "Epoch 15: val_loss improved from 0.44074 to 0.44053, saving model to folds2.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7372 - loss: 0.4635 - val_accuracy: 0.7553 - val_loss: 0.4405 - learning_rate: 0.0100\n",
      "Epoch 16/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7423 - loss: 0.4602 \n",
      "Epoch 16: val_loss improved from 0.44053 to 0.43884, saving model to folds2.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7406 - loss: 0.4611 - val_accuracy: 0.7627 - val_loss: 0.4388 - learning_rate: 0.0100\n",
      "Epoch 17/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7369 - loss: 0.4576 \n",
      "Epoch 17: val_loss did not improve from 0.43884\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7359 - loss: 0.4593 - val_accuracy: 0.7346 - val_loss: 0.4621 - learning_rate: 0.0100\n",
      "Epoch 18/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7391 - loss: 0.4712 \n",
      "Epoch 18: val_loss did not improve from 0.43884\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.7398 - loss: 0.4695 - val_accuracy: 0.7425 - val_loss: 0.4426 - learning_rate: 0.0100\n",
      "Epoch 19/300\n",
      "\u001b[1m13/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7365 - loss: 0.4629\n",
      "Epoch 19: val_loss did not improve from 0.43884\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.7374 - loss: 0.4625 - val_accuracy: 0.7694 - val_loss: 0.4392 - learning_rate: 0.0100\n",
      "Epoch 20/300\n",
      "\u001b[1m13/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7351 - loss: 0.4658 \n",
      "Epoch 20: val_loss did not improve from 0.43884\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.7348 - loss: 0.4656 - val_accuracy: 0.7547 - val_loss: 0.4446 - learning_rate: 0.0100\n",
      "Epoch 21/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7407 - loss: 0.4619\n",
      "Epoch 21: val_loss did not improve from 0.43884\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7405 - loss: 0.4623 - val_accuracy: 0.7614 - val_loss: 0.4446 - learning_rate: 0.0100\n",
      "Epoch 22/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7419 - loss: 0.4627\n",
      "Epoch 22: val_loss did not improve from 0.43884\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7424 - loss: 0.4623 - val_accuracy: 0.7590 - val_loss: 0.4391 - learning_rate: 0.0100\n",
      "Epoch 23/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7488 - loss: 0.4545\n",
      "Epoch 23: val_loss did not improve from 0.43884\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7490 - loss: 0.4548 - val_accuracy: 0.7517 - val_loss: 0.4395 - learning_rate: 0.0100\n",
      "Epoch 24/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7391 - loss: 0.4605\n",
      "Epoch 24: val_loss did not improve from 0.43884\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.7391 - loss: 0.4604 - val_accuracy: 0.7462 - val_loss: 0.4486 - learning_rate: 0.0100\n",
      "Epoch 25/300\n",
      "\u001b[1m13/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7401 - loss: 0.4593\n",
      "Epoch 25: val_loss did not improve from 0.43884\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.7402 - loss: 0.4593 - val_accuracy: 0.7395 - val_loss: 0.4545 - learning_rate: 0.0100\n",
      "Epoch 26/300\n",
      "\u001b[1m13/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7413 - loss: 0.4637\n",
      "Epoch 26: val_loss improved from 0.43884 to 0.43436, saving model to folds2.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.7421 - loss: 0.4622 - val_accuracy: 0.7584 - val_loss: 0.4344 - learning_rate: 0.0100\n",
      "Epoch 27/300\n",
      "\u001b[1m13/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7441 - loss: 0.4546\n",
      "Epoch 27: val_loss did not improve from 0.43436\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.7445 - loss: 0.4541 - val_accuracy: 0.7511 - val_loss: 0.4363 - learning_rate: 0.0100\n",
      "Epoch 28/300\n",
      "\u001b[1m13/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7447 - loss: 0.4533\n",
      "Epoch 28: val_loss improved from 0.43436 to 0.43329, saving model to folds2.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.7447 - loss: 0.4533 - val_accuracy: 0.7614 - val_loss: 0.4333 - learning_rate: 0.0100\n",
      "Epoch 29/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7442 - loss: 0.4524\n",
      "Epoch 29: val_loss improved from 0.43329 to 0.43027, saving model to folds2.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.7447 - loss: 0.4522 - val_accuracy: 0.7639 - val_loss: 0.4303 - learning_rate: 0.0100\n",
      "Epoch 30/300\n",
      "\u001b[1m13/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7495 - loss: 0.4539\n",
      "Epoch 30: val_loss did not improve from 0.43027\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7493 - loss: 0.4534 - val_accuracy: 0.7547 - val_loss: 0.4315 - learning_rate: 0.0100\n",
      "Epoch 31/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7358 - loss: 0.4537  \n",
      "Epoch 31: val_loss improved from 0.43027 to 0.42770, saving model to folds2.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7376 - loss: 0.4532 - val_accuracy: 0.7675 - val_loss: 0.4277 - learning_rate: 0.0100\n",
      "Epoch 32/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7494 - loss: 0.4477 \n",
      "Epoch 32: val_loss did not improve from 0.42770\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7488 - loss: 0.4479 - val_accuracy: 0.7663 - val_loss: 0.4363 - learning_rate: 0.0100\n",
      "Epoch 33/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7482 - loss: 0.4461  \n",
      "Epoch 33: val_loss improved from 0.42770 to 0.42185, saving model to folds2.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7469 - loss: 0.4473 - val_accuracy: 0.7639 - val_loss: 0.4218 - learning_rate: 0.0100\n",
      "Epoch 34/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7496 - loss: 0.4467 \n",
      "Epoch 34: val_loss improved from 0.42185 to 0.42141, saving model to folds2.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7500 - loss: 0.4463 - val_accuracy: 0.7785 - val_loss: 0.4214 - learning_rate: 0.0100\n",
      "Epoch 35/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7489 - loss: 0.4467  \n",
      "Epoch 35: val_loss did not improve from 0.42141\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7473 - loss: 0.4479 - val_accuracy: 0.7608 - val_loss: 0.4332 - learning_rate: 0.0100\n",
      "Epoch 36/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7446 - loss: 0.4509 \n",
      "Epoch 36: val_loss did not improve from 0.42141\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7457 - loss: 0.4506 - val_accuracy: 0.7688 - val_loss: 0.4256 - learning_rate: 0.0100\n",
      "Epoch 37/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7595 - loss: 0.4378 \n",
      "Epoch 37: val_loss did not improve from 0.42141\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7566 - loss: 0.4394 - val_accuracy: 0.7639 - val_loss: 0.4273 - learning_rate: 0.0100\n",
      "Epoch 38/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7455 - loss: 0.4531 \n",
      "Epoch 38: val_loss improved from 0.42141 to 0.42123, saving model to folds2.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7485 - loss: 0.4497 - val_accuracy: 0.7761 - val_loss: 0.4212 - learning_rate: 0.0100\n",
      "Epoch 39/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7571 - loss: 0.4398 \n",
      "Epoch 39: val_loss improved from 0.42123 to 0.41663, saving model to folds2.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7564 - loss: 0.4401 - val_accuracy: 0.7767 - val_loss: 0.4166 - learning_rate: 0.0100\n",
      "Epoch 40/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7549 - loss: 0.4368 \n",
      "Epoch 40: val_loss did not improve from 0.41663\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7528 - loss: 0.4402 - val_accuracy: 0.7736 - val_loss: 0.4211 - learning_rate: 0.0100\n",
      "Epoch 41/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7631 - loss: 0.4341  \n",
      "Epoch 41: val_loss did not improve from 0.41663\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7609 - loss: 0.4366 - val_accuracy: 0.7663 - val_loss: 0.4255 - learning_rate: 0.0100\n",
      "Epoch 42/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7522 - loss: 0.4420 \n",
      "Epoch 42: val_loss improved from 0.41663 to 0.41225, saving model to folds2.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7523 - loss: 0.4405 - val_accuracy: 0.7822 - val_loss: 0.4122 - learning_rate: 0.0100\n",
      "Epoch 43/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7676 - loss: 0.4293 \n",
      "Epoch 43: val_loss improved from 0.41225 to 0.41215, saving model to folds2.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7665 - loss: 0.4306 - val_accuracy: 0.7846 - val_loss: 0.4122 - learning_rate: 0.0100\n",
      "Epoch 44/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7550 - loss: 0.4374 \n",
      "Epoch 44: val_loss improved from 0.41215 to 0.41044, saving model to folds2.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7567 - loss: 0.4375 - val_accuracy: 0.7779 - val_loss: 0.4104 - learning_rate: 0.0100\n",
      "Epoch 45/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7701 - loss: 0.4316 \n",
      "Epoch 45: val_loss did not improve from 0.41044\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7698 - loss: 0.4324 - val_accuracy: 0.7767 - val_loss: 0.4124 - learning_rate: 0.0100\n",
      "Epoch 46/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7637 - loss: 0.4280  \n",
      "Epoch 46: val_loss improved from 0.41044 to 0.41020, saving model to folds2.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7636 - loss: 0.4294 - val_accuracy: 0.7877 - val_loss: 0.4102 - learning_rate: 0.0100\n",
      "Epoch 47/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7796 - loss: 0.4244 \n",
      "Epoch 47: val_loss did not improve from 0.41020\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7761 - loss: 0.4255 - val_accuracy: 0.7590 - val_loss: 0.4103 - learning_rate: 0.0100\n",
      "Epoch 48/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7615 - loss: 0.4314 \n",
      "Epoch 48: val_loss did not improve from 0.41020\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7617 - loss: 0.4329 - val_accuracy: 0.7822 - val_loss: 0.4117 - learning_rate: 0.0100\n",
      "Epoch 49/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7638 - loss: 0.4366 \n",
      "Epoch 49: val_loss did not improve from 0.41020\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7650 - loss: 0.4353 - val_accuracy: 0.7810 - val_loss: 0.4106 - learning_rate: 0.0100\n",
      "Epoch 50/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7668 - loss: 0.4384 \n",
      "Epoch 50: val_loss improved from 0.41020 to 0.40766, saving model to folds2.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7669 - loss: 0.4362 - val_accuracy: 0.7834 - val_loss: 0.4077 - learning_rate: 0.0100\n",
      "Epoch 51/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7672 - loss: 0.4270  \n",
      "Epoch 51: val_loss did not improve from 0.40766\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7644 - loss: 0.4282 - val_accuracy: 0.7700 - val_loss: 0.4154 - learning_rate: 0.0100\n",
      "Epoch 52/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7546 - loss: 0.4395 \n",
      "Epoch 52: val_loss improved from 0.40766 to 0.40670, saving model to folds2.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7553 - loss: 0.4379 - val_accuracy: 0.7877 - val_loss: 0.4067 - learning_rate: 0.0100\n",
      "Epoch 53/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7640 - loss: 0.4288  \n",
      "Epoch 53: val_loss did not improve from 0.40670\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7649 - loss: 0.4291 - val_accuracy: 0.7663 - val_loss: 0.4285 - learning_rate: 0.0100\n",
      "Epoch 54/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7600 - loss: 0.4405 \n",
      "Epoch 54: val_loss did not improve from 0.40670\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7588 - loss: 0.4414 - val_accuracy: 0.7712 - val_loss: 0.4278 - learning_rate: 0.0100\n",
      "Epoch 55/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7465 - loss: 0.4420 \n",
      "Epoch 55: val_loss did not improve from 0.40670\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7486 - loss: 0.4414 - val_accuracy: 0.7706 - val_loss: 0.4249 - learning_rate: 0.0100\n",
      "Epoch 56/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7582 - loss: 0.4375 \n",
      "Epoch 56: val_loss did not improve from 0.40670\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7605 - loss: 0.4367 - val_accuracy: 0.7779 - val_loss: 0.4169 - learning_rate: 0.0100\n",
      "Epoch 57/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7649 - loss: 0.4339  \n",
      "Epoch 57: val_loss did not improve from 0.40670\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7653 - loss: 0.4329 - val_accuracy: 0.7791 - val_loss: 0.4197 - learning_rate: 0.0100\n",
      "Epoch 58/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7639 - loss: 0.4331 \n",
      "Epoch 58: val_loss did not improve from 0.40670\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7643 - loss: 0.4330 - val_accuracy: 0.7700 - val_loss: 0.4132 - learning_rate: 0.0100\n",
      "Epoch 59/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7545 - loss: 0.4380 \n",
      "Epoch 59: val_loss did not improve from 0.40670\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7574 - loss: 0.4360 - val_accuracy: 0.7761 - val_loss: 0.4131 - learning_rate: 0.0100\n",
      "Epoch 60/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7675 - loss: 0.4267 \n",
      "Epoch 60: val_loss did not improve from 0.40670\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7679 - loss: 0.4272 - val_accuracy: 0.7694 - val_loss: 0.4220 - learning_rate: 0.0100\n",
      "Epoch 61/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7579 - loss: 0.4401 \n",
      "Epoch 61: val_loss did not improve from 0.40670\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7580 - loss: 0.4396 - val_accuracy: 0.7657 - val_loss: 0.4181 - learning_rate: 0.0100\n",
      "Epoch 62/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7593 - loss: 0.4352 \n",
      "Epoch 62: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
      "\n",
      "Epoch 62: val_loss did not improve from 0.40670\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7600 - loss: 0.4341 - val_accuracy: 0.7639 - val_loss: 0.4220 - learning_rate: 0.0100\n",
      "Epoch 63/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7611 - loss: 0.4333 \n",
      "Epoch 63: val_loss did not improve from 0.40670\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7621 - loss: 0.4323 - val_accuracy: 0.7785 - val_loss: 0.4116 - learning_rate: 0.0050\n",
      "Epoch 64/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7674 - loss: 0.4214 \n",
      "Epoch 64: val_loss did not improve from 0.40670\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7694 - loss: 0.4215 - val_accuracy: 0.7755 - val_loss: 0.4096 - learning_rate: 0.0050\n",
      "Epoch 65/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7724 - loss: 0.4278 \n",
      "Epoch 65: val_loss improved from 0.40670 to 0.40417, saving model to folds2.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7730 - loss: 0.4255 - val_accuracy: 0.7810 - val_loss: 0.4042 - learning_rate: 0.0050\n",
      "Epoch 66/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7725 - loss: 0.4167 \n",
      "Epoch 66: val_loss improved from 0.40417 to 0.40356, saving model to folds2.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7736 - loss: 0.4174 - val_accuracy: 0.7834 - val_loss: 0.4036 - learning_rate: 0.0050\n",
      "Epoch 67/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7695 - loss: 0.4209 \n",
      "Epoch 67: val_loss improved from 0.40356 to 0.39919, saving model to folds2.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7714 - loss: 0.4192 - val_accuracy: 0.7901 - val_loss: 0.3992 - learning_rate: 0.0050\n",
      "Epoch 68/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7793 - loss: 0.4155 \n",
      "Epoch 68: val_loss improved from 0.39919 to 0.39553, saving model to folds2.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7792 - loss: 0.4153 - val_accuracy: 0.7865 - val_loss: 0.3955 - learning_rate: 0.0050\n",
      "Epoch 69/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7753 - loss: 0.4143 \n",
      "Epoch 69: val_loss did not improve from 0.39553\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7773 - loss: 0.4141 - val_accuracy: 0.7797 - val_loss: 0.3992 - learning_rate: 0.0050\n",
      "Epoch 70/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7697 - loss: 0.4186  \n",
      "Epoch 70: val_loss did not improve from 0.39553\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7731 - loss: 0.4169 - val_accuracy: 0.7858 - val_loss: 0.3959 - learning_rate: 0.0050\n",
      "Epoch 71/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7821 - loss: 0.4114 \n",
      "Epoch 71: val_loss did not improve from 0.39553\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7804 - loss: 0.4114 - val_accuracy: 0.7810 - val_loss: 0.3984 - learning_rate: 0.0050\n",
      "Epoch 72/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7689 - loss: 0.4238  \n",
      "Epoch 72: val_loss improved from 0.39553 to 0.39197, saving model to folds2.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7723 - loss: 0.4199 - val_accuracy: 0.7907 - val_loss: 0.3920 - learning_rate: 0.0050\n",
      "Epoch 73/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7793 - loss: 0.4137 \n",
      "Epoch 73: val_loss did not improve from 0.39197\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7807 - loss: 0.4125 - val_accuracy: 0.7926 - val_loss: 0.3922 - learning_rate: 0.0050\n",
      "Epoch 74/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7809 - loss: 0.4113 \n",
      "Epoch 74: val_loss did not improve from 0.39197\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7813 - loss: 0.4109 - val_accuracy: 0.7907 - val_loss: 0.3933 - learning_rate: 0.0050\n",
      "Epoch 75/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7794 - loss: 0.4104 \n",
      "Epoch 75: val_loss did not improve from 0.39197\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7790 - loss: 0.4108 - val_accuracy: 0.7944 - val_loss: 0.3979 - learning_rate: 0.0050\n",
      "Epoch 76/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7781 - loss: 0.4215\n",
      "Epoch 76: val_loss did not improve from 0.39197\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7785 - loss: 0.4208 - val_accuracy: 0.7950 - val_loss: 0.3943 - learning_rate: 0.0050\n",
      "Epoch 77/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7764 - loss: 0.4139\n",
      "Epoch 77: val_loss did not improve from 0.39197\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7773 - loss: 0.4136 - val_accuracy: 0.7816 - val_loss: 0.3943 - learning_rate: 0.0050\n",
      "Epoch 78/300\n",
      "\u001b[1m12/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7781 - loss: 0.4107\n",
      "Epoch 78: val_loss did not improve from 0.39197\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.7772 - loss: 0.4114 - val_accuracy: 0.7895 - val_loss: 0.3928 - learning_rate: 0.0050\n",
      "Epoch 79/300\n",
      "\u001b[1m13/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7761 - loss: 0.4114\n",
      "Epoch 79: val_loss improved from 0.39197 to 0.39079, saving model to folds2.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.7761 - loss: 0.4115 - val_accuracy: 0.7919 - val_loss: 0.3908 - learning_rate: 0.0050\n",
      "Epoch 80/300\n",
      "\u001b[1m13/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7776 - loss: 0.4138\n",
      "Epoch 80: val_loss improved from 0.39079 to 0.38899, saving model to folds2.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.7782 - loss: 0.4133 - val_accuracy: 0.7901 - val_loss: 0.3890 - learning_rate: 0.0050\n",
      "Epoch 81/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7724 - loss: 0.4103\n",
      "Epoch 81: val_loss did not improve from 0.38899\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.7738 - loss: 0.4103 - val_accuracy: 0.7761 - val_loss: 0.3945 - learning_rate: 0.0050\n",
      "Epoch 82/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7792 - loss: 0.4130 \n",
      "Epoch 82: val_loss did not improve from 0.38899\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.7791 - loss: 0.4126 - val_accuracy: 0.7901 - val_loss: 0.3905 - learning_rate: 0.0050\n",
      "Epoch 83/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7703 - loss: 0.4178\n",
      "Epoch 83: val_loss improved from 0.38899 to 0.38898, saving model to folds2.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.7709 - loss: 0.4170 - val_accuracy: 0.7895 - val_loss: 0.3890 - learning_rate: 0.0050\n",
      "Epoch 84/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7786 - loss: 0.4135\n",
      "Epoch 84: val_loss did not improve from 0.38898\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7780 - loss: 0.4134 - val_accuracy: 0.7883 - val_loss: 0.3902 - learning_rate: 0.0050\n",
      "Epoch 85/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7740 - loss: 0.4046 \n",
      "Epoch 85: val_loss improved from 0.38898 to 0.38817, saving model to folds2.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7764 - loss: 0.4064 - val_accuracy: 0.7895 - val_loss: 0.3882 - learning_rate: 0.0050\n",
      "Epoch 86/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7780 - loss: 0.4082 \n",
      "Epoch 86: val_loss did not improve from 0.38817\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7788 - loss: 0.4083 - val_accuracy: 0.8029 - val_loss: 0.3893 - learning_rate: 0.0050\n",
      "Epoch 87/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7848 - loss: 0.4089  \n",
      "Epoch 87: val_loss did not improve from 0.38817\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7832 - loss: 0.4080 - val_accuracy: 0.7913 - val_loss: 0.3882 - learning_rate: 0.0050\n",
      "Epoch 88/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7833 - loss: 0.4031 \n",
      "Epoch 88: val_loss did not improve from 0.38817\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7824 - loss: 0.4047 - val_accuracy: 0.7901 - val_loss: 0.3900 - learning_rate: 0.0050\n",
      "Epoch 89/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7817 - loss: 0.4107 \n",
      "Epoch 89: val_loss improved from 0.38817 to 0.38740, saving model to folds2.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7815 - loss: 0.4098 - val_accuracy: 0.7889 - val_loss: 0.3874 - learning_rate: 0.0050\n",
      "Epoch 90/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7862 - loss: 0.3994 \n",
      "Epoch 90: val_loss improved from 0.38740 to 0.38693, saving model to folds2.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7838 - loss: 0.4018 - val_accuracy: 0.7919 - val_loss: 0.3869 - learning_rate: 0.0050\n",
      "Epoch 91/300\n",
      "\u001b[1m 7/15\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7877 - loss: 0.4014 \n",
      "Epoch 91: val_loss improved from 0.38693 to 0.38278, saving model to folds2.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7845 - loss: 0.4030 - val_accuracy: 0.7993 - val_loss: 0.3828 - learning_rate: 0.0050\n",
      "Epoch 92/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7769 - loss: 0.4051 \n",
      "Epoch 92: val_loss did not improve from 0.38278\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7780 - loss: 0.4051 - val_accuracy: 0.7944 - val_loss: 0.3831 - learning_rate: 0.0050\n",
      "Epoch 93/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7788 - loss: 0.4035 \n",
      "Epoch 93: val_loss did not improve from 0.38278\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7810 - loss: 0.4037 - val_accuracy: 0.7913 - val_loss: 0.3874 - learning_rate: 0.0050\n",
      "Epoch 94/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7904 - loss: 0.4003  \n",
      "Epoch 94: val_loss improved from 0.38278 to 0.38127, saving model to folds2.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7878 - loss: 0.4013 - val_accuracy: 0.7926 - val_loss: 0.3813 - learning_rate: 0.0050\n",
      "Epoch 95/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7803 - loss: 0.4006 \n",
      "Epoch 95: val_loss did not improve from 0.38127\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7814 - loss: 0.4017 - val_accuracy: 0.7865 - val_loss: 0.3852 - learning_rate: 0.0050\n",
      "Epoch 96/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7780 - loss: 0.4135 \n",
      "Epoch 96: val_loss did not improve from 0.38127\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7786 - loss: 0.4116 - val_accuracy: 0.7895 - val_loss: 0.3890 - learning_rate: 0.0050\n",
      "Epoch 97/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7809 - loss: 0.4084 \n",
      "Epoch 97: val_loss did not improve from 0.38127\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7806 - loss: 0.4073 - val_accuracy: 0.7858 - val_loss: 0.3907 - learning_rate: 0.0050\n",
      "Epoch 98/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7751 - loss: 0.4080 \n",
      "Epoch 98: val_loss did not improve from 0.38127\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7771 - loss: 0.4071 - val_accuracy: 0.7913 - val_loss: 0.3821 - learning_rate: 0.0050\n",
      "Epoch 99/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7773 - loss: 0.4056 \n",
      "Epoch 99: val_loss did not improve from 0.38127\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7790 - loss: 0.4051 - val_accuracy: 0.7858 - val_loss: 0.3871 - learning_rate: 0.0050\n",
      "Epoch 100/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7716 - loss: 0.4072 \n",
      "Epoch 100: val_loss did not improve from 0.38127\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7726 - loss: 0.4069 - val_accuracy: 0.7901 - val_loss: 0.3844 - learning_rate: 0.0050\n",
      "Epoch 101/300\n",
      "\u001b[1m 7/15\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7919 - loss: 0.3971 \n",
      "Epoch 101: val_loss did not improve from 0.38127\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7878 - loss: 0.4016 - val_accuracy: 0.7889 - val_loss: 0.3832 - learning_rate: 0.0050\n",
      "Epoch 102/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7812 - loss: 0.4082 \n",
      "Epoch 102: val_loss improved from 0.38127 to 0.38044, saving model to folds2.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7817 - loss: 0.4071 - val_accuracy: 0.7913 - val_loss: 0.3804 - learning_rate: 0.0050\n",
      "Epoch 103/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7769 - loss: 0.4035 \n",
      "Epoch 103: val_loss improved from 0.38044 to 0.37936, saving model to folds2.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7791 - loss: 0.4028 - val_accuracy: 0.7950 - val_loss: 0.3794 - learning_rate: 0.0050\n",
      "Epoch 104/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7973 - loss: 0.3846 \n",
      "Epoch 104: val_loss improved from 0.37936 to 0.37866, saving model to folds2.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7929 - loss: 0.3906 - val_accuracy: 0.7907 - val_loss: 0.3787 - learning_rate: 0.0050\n",
      "Epoch 105/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7901 - loss: 0.3973 \n",
      "Epoch 105: val_loss did not improve from 0.37866\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7881 - loss: 0.3981 - val_accuracy: 0.7895 - val_loss: 0.3811 - learning_rate: 0.0050\n",
      "Epoch 106/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7814 - loss: 0.4066 \n",
      "Epoch 106: val_loss improved from 0.37866 to 0.37674, saving model to folds2.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7842 - loss: 0.4034 - val_accuracy: 0.7907 - val_loss: 0.3767 - learning_rate: 0.0050\n",
      "Epoch 107/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7832 - loss: 0.3965 \n",
      "Epoch 107: val_loss did not improve from 0.37674\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7834 - loss: 0.3988 - val_accuracy: 0.8017 - val_loss: 0.3786 - learning_rate: 0.0050\n",
      "Epoch 108/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7884 - loss: 0.3991 \n",
      "Epoch 108: val_loss did not improve from 0.37674\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7871 - loss: 0.4000 - val_accuracy: 0.8041 - val_loss: 0.3803 - learning_rate: 0.0050\n",
      "Epoch 109/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7918 - loss: 0.3964 \n",
      "Epoch 109: val_loss improved from 0.37674 to 0.37497, saving model to folds2.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7887 - loss: 0.3977 - val_accuracy: 0.8060 - val_loss: 0.3750 - learning_rate: 0.0050\n",
      "Epoch 110/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7802 - loss: 0.4032 \n",
      "Epoch 110: val_loss did not improve from 0.37497\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7814 - loss: 0.4012 - val_accuracy: 0.8041 - val_loss: 0.3810 - learning_rate: 0.0050\n",
      "Epoch 111/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7838 - loss: 0.4015 \n",
      "Epoch 111: val_loss did not improve from 0.37497\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7838 - loss: 0.4005 - val_accuracy: 0.7919 - val_loss: 0.3779 - learning_rate: 0.0050\n",
      "Epoch 112/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7939 - loss: 0.3931 \n",
      "Epoch 112: val_loss did not improve from 0.37497\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7898 - loss: 0.3950 - val_accuracy: 0.8011 - val_loss: 0.3772 - learning_rate: 0.0050\n",
      "Epoch 113/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7862 - loss: 0.3988 \n",
      "Epoch 113: val_loss did not improve from 0.37497\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7859 - loss: 0.3986 - val_accuracy: 0.8035 - val_loss: 0.3769 - learning_rate: 0.0050\n",
      "Epoch 114/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7949 - loss: 0.3924 \n",
      "Epoch 114: val_loss did not improve from 0.37497\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7921 - loss: 0.3939 - val_accuracy: 0.8005 - val_loss: 0.3774 - learning_rate: 0.0050\n",
      "Epoch 115/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7890 - loss: 0.3922 \n",
      "Epoch 115: val_loss did not improve from 0.37497\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7876 - loss: 0.3941 - val_accuracy: 0.8029 - val_loss: 0.3768 - learning_rate: 0.0050\n",
      "Epoch 116/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7907 - loss: 0.3958 \n",
      "Epoch 116: val_loss did not improve from 0.37497\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7903 - loss: 0.3962 - val_accuracy: 0.7987 - val_loss: 0.3766 - learning_rate: 0.0050\n",
      "Epoch 117/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7835 - loss: 0.3940 \n",
      "Epoch 117: val_loss did not improve from 0.37497\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7848 - loss: 0.3953 - val_accuracy: 0.7962 - val_loss: 0.3770 - learning_rate: 0.0050\n",
      "Epoch 118/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7777 - loss: 0.4064 \n",
      "Epoch 118: val_loss did not improve from 0.37497\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7804 - loss: 0.4030 - val_accuracy: 0.7962 - val_loss: 0.3795 - learning_rate: 0.0050\n",
      "Epoch 119/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7975 - loss: 0.3935 \n",
      "Epoch 119: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "\n",
      "Epoch 119: val_loss did not improve from 0.37497\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7941 - loss: 0.3953 - val_accuracy: 0.7944 - val_loss: 0.3783 - learning_rate: 0.0050\n",
      "Epoch 120/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7809 - loss: 0.3937 \n",
      "Epoch 120: val_loss improved from 0.37497 to 0.37385, saving model to folds2.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7826 - loss: 0.3946 - val_accuracy: 0.8005 - val_loss: 0.3739 - learning_rate: 0.0025\n",
      "Epoch 121/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7914 - loss: 0.3880 \n",
      "Epoch 121: val_loss improved from 0.37385 to 0.37038, saving model to folds2.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7904 - loss: 0.3902 - val_accuracy: 0.7980 - val_loss: 0.3704 - learning_rate: 0.0025\n",
      "Epoch 122/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7877 - loss: 0.3972 \n",
      "Epoch 122: val_loss did not improve from 0.37038\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7883 - loss: 0.3961 - val_accuracy: 0.7993 - val_loss: 0.3720 - learning_rate: 0.0025\n",
      "Epoch 123/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7838 - loss: 0.4000 \n",
      "Epoch 123: val_loss did not improve from 0.37038\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7841 - loss: 0.3982 - val_accuracy: 0.8054 - val_loss: 0.3735 - learning_rate: 0.0025\n",
      "Epoch 124/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7936 - loss: 0.3894 \n",
      "Epoch 124: val_loss did not improve from 0.37038\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7923 - loss: 0.3904 - val_accuracy: 0.8029 - val_loss: 0.3705 - learning_rate: 0.0025\n",
      "Epoch 125/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7771 - loss: 0.4036 \n",
      "Epoch 125: val_loss improved from 0.37038 to 0.36967, saving model to folds2.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7808 - loss: 0.3999 - val_accuracy: 0.8035 - val_loss: 0.3697 - learning_rate: 0.0025\n",
      "Epoch 126/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7905 - loss: 0.3916 \n",
      "Epoch 126: val_loss did not improve from 0.36967\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7899 - loss: 0.3922 - val_accuracy: 0.8048 - val_loss: 0.3708 - learning_rate: 0.0025\n",
      "Epoch 127/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7870 - loss: 0.3958 \n",
      "Epoch 127: val_loss did not improve from 0.36967\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7882 - loss: 0.3947 - val_accuracy: 0.8054 - val_loss: 0.3700 - learning_rate: 0.0025\n",
      "Epoch 128/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7987 - loss: 0.3873 \n",
      "Epoch 128: val_loss did not improve from 0.36967\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7962 - loss: 0.3887 - val_accuracy: 0.8011 - val_loss: 0.3702 - learning_rate: 0.0025\n",
      "Epoch 129/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7870 - loss: 0.3915  \n",
      "Epoch 129: val_loss did not improve from 0.36967\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7876 - loss: 0.3919 - val_accuracy: 0.8078 - val_loss: 0.3708 - learning_rate: 0.0025\n",
      "Epoch 130/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7825 - loss: 0.4029 \n",
      "Epoch 130: val_loss improved from 0.36967 to 0.36845, saving model to folds2.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7851 - loss: 0.3990 - val_accuracy: 0.8023 - val_loss: 0.3684 - learning_rate: 0.0025\n",
      "Epoch 131/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7897 - loss: 0.3910 \n",
      "Epoch 131: val_loss did not improve from 0.36845\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7891 - loss: 0.3916 - val_accuracy: 0.8005 - val_loss: 0.3700 - learning_rate: 0.0025\n",
      "Epoch 132/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7831 - loss: 0.3936 \n",
      "Epoch 132: val_loss did not improve from 0.36845\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7837 - loss: 0.3928 - val_accuracy: 0.8054 - val_loss: 0.3709 - learning_rate: 0.0025\n",
      "Epoch 133/300\n",
      "\u001b[1m12/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7907 - loss: 0.3892\n",
      "Epoch 133: val_loss improved from 0.36845 to 0.36841, saving model to folds2.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.7907 - loss: 0.3901 - val_accuracy: 0.8048 - val_loss: 0.3684 - learning_rate: 0.0025\n",
      "Epoch 134/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7902 - loss: 0.3914\n",
      "Epoch 134: val_loss did not improve from 0.36841\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.7903 - loss: 0.3914 - val_accuracy: 0.8011 - val_loss: 0.3686 - learning_rate: 0.0025\n",
      "Epoch 135/300\n",
      "\u001b[1m13/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7913 - loss: 0.3912\n",
      "Epoch 135: val_loss did not improve from 0.36841\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7913 - loss: 0.3913 - val_accuracy: 0.8017 - val_loss: 0.3691 - learning_rate: 0.0025\n",
      "Epoch 136/300\n",
      "\u001b[1m13/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7958 - loss: 0.3851\n",
      "Epoch 136: val_loss improved from 0.36841 to 0.36792, saving model to folds2.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.7945 - loss: 0.3863 - val_accuracy: 0.8005 - val_loss: 0.3679 - learning_rate: 0.0025\n",
      "Epoch 137/300\n",
      "\u001b[1m12/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7916 - loss: 0.3851\n",
      "Epoch 137: val_loss did not improve from 0.36792\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.7909 - loss: 0.3866 - val_accuracy: 0.8029 - val_loss: 0.3712 - learning_rate: 0.0025\n",
      "Epoch 138/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7909 - loss: 0.3924\n",
      "Epoch 138: val_loss did not improve from 0.36792\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.7901 - loss: 0.3923 - val_accuracy: 0.8005 - val_loss: 0.3698 - learning_rate: 0.0025\n",
      "Epoch 139/300\n",
      "\u001b[1m13/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7894 - loss: 0.3944\n",
      "Epoch 139: val_loss did not improve from 0.36792\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.7895 - loss: 0.3941 - val_accuracy: 0.7987 - val_loss: 0.3702 - learning_rate: 0.0025\n",
      "Epoch 140/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7821 - loss: 0.3965\n",
      "Epoch 140: val_loss did not improve from 0.36792\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.7826 - loss: 0.3962 - val_accuracy: 0.8011 - val_loss: 0.3713 - learning_rate: 0.0025\n",
      "Epoch 141/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7911 - loss: 0.3923\n",
      "Epoch 141: val_loss did not improve from 0.36792\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.7909 - loss: 0.3923 - val_accuracy: 0.8060 - val_loss: 0.3681 - learning_rate: 0.0025\n",
      "Epoch 142/300\n",
      "\u001b[1m12/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7932 - loss: 0.3849\n",
      "Epoch 142: val_loss improved from 0.36792 to 0.36756, saving model to folds2.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.7927 - loss: 0.3863 - val_accuracy: 0.8011 - val_loss: 0.3676 - learning_rate: 0.0025\n",
      "Epoch 143/300\n",
      "\u001b[1m12/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7913 - loss: 0.3901\n",
      "Epoch 143: val_loss did not improve from 0.36756\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.7913 - loss: 0.3902 - val_accuracy: 0.7987 - val_loss: 0.3683 - learning_rate: 0.0025\n",
      "Epoch 144/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7900 - loss: 0.3888 \n",
      "Epoch 144: val_loss did not improve from 0.36756\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7905 - loss: 0.3893 - val_accuracy: 0.7956 - val_loss: 0.3688 - learning_rate: 0.0025\n",
      "Epoch 145/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7879 - loss: 0.3856 \n",
      "Epoch 145: val_loss improved from 0.36756 to 0.36646, saving model to folds2.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7881 - loss: 0.3877 - val_accuracy: 0.7980 - val_loss: 0.3665 - learning_rate: 0.0025\n",
      "Epoch 146/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7872 - loss: 0.3908  \n",
      "Epoch 146: val_loss did not improve from 0.36646\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7877 - loss: 0.3906 - val_accuracy: 0.8066 - val_loss: 0.3671 - learning_rate: 0.0025\n",
      "Epoch 147/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7943 - loss: 0.3856 \n",
      "Epoch 147: val_loss did not improve from 0.36646\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7926 - loss: 0.3876 - val_accuracy: 0.8048 - val_loss: 0.3687 - learning_rate: 0.0025\n",
      "Epoch 148/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7947 - loss: 0.3912 \n",
      "Epoch 148: val_loss did not improve from 0.36646\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7921 - loss: 0.3914 - val_accuracy: 0.8121 - val_loss: 0.3665 - learning_rate: 0.0025\n",
      "Epoch 149/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7947 - loss: 0.3888 \n",
      "Epoch 149: val_loss did not improve from 0.36646\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7933 - loss: 0.3895 - val_accuracy: 0.8005 - val_loss: 0.3675 - learning_rate: 0.0025\n",
      "Epoch 150/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7868 - loss: 0.3910 \n",
      "Epoch 150: val_loss did not improve from 0.36646\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7884 - loss: 0.3907 - val_accuracy: 0.8041 - val_loss: 0.3667 - learning_rate: 0.0025\n",
      "Epoch 151/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7888 - loss: 0.3909  \n",
      "Epoch 151: val_loss did not improve from 0.36646\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7896 - loss: 0.3911 - val_accuracy: 0.7999 - val_loss: 0.3671 - learning_rate: 0.0025\n",
      "Epoch 152/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7922 - loss: 0.3847 \n",
      "Epoch 152: val_loss did not improve from 0.36646\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7915 - loss: 0.3866 - val_accuracy: 0.8060 - val_loss: 0.3681 - learning_rate: 0.0025\n",
      "Epoch 153/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7923 - loss: 0.3863 \n",
      "Epoch 153: val_loss did not improve from 0.36646\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7912 - loss: 0.3875 - val_accuracy: 0.8023 - val_loss: 0.3714 - learning_rate: 0.0025\n",
      "Epoch 154/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7901 - loss: 0.3921 \n",
      "Epoch 154: val_loss did not improve from 0.36646\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7893 - loss: 0.3914 - val_accuracy: 0.8048 - val_loss: 0.3667 - learning_rate: 0.0025\n",
      "Epoch 155/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7935 - loss: 0.3821 \n",
      "Epoch 155: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
      "\n",
      "Epoch 155: val_loss did not improve from 0.36646\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7909 - loss: 0.3859 - val_accuracy: 0.8048 - val_loss: 0.3679 - learning_rate: 0.0025\n",
      "Epoch 156/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7921 - loss: 0.3883 \n",
      "Epoch 156: val_loss did not improve from 0.36646\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7917 - loss: 0.3884 - val_accuracy: 0.8060 - val_loss: 0.3666 - learning_rate: 0.0012\n",
      "Epoch 157/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8010 - loss: 0.3858 \n",
      "Epoch 157: val_loss improved from 0.36646 to 0.36476, saving model to folds2.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7991 - loss: 0.3864 - val_accuracy: 0.7999 - val_loss: 0.3648 - learning_rate: 0.0012\n",
      "Epoch 158/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7748 - loss: 0.3974 \n",
      "Epoch 158: val_loss did not improve from 0.36476\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7810 - loss: 0.3932 - val_accuracy: 0.8084 - val_loss: 0.3655 - learning_rate: 0.0012\n",
      "Epoch 159/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7948 - loss: 0.3909 \n",
      "Epoch 159: val_loss did not improve from 0.36476\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7934 - loss: 0.3899 - val_accuracy: 0.8035 - val_loss: 0.3651 - learning_rate: 0.0012\n",
      "Epoch 160/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7942 - loss: 0.3852 \n",
      "Epoch 160: val_loss did not improve from 0.36476\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7946 - loss: 0.3858 - val_accuracy: 0.8011 - val_loss: 0.3648 - learning_rate: 0.0012\n",
      "Epoch 161/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7840 - loss: 0.3872 \n",
      "Epoch 161: val_loss improved from 0.36476 to 0.36419, saving model to folds2.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7867 - loss: 0.3874 - val_accuracy: 0.8084 - val_loss: 0.3642 - learning_rate: 0.0012\n",
      "Epoch 162/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7936 - loss: 0.3866 \n",
      "Epoch 162: val_loss did not improve from 0.36419\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7935 - loss: 0.3873 - val_accuracy: 0.8048 - val_loss: 0.3649 - learning_rate: 0.0012\n",
      "Epoch 163/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7992 - loss: 0.3799 \n",
      "Epoch 163: val_loss improved from 0.36419 to 0.36336, saving model to folds2.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7977 - loss: 0.3823 - val_accuracy: 0.8041 - val_loss: 0.3634 - learning_rate: 0.0012\n",
      "Epoch 164/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7950 - loss: 0.3904 \n",
      "Epoch 164: val_loss improved from 0.36336 to 0.36320, saving model to folds2.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7950 - loss: 0.3889 - val_accuracy: 0.8060 - val_loss: 0.3632 - learning_rate: 0.0012\n",
      "Epoch 165/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7845 - loss: 0.3944 \n",
      "Epoch 165: val_loss did not improve from 0.36320\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7874 - loss: 0.3912 - val_accuracy: 0.8078 - val_loss: 0.3637 - learning_rate: 0.0012\n",
      "Epoch 166/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7957 - loss: 0.3874 \n",
      "Epoch 166: val_loss did not improve from 0.36320\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7946 - loss: 0.3863 - val_accuracy: 0.8115 - val_loss: 0.3637 - learning_rate: 0.0012\n",
      "Epoch 167/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7950 - loss: 0.3877 \n",
      "Epoch 167: val_loss did not improve from 0.36320\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7952 - loss: 0.3868 - val_accuracy: 0.8060 - val_loss: 0.3639 - learning_rate: 0.0012\n",
      "Epoch 168/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8003 - loss: 0.3821 \n",
      "Epoch 168: val_loss did not improve from 0.36320\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7993 - loss: 0.3832 - val_accuracy: 0.8066 - val_loss: 0.3639 - learning_rate: 0.0012\n",
      "Epoch 169/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7935 - loss: 0.3831 \n",
      "Epoch 169: val_loss did not improve from 0.36320\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7939 - loss: 0.3840 - val_accuracy: 0.8084 - val_loss: 0.3635 - learning_rate: 0.0012\n",
      "Epoch 170/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7923 - loss: 0.3865 \n",
      "Epoch 170: val_loss did not improve from 0.36320\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7932 - loss: 0.3861 - val_accuracy: 0.8072 - val_loss: 0.3642 - learning_rate: 0.0012\n",
      "Epoch 171/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7980 - loss: 0.3762 \n",
      "Epoch 171: val_loss improved from 0.36320 to 0.36276, saving model to folds2.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7965 - loss: 0.3799 - val_accuracy: 0.8078 - val_loss: 0.3628 - learning_rate: 0.0012\n",
      "Epoch 172/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7949 - loss: 0.3885 \n",
      "Epoch 172: val_loss did not improve from 0.36276\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7949 - loss: 0.3871 - val_accuracy: 0.8090 - val_loss: 0.3629 - learning_rate: 0.0012\n",
      "Epoch 173/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7949 - loss: 0.3890 \n",
      "Epoch 173: val_loss did not improve from 0.36276\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7948 - loss: 0.3874 - val_accuracy: 0.8072 - val_loss: 0.3634 - learning_rate: 0.0012\n",
      "Epoch 174/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7942 - loss: 0.3855  \n",
      "Epoch 174: val_loss did not improve from 0.36276\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7945 - loss: 0.3850 - val_accuracy: 0.8078 - val_loss: 0.3635 - learning_rate: 0.0012\n",
      "Epoch 175/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7947 - loss: 0.3822 \n",
      "Epoch 175: val_loss did not improve from 0.36276\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7947 - loss: 0.3824 - val_accuracy: 0.8096 - val_loss: 0.3652 - learning_rate: 0.0012\n",
      "Epoch 176/300\n",
      "\u001b[1m 7/15\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7907 - loss: 0.3933 \n",
      "Epoch 176: val_loss did not improve from 0.36276\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7924 - loss: 0.3898 - val_accuracy: 0.8151 - val_loss: 0.3638 - learning_rate: 0.0012\n",
      "Epoch 177/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7940 - loss: 0.3941 \n",
      "Epoch 177: val_loss did not improve from 0.36276\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7939 - loss: 0.3909 - val_accuracy: 0.8103 - val_loss: 0.3642 - learning_rate: 0.0012\n",
      "Epoch 178/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7960 - loss: 0.3872 \n",
      "Epoch 178: val_loss improved from 0.36276 to 0.36213, saving model to folds2.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7957 - loss: 0.3866 - val_accuracy: 0.8066 - val_loss: 0.3621 - learning_rate: 0.0012\n",
      "Epoch 179/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7907 - loss: 0.3885 \n",
      "Epoch 179: val_loss did not improve from 0.36213\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7934 - loss: 0.3871 - val_accuracy: 0.8066 - val_loss: 0.3627 - learning_rate: 0.0012\n",
      "Epoch 180/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7916 - loss: 0.3885 \n",
      "Epoch 180: val_loss improved from 0.36213 to 0.36186, saving model to folds2.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7938 - loss: 0.3857 - val_accuracy: 0.8072 - val_loss: 0.3619 - learning_rate: 0.0012\n",
      "Epoch 181/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7967 - loss: 0.3765 \n",
      "Epoch 181: val_loss did not improve from 0.36186\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7953 - loss: 0.3794 - val_accuracy: 0.8139 - val_loss: 0.3624 - learning_rate: 0.0012\n",
      "Epoch 182/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8022 - loss: 0.3814 \n",
      "Epoch 182: val_loss improved from 0.36186 to 0.36184, saving model to folds2.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7995 - loss: 0.3822 - val_accuracy: 0.8090 - val_loss: 0.3618 - learning_rate: 0.0012\n",
      "Epoch 183/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7879 - loss: 0.3871  \n",
      "Epoch 183: val_loss did not improve from 0.36184\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7908 - loss: 0.3862 - val_accuracy: 0.8121 - val_loss: 0.3620 - learning_rate: 0.0012\n",
      "Epoch 184/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8032 - loss: 0.3755 \n",
      "Epoch 184: val_loss improved from 0.36184 to 0.36156, saving model to folds2.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8003 - loss: 0.3784 - val_accuracy: 0.8072 - val_loss: 0.3616 - learning_rate: 0.0012\n",
      "Epoch 185/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7971 - loss: 0.3827 \n",
      "Epoch 185: val_loss did not improve from 0.36156\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7957 - loss: 0.3844 - val_accuracy: 0.8084 - val_loss: 0.3620 - learning_rate: 0.0012\n",
      "Epoch 186/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7987 - loss: 0.3856  \n",
      "Epoch 186: val_loss did not improve from 0.36156\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7982 - loss: 0.3852 - val_accuracy: 0.8072 - val_loss: 0.3624 - learning_rate: 0.0012\n",
      "Epoch 187/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7913 - loss: 0.3839 \n",
      "Epoch 187: val_loss did not improve from 0.36156\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7923 - loss: 0.3837 - val_accuracy: 0.8145 - val_loss: 0.3620 - learning_rate: 0.0012\n",
      "Epoch 188/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7979 - loss: 0.3818 \n",
      "Epoch 188: val_loss improved from 0.36156 to 0.36137, saving model to folds2.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7973 - loss: 0.3820 - val_accuracy: 0.8084 - val_loss: 0.3614 - learning_rate: 0.0012\n",
      "Epoch 189/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7970 - loss: 0.3784 \n",
      "Epoch 189: val_loss did not improve from 0.36137\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7970 - loss: 0.3797 - val_accuracy: 0.8029 - val_loss: 0.3617 - learning_rate: 0.0012\n",
      "Epoch 190/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7917 - loss: 0.3903 \n",
      "Epoch 190: val_loss improved from 0.36137 to 0.36110, saving model to folds2.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7928 - loss: 0.3874 - val_accuracy: 0.8096 - val_loss: 0.3611 - learning_rate: 0.0012\n",
      "Epoch 191/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7925 - loss: 0.3845\n",
      "Epoch 191: val_loss did not improve from 0.36110\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.7935 - loss: 0.3844 - val_accuracy: 0.8066 - val_loss: 0.3620 - learning_rate: 0.0012\n",
      "Epoch 192/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7943 - loss: 0.3873 \n",
      "Epoch 192: val_loss improved from 0.36110 to 0.36072, saving model to folds2.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.7943 - loss: 0.3847 - val_accuracy: 0.8109 - val_loss: 0.3607 - learning_rate: 0.0012\n",
      "Epoch 193/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7910 - loss: 0.3866\n",
      "Epoch 193: val_loss did not improve from 0.36072\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7915 - loss: 0.3863 - val_accuracy: 0.8109 - val_loss: 0.3611 - learning_rate: 0.0012\n",
      "Epoch 194/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7942 - loss: 0.3853\n",
      "Epoch 194: val_loss did not improve from 0.36072\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.7943 - loss: 0.3850 - val_accuracy: 0.8084 - val_loss: 0.3612 - learning_rate: 0.0012\n",
      "Epoch 195/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7997 - loss: 0.3818\n",
      "Epoch 195: val_loss improved from 0.36072 to 0.36032, saving model to folds2.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.7980 - loss: 0.3825 - val_accuracy: 0.8103 - val_loss: 0.3603 - learning_rate: 0.0012\n",
      "Epoch 196/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7946 - loss: 0.3783\n",
      "Epoch 196: val_loss did not improve from 0.36032\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.7941 - loss: 0.3803 - val_accuracy: 0.7993 - val_loss: 0.3604 - learning_rate: 0.0012\n",
      "Epoch 197/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7943 - loss: 0.3815\n",
      "Epoch 197: val_loss did not improve from 0.36032\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.7949 - loss: 0.3820 - val_accuracy: 0.8054 - val_loss: 0.3613 - learning_rate: 0.0012\n",
      "Epoch 198/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7998 - loss: 0.3761\n",
      "Epoch 198: val_loss did not improve from 0.36032\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7995 - loss: 0.3765 - val_accuracy: 0.8041 - val_loss: 0.3610 - learning_rate: 0.0012\n",
      "Epoch 199/300\n",
      "\u001b[1m13/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7901 - loss: 0.3870 \n",
      "Epoch 199: val_loss did not improve from 0.36032\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7908 - loss: 0.3863 - val_accuracy: 0.8029 - val_loss: 0.3611 - learning_rate: 0.0012\n",
      "Epoch 200/300\n",
      "\u001b[1m13/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7972 - loss: 0.3810\n",
      "Epoch 200: val_loss did not improve from 0.36032\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.7966 - loss: 0.3815 - val_accuracy: 0.8072 - val_loss: 0.3626 - learning_rate: 0.0012\n",
      "Epoch 201/300\n",
      "\u001b[1m12/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7945 - loss: 0.3844\n",
      "Epoch 201: val_loss did not improve from 0.36032\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.7946 - loss: 0.3841 - val_accuracy: 0.8066 - val_loss: 0.3604 - learning_rate: 0.0012\n",
      "Epoch 202/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7943 - loss: 0.3821 \n",
      "Epoch 202: val_loss did not improve from 0.36032\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7947 - loss: 0.3819 - val_accuracy: 0.8054 - val_loss: 0.3609 - learning_rate: 0.0012\n",
      "Epoch 203/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7979 - loss: 0.3785 \n",
      "Epoch 203: val_loss did not improve from 0.36032\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7962 - loss: 0.3797 - val_accuracy: 0.8103 - val_loss: 0.3608 - learning_rate: 0.0012\n",
      "Epoch 204/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7991 - loss: 0.3798 \n",
      "Epoch 204: val_loss did not improve from 0.36032\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7977 - loss: 0.3804 - val_accuracy: 0.8078 - val_loss: 0.3606 - learning_rate: 0.0012\n",
      "Epoch 205/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7986 - loss: 0.3860 \n",
      "Epoch 205: val_loss improved from 0.36032 to 0.35933, saving model to folds2.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7975 - loss: 0.3843 - val_accuracy: 0.8096 - val_loss: 0.3593 - learning_rate: 0.0012\n",
      "Epoch 206/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7918 - loss: 0.3916 \n",
      "Epoch 206: val_loss did not improve from 0.35933\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7931 - loss: 0.3881 - val_accuracy: 0.8096 - val_loss: 0.3607 - learning_rate: 0.0012\n",
      "Epoch 207/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7912 - loss: 0.3862 \n",
      "Epoch 207: val_loss did not improve from 0.35933\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7909 - loss: 0.3854 - val_accuracy: 0.8054 - val_loss: 0.3597 - learning_rate: 0.0012\n",
      "Epoch 208/300\n",
      "\u001b[1m 7/15\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7944 - loss: 0.3851 \n",
      "Epoch 208: val_loss did not improve from 0.35933\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7944 - loss: 0.3833 - val_accuracy: 0.8121 - val_loss: 0.3601 - learning_rate: 0.0012\n",
      "Epoch 209/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7895 - loss: 0.3858 \n",
      "Epoch 209: val_loss did not improve from 0.35933\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7926 - loss: 0.3841 - val_accuracy: 0.8066 - val_loss: 0.3596 - learning_rate: 0.0012\n",
      "Epoch 210/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7917 - loss: 0.3844 \n",
      "Epoch 210: val_loss improved from 0.35933 to 0.35820, saving model to folds2.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7925 - loss: 0.3838 - val_accuracy: 0.8048 - val_loss: 0.3582 - learning_rate: 0.0012\n",
      "Epoch 211/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7966 - loss: 0.3800 \n",
      "Epoch 211: val_loss did not improve from 0.35820\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7959 - loss: 0.3806 - val_accuracy: 0.8096 - val_loss: 0.3597 - learning_rate: 0.0012\n",
      "Epoch 212/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7935 - loss: 0.3847 \n",
      "Epoch 212: val_loss did not improve from 0.35820\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7943 - loss: 0.3835 - val_accuracy: 0.8133 - val_loss: 0.3587 - learning_rate: 0.0012\n",
      "Epoch 213/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7998 - loss: 0.3775\n",
      "Epoch 213: val_loss did not improve from 0.35820\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7996 - loss: 0.3778 - val_accuracy: 0.8182 - val_loss: 0.3596 - learning_rate: 0.0012\n",
      "Epoch 214/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7899 - loss: 0.3863 \n",
      "Epoch 214: val_loss did not improve from 0.35820\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7912 - loss: 0.3851 - val_accuracy: 0.8115 - val_loss: 0.3589 - learning_rate: 0.0012\n",
      "Epoch 215/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7922 - loss: 0.3877 \n",
      "Epoch 215: val_loss did not improve from 0.35820\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7932 - loss: 0.3858 - val_accuracy: 0.8103 - val_loss: 0.3603 - learning_rate: 0.0012\n",
      "Epoch 216/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7946 - loss: 0.3800 \n",
      "Epoch 216: val_loss did not improve from 0.35820\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7961 - loss: 0.3800 - val_accuracy: 0.8035 - val_loss: 0.3600 - learning_rate: 0.0012\n",
      "Epoch 217/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7887 - loss: 0.3899 \n",
      "Epoch 217: val_loss did not improve from 0.35820\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7912 - loss: 0.3865 - val_accuracy: 0.8121 - val_loss: 0.3585 - learning_rate: 0.0012\n",
      "Epoch 218/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8035 - loss: 0.3772 \n",
      "Epoch 218: val_loss did not improve from 0.35820\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8008 - loss: 0.3789 - val_accuracy: 0.8090 - val_loss: 0.3607 - learning_rate: 0.0012\n",
      "Epoch 219/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8013 - loss: 0.3789 \n",
      "Epoch 219: val_loss did not improve from 0.35820\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7987 - loss: 0.3805 - val_accuracy: 0.8066 - val_loss: 0.3595 - learning_rate: 0.0012\n",
      "Epoch 220/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7961 - loss: 0.3794 \n",
      "Epoch 220: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "\n",
      "Epoch 220: val_loss did not improve from 0.35820\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7960 - loss: 0.3802 - val_accuracy: 0.8096 - val_loss: 0.3597 - learning_rate: 0.0012\n",
      "Epoch 221/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8032 - loss: 0.3743 \n",
      "Epoch 221: val_loss improved from 0.35820 to 0.35817, saving model to folds2.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8013 - loss: 0.3767 - val_accuracy: 0.8096 - val_loss: 0.3582 - learning_rate: 6.2500e-04\n",
      "Epoch 222/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7937 - loss: 0.3840 \n",
      "Epoch 222: val_loss improved from 0.35817 to 0.35804, saving model to folds2.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7940 - loss: 0.3827 - val_accuracy: 0.8090 - val_loss: 0.3580 - learning_rate: 6.2500e-04\n",
      "Epoch 223/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7987 - loss: 0.3849 \n",
      "Epoch 223: val_loss did not improve from 0.35804\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7981 - loss: 0.3834 - val_accuracy: 0.8151 - val_loss: 0.3581 - learning_rate: 6.2500e-04\n",
      "Epoch 224/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8015 - loss: 0.3764  \n",
      "Epoch 224: val_loss improved from 0.35804 to 0.35745, saving model to folds2.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8007 - loss: 0.3776 - val_accuracy: 0.8090 - val_loss: 0.3575 - learning_rate: 6.2500e-04\n",
      "Epoch 225/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7907 - loss: 0.3802  \n",
      "Epoch 225: val_loss did not improve from 0.35745\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7925 - loss: 0.3798 - val_accuracy: 0.8115 - val_loss: 0.3581 - learning_rate: 6.2500e-04\n",
      "Epoch 226/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7956 - loss: 0.3841 \n",
      "Epoch 226: val_loss did not improve from 0.35745\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7969 - loss: 0.3820 - val_accuracy: 0.8133 - val_loss: 0.3576 - learning_rate: 6.2500e-04\n",
      "Epoch 227/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7995 - loss: 0.3756 \n",
      "Epoch 227: val_loss did not improve from 0.35745\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7984 - loss: 0.3771 - val_accuracy: 0.8145 - val_loss: 0.3576 - learning_rate: 6.2500e-04\n",
      "Epoch 228/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8003 - loss: 0.3735 \n",
      "Epoch 228: val_loss did not improve from 0.35745\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7994 - loss: 0.3752 - val_accuracy: 0.8090 - val_loss: 0.3578 - learning_rate: 6.2500e-04\n",
      "Epoch 229/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7862 - loss: 0.3860 \n",
      "Epoch 229: val_loss improved from 0.35745 to 0.35727, saving model to folds2.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7901 - loss: 0.3832 - val_accuracy: 0.8115 - val_loss: 0.3573 - learning_rate: 6.2500e-04\n",
      "Epoch 230/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7927 - loss: 0.3836 \n",
      "Epoch 230: val_loss did not improve from 0.35727\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7947 - loss: 0.3818 - val_accuracy: 0.8109 - val_loss: 0.3578 - learning_rate: 6.2500e-04\n",
      "Epoch 231/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7947 - loss: 0.3779 \n",
      "Epoch 231: val_loss improved from 0.35727 to 0.35630, saving model to folds2.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7965 - loss: 0.3783 - val_accuracy: 0.8164 - val_loss: 0.3563 - learning_rate: 6.2500e-04\n",
      "Epoch 232/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7878 - loss: 0.3871 \n",
      "Epoch 232: val_loss did not improve from 0.35630\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7915 - loss: 0.3845 - val_accuracy: 0.8157 - val_loss: 0.3565 - learning_rate: 6.2500e-04\n",
      "Epoch 233/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7998 - loss: 0.3786 \n",
      "Epoch 233: val_loss did not improve from 0.35630\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7989 - loss: 0.3786 - val_accuracy: 0.8127 - val_loss: 0.3571 - learning_rate: 6.2500e-04\n",
      "Epoch 234/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7997 - loss: 0.3819 \n",
      "Epoch 234: val_loss did not improve from 0.35630\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8002 - loss: 0.3810 - val_accuracy: 0.8164 - val_loss: 0.3563 - learning_rate: 6.2500e-04\n",
      "Epoch 235/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8001 - loss: 0.3796 \n",
      "Epoch 235: val_loss improved from 0.35630 to 0.35574, saving model to folds2.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8008 - loss: 0.3781 - val_accuracy: 0.8206 - val_loss: 0.3557 - learning_rate: 6.2500e-04\n",
      "Epoch 236/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8030 - loss: 0.3758 \n",
      "Epoch 236: val_loss did not improve from 0.35574\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8013 - loss: 0.3770 - val_accuracy: 0.8182 - val_loss: 0.3563 - learning_rate: 6.2500e-04\n",
      "Epoch 237/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8012 - loss: 0.3796\n",
      "Epoch 237: val_loss did not improve from 0.35574\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8010 - loss: 0.3795 - val_accuracy: 0.8176 - val_loss: 0.3564 - learning_rate: 6.2500e-04\n",
      "Epoch 238/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7994 - loss: 0.3804 \n",
      "Epoch 238: val_loss did not improve from 0.35574\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7990 - loss: 0.3795 - val_accuracy: 0.8200 - val_loss: 0.3560 - learning_rate: 6.2500e-04\n",
      "Epoch 239/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8131 - loss: 0.3720 \n",
      "Epoch 239: val_loss improved from 0.35574 to 0.35527, saving model to folds2.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8086 - loss: 0.3741 - val_accuracy: 0.8151 - val_loss: 0.3553 - learning_rate: 6.2500e-04\n",
      "Epoch 240/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7956 - loss: 0.3800 \n",
      "Epoch 240: val_loss improved from 0.35527 to 0.35507, saving model to folds2.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7967 - loss: 0.3794 - val_accuracy: 0.8182 - val_loss: 0.3551 - learning_rate: 6.2500e-04\n",
      "Epoch 241/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8072 - loss: 0.3682 \n",
      "Epoch 241: val_loss did not improve from 0.35507\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8036 - loss: 0.3720 - val_accuracy: 0.8200 - val_loss: 0.3562 - learning_rate: 6.2500e-04\n",
      "Epoch 242/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7992 - loss: 0.3754 \n",
      "Epoch 242: val_loss improved from 0.35507 to 0.35488, saving model to folds2.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7997 - loss: 0.3758 - val_accuracy: 0.8182 - val_loss: 0.3549 - learning_rate: 6.2500e-04\n",
      "Epoch 243/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7931 - loss: 0.3845\n",
      "Epoch 243: val_loss did not improve from 0.35488\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7941 - loss: 0.3836 - val_accuracy: 0.8151 - val_loss: 0.3550 - learning_rate: 6.2500e-04\n",
      "Epoch 244/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7943 - loss: 0.3791  \n",
      "Epoch 244: val_loss improved from 0.35488 to 0.35476, saving model to folds2.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7967 - loss: 0.3786 - val_accuracy: 0.8164 - val_loss: 0.3548 - learning_rate: 6.2500e-04\n",
      "Epoch 245/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7959 - loss: 0.3786 \n",
      "Epoch 245: val_loss did not improve from 0.35476\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7970 - loss: 0.3782 - val_accuracy: 0.8194 - val_loss: 0.3553 - learning_rate: 6.2500e-04\n",
      "Epoch 246/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7927 - loss: 0.3855 \n",
      "Epoch 246: val_loss improved from 0.35476 to 0.35438, saving model to folds2.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7961 - loss: 0.3819 - val_accuracy: 0.8164 - val_loss: 0.3544 - learning_rate: 6.2500e-04\n",
      "Epoch 247/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7972 - loss: 0.3743 \n",
      "Epoch 247: val_loss did not improve from 0.35438\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7987 - loss: 0.3752 - val_accuracy: 0.8194 - val_loss: 0.3551 - learning_rate: 6.2500e-04\n",
      "Epoch 248/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8096 - loss: 0.3683 \n",
      "Epoch 248: val_loss did not improve from 0.35438\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8059 - loss: 0.3714 - val_accuracy: 0.8218 - val_loss: 0.3551 - learning_rate: 6.2500e-04\n",
      "Epoch 249/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8068 - loss: 0.3780\n",
      "Epoch 249: val_loss did not improve from 0.35438\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8064 - loss: 0.3779 - val_accuracy: 0.8157 - val_loss: 0.3550 - learning_rate: 6.2500e-04\n",
      "Epoch 250/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8036 - loss: 0.3721\n",
      "Epoch 250: val_loss did not improve from 0.35438\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.8030 - loss: 0.3734 - val_accuracy: 0.8176 - val_loss: 0.3554 - learning_rate: 6.2500e-04\n",
      "Epoch 251/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7979 - loss: 0.3780\n",
      "Epoch 251: val_loss did not improve from 0.35438\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.7980 - loss: 0.3779 - val_accuracy: 0.8182 - val_loss: 0.3552 - learning_rate: 6.2500e-04\n",
      "Epoch 252/300\n",
      "\u001b[1m12/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8036 - loss: 0.3746\n",
      "Epoch 252: val_loss improved from 0.35438 to 0.35412, saving model to folds2.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.8031 - loss: 0.3751 - val_accuracy: 0.8206 - val_loss: 0.3541 - learning_rate: 6.2500e-04\n",
      "Epoch 253/300\n",
      "\u001b[1m13/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7996 - loss: 0.3771\n",
      "Epoch 253: val_loss did not improve from 0.35412\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.7999 - loss: 0.3769 - val_accuracy: 0.8206 - val_loss: 0.3548 - learning_rate: 6.2500e-04\n",
      "Epoch 254/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8029 - loss: 0.3781\n",
      "Epoch 254: val_loss improved from 0.35412 to 0.35373, saving model to folds2.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.8029 - loss: 0.3780 - val_accuracy: 0.8212 - val_loss: 0.3537 - learning_rate: 6.2500e-04\n",
      "Epoch 255/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8026 - loss: 0.3722\n",
      "Epoch 255: val_loss did not improve from 0.35373\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.8025 - loss: 0.3732 - val_accuracy: 0.8164 - val_loss: 0.3540 - learning_rate: 6.2500e-04\n",
      "Epoch 256/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7974 - loss: 0.3801 \n",
      "Epoch 256: val_loss did not improve from 0.35373\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7996 - loss: 0.3782 - val_accuracy: 0.8182 - val_loss: 0.3540 - learning_rate: 6.2500e-04\n",
      "Epoch 257/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8009 - loss: 0.3778\n",
      "Epoch 257: val_loss did not improve from 0.35373\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.8009 - loss: 0.3777 - val_accuracy: 0.8182 - val_loss: 0.3542 - learning_rate: 6.2500e-04\n",
      "Epoch 258/300\n",
      "\u001b[1m13/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8038 - loss: 0.3735 \n",
      "Epoch 258: val_loss improved from 0.35373 to 0.35367, saving model to folds2.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.8037 - loss: 0.3739 - val_accuracy: 0.8188 - val_loss: 0.3537 - learning_rate: 6.2500e-04\n",
      "Epoch 259/300\n",
      "\u001b[1m12/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8046 - loss: 0.3701 \n",
      "Epoch 259: val_loss did not improve from 0.35367\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8038 - loss: 0.3714 - val_accuracy: 0.8188 - val_loss: 0.3548 - learning_rate: 6.2500e-04\n",
      "Epoch 260/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8034 - loss: 0.3728 \n",
      "Epoch 260: val_loss did not improve from 0.35367\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8025 - loss: 0.3742 - val_accuracy: 0.8194 - val_loss: 0.3554 - learning_rate: 6.2500e-04\n",
      "Epoch 261/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7991 - loss: 0.3731 \n",
      "Epoch 261: val_loss did not improve from 0.35367\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8003 - loss: 0.3742 - val_accuracy: 0.8182 - val_loss: 0.3538 - learning_rate: 6.2500e-04\n",
      "Epoch 262/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7999 - loss: 0.3743 \n",
      "Epoch 262: val_loss improved from 0.35367 to 0.35355, saving model to folds2.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8004 - loss: 0.3746 - val_accuracy: 0.8188 - val_loss: 0.3535 - learning_rate: 6.2500e-04\n",
      "Epoch 263/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8047 - loss: 0.3788 \n",
      "Epoch 263: val_loss improved from 0.35355 to 0.35353, saving model to folds2.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8028 - loss: 0.3774 - val_accuracy: 0.8170 - val_loss: 0.3535 - learning_rate: 6.2500e-04\n",
      "Epoch 264/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8011 - loss: 0.3785  \n",
      "Epoch 264: val_loss did not improve from 0.35353\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8015 - loss: 0.3773 - val_accuracy: 0.8164 - val_loss: 0.3541 - learning_rate: 6.2500e-04\n",
      "Epoch 265/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7959 - loss: 0.3747 \n",
      "Epoch 265: val_loss improved from 0.35353 to 0.35312, saving model to folds2.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7986 - loss: 0.3754 - val_accuracy: 0.8206 - val_loss: 0.3531 - learning_rate: 6.2500e-04\n",
      "Epoch 266/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7978 - loss: 0.3778 \n",
      "Epoch 266: val_loss did not improve from 0.35312\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7989 - loss: 0.3767 - val_accuracy: 0.8206 - val_loss: 0.3538 - learning_rate: 6.2500e-04\n",
      "Epoch 267/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8073 - loss: 0.3687 \n",
      "Epoch 267: val_loss improved from 0.35312 to 0.35288, saving model to folds2.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8044 - loss: 0.3715 - val_accuracy: 0.8194 - val_loss: 0.3529 - learning_rate: 6.2500e-04\n",
      "Epoch 268/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8039 - loss: 0.3777 \n",
      "Epoch 268: val_loss did not improve from 0.35288\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8029 - loss: 0.3774 - val_accuracy: 0.8194 - val_loss: 0.3533 - learning_rate: 6.2500e-04\n",
      "Epoch 269/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8055 - loss: 0.3730 \n",
      "Epoch 269: val_loss improved from 0.35288 to 0.35286, saving model to folds2.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8036 - loss: 0.3740 - val_accuracy: 0.8225 - val_loss: 0.3529 - learning_rate: 6.2500e-04\n",
      "Epoch 270/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8049 - loss: 0.3701 \n",
      "Epoch 270: val_loss did not improve from 0.35286\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8036 - loss: 0.3718 - val_accuracy: 0.8218 - val_loss: 0.3534 - learning_rate: 6.2500e-04\n",
      "Epoch 271/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8018 - loss: 0.3733 \n",
      "Epoch 271: val_loss did not improve from 0.35286\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8018 - loss: 0.3740 - val_accuracy: 0.8206 - val_loss: 0.3531 - learning_rate: 6.2500e-04\n",
      "Epoch 272/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8071 - loss: 0.3671 \n",
      "Epoch 272: val_loss did not improve from 0.35286\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8058 - loss: 0.3698 - val_accuracy: 0.8182 - val_loss: 0.3530 - learning_rate: 6.2500e-04\n",
      "Epoch 273/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8000 - loss: 0.3705 \n",
      "Epoch 273: val_loss improved from 0.35286 to 0.35227, saving model to folds2.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8012 - loss: 0.3721 - val_accuracy: 0.8243 - val_loss: 0.3523 - learning_rate: 6.2500e-04\n",
      "Epoch 274/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8008 - loss: 0.3745 \n",
      "Epoch 274: val_loss did not improve from 0.35227\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8011 - loss: 0.3746 - val_accuracy: 0.8206 - val_loss: 0.3543 - learning_rate: 6.2500e-04\n",
      "Epoch 275/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8058 - loss: 0.3717 \n",
      "Epoch 275: val_loss improved from 0.35227 to 0.35187, saving model to folds2.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8041 - loss: 0.3731 - val_accuracy: 0.8176 - val_loss: 0.3519 - learning_rate: 6.2500e-04\n",
      "Epoch 276/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7947 - loss: 0.3798 \n",
      "Epoch 276: val_loss did not improve from 0.35187\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7979 - loss: 0.3773 - val_accuracy: 0.8182 - val_loss: 0.3519 - learning_rate: 6.2500e-04\n",
      "Epoch 277/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8074 - loss: 0.3682 \n",
      "Epoch 277: val_loss did not improve from 0.35187\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8048 - loss: 0.3712 - val_accuracy: 0.8206 - val_loss: 0.3526 - learning_rate: 6.2500e-04\n",
      "Epoch 278/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8009 - loss: 0.3778 \n",
      "Epoch 278: val_loss did not improve from 0.35187\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8013 - loss: 0.3761 - val_accuracy: 0.8206 - val_loss: 0.3524 - learning_rate: 6.2500e-04\n",
      "Epoch 279/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8047 - loss: 0.3711 \n",
      "Epoch 279: val_loss improved from 0.35187 to 0.35163, saving model to folds2.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8039 - loss: 0.3722 - val_accuracy: 0.8182 - val_loss: 0.3516 - learning_rate: 6.2500e-04\n",
      "Epoch 280/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8122 - loss: 0.3657 \n",
      "Epoch 280: val_loss did not improve from 0.35163\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8085 - loss: 0.3690 - val_accuracy: 0.8200 - val_loss: 0.3518 - learning_rate: 6.2500e-04\n",
      "Epoch 281/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8061 - loss: 0.3697 \n",
      "Epoch 281: val_loss did not improve from 0.35163\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8050 - loss: 0.3712 - val_accuracy: 0.8188 - val_loss: 0.3525 - learning_rate: 6.2500e-04\n",
      "Epoch 282/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8053 - loss: 0.3686  \n",
      "Epoch 282: val_loss did not improve from 0.35163\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8044 - loss: 0.3699 - val_accuracy: 0.8206 - val_loss: 0.3518 - learning_rate: 6.2500e-04\n",
      "Epoch 283/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8023 - loss: 0.3741 \n",
      "Epoch 283: val_loss did not improve from 0.35163\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8024 - loss: 0.3740 - val_accuracy: 0.8188 - val_loss: 0.3519 - learning_rate: 6.2500e-04\n",
      "Epoch 284/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8082 - loss: 0.3696 \n",
      "Epoch 284: val_loss did not improve from 0.35163\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8061 - loss: 0.3709 - val_accuracy: 0.8206 - val_loss: 0.3522 - learning_rate: 6.2500e-04\n",
      "Epoch 285/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8086 - loss: 0.3700  \n",
      "Epoch 285: val_loss improved from 0.35163 to 0.35140, saving model to folds2.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8062 - loss: 0.3719 - val_accuracy: 0.8200 - val_loss: 0.3514 - learning_rate: 6.2500e-04\n",
      "Epoch 286/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8038 - loss: 0.3734  \n",
      "Epoch 286: val_loss improved from 0.35140 to 0.35106, saving model to folds2.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8035 - loss: 0.3738 - val_accuracy: 0.8231 - val_loss: 0.3511 - learning_rate: 6.2500e-04\n",
      "Epoch 287/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8041 - loss: 0.3748 \n",
      "Epoch 287: val_loss did not improve from 0.35106\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8039 - loss: 0.3745 - val_accuracy: 0.8170 - val_loss: 0.3517 - learning_rate: 6.2500e-04\n",
      "Epoch 288/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8022 - loss: 0.3714 \n",
      "Epoch 288: val_loss did not improve from 0.35106\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8015 - loss: 0.3719 - val_accuracy: 0.8206 - val_loss: 0.3515 - learning_rate: 6.2500e-04\n",
      "Epoch 289/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8057 - loss: 0.3735 \n",
      "Epoch 289: val_loss improved from 0.35106 to 0.35068, saving model to folds2.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8054 - loss: 0.3733 - val_accuracy: 0.8176 - val_loss: 0.3507 - learning_rate: 6.2500e-04\n",
      "Epoch 290/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8001 - loss: 0.3694 \n",
      "Epoch 290: val_loss did not improve from 0.35068\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8006 - loss: 0.3703 - val_accuracy: 0.8200 - val_loss: 0.3510 - learning_rate: 6.2500e-04\n",
      "Epoch 291/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8049 - loss: 0.3744 \n",
      "Epoch 291: val_loss did not improve from 0.35068\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8056 - loss: 0.3730 - val_accuracy: 0.8182 - val_loss: 0.3514 - learning_rate: 6.2500e-04\n",
      "Epoch 292/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8088 - loss: 0.3680  \n",
      "Epoch 292: val_loss did not improve from 0.35068\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8067 - loss: 0.3702 - val_accuracy: 0.8188 - val_loss: 0.3507 - learning_rate: 6.2500e-04\n",
      "Epoch 293/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8036 - loss: 0.3708 \n",
      "Epoch 293: val_loss did not improve from 0.35068\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8040 - loss: 0.3713 - val_accuracy: 0.8164 - val_loss: 0.3511 - learning_rate: 6.2500e-04\n",
      "Epoch 294/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8102 - loss: 0.3665 \n",
      "Epoch 294: val_loss improved from 0.35068 to 0.35048, saving model to folds2.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8079 - loss: 0.3687 - val_accuracy: 0.8212 - val_loss: 0.3505 - learning_rate: 6.2500e-04\n",
      "Epoch 295/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8077 - loss: 0.3694 \n",
      "Epoch 295: val_loss did not improve from 0.35048\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8058 - loss: 0.3704 - val_accuracy: 0.8182 - val_loss: 0.3510 - learning_rate: 6.2500e-04\n",
      "Epoch 296/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7983 - loss: 0.3730\n",
      "Epoch 296: val_loss did not improve from 0.35048\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7988 - loss: 0.3728 - val_accuracy: 0.8212 - val_loss: 0.3508 - learning_rate: 6.2500e-04\n",
      "Epoch 297/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8064 - loss: 0.3751 \n",
      "Epoch 297: val_loss improved from 0.35048 to 0.35031, saving model to folds2.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8060 - loss: 0.3742 - val_accuracy: 0.8188 - val_loss: 0.3503 - learning_rate: 6.2500e-04\n",
      "Epoch 298/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8036 - loss: 0.3720 \n",
      "Epoch 298: val_loss improved from 0.35031 to 0.35015, saving model to folds2.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8037 - loss: 0.3720 - val_accuracy: 0.8194 - val_loss: 0.3501 - learning_rate: 6.2500e-04\n",
      "Epoch 299/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8071 - loss: 0.3685 \n",
      "Epoch 299: val_loss did not improve from 0.35015\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8063 - loss: 0.3690 - val_accuracy: 0.8176 - val_loss: 0.3511 - learning_rate: 6.2500e-04\n",
      "Epoch 300/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8050 - loss: 0.3694\n",
      "Epoch 300: val_loss improved from 0.35015 to 0.35015, saving model to folds2.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8049 - loss: 0.3696 - val_accuracy: 0.8261 - val_loss: 0.3501 - learning_rate: 6.2500e-04\n",
      "Restoring model weights from the end of the best epoch: 300.\n",
      " Computing LSTM feature importance...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:02<00:00,  3.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- > Fold 4 < ---------------\n",
      "Epoch 1/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5888 - loss: 0.6617\n",
      "Epoch 1: val_loss improved from inf to 0.53558, saving model to folds3.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step - accuracy: 0.6289 - loss: 0.6273 - val_accuracy: 0.6968 - val_loss: 0.5356 - learning_rate: 0.0100\n",
      "Epoch 2/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7064 - loss: 0.5284 \n",
      "Epoch 2: val_loss improved from 0.53558 to 0.52119, saving model to folds3.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7092 - loss: 0.5265 - val_accuracy: 0.7120 - val_loss: 0.5212 - learning_rate: 0.0100\n",
      "Epoch 3/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7166 - loss: 0.5127 \n",
      "Epoch 3: val_loss improved from 0.52119 to 0.51415, saving model to folds3.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7179 - loss: 0.5113 - val_accuracy: 0.7096 - val_loss: 0.5141 - learning_rate: 0.0100\n",
      "Epoch 4/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7173 - loss: 0.5058 \n",
      "Epoch 4: val_loss improved from 0.51415 to 0.50579, saving model to folds3.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7179 - loss: 0.5037 - val_accuracy: 0.7077 - val_loss: 0.5058 - learning_rate: 0.0100\n",
      "Epoch 5/300\n",
      "\u001b[1m 7/15\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7195 - loss: 0.4872 \n",
      "Epoch 5: val_loss improved from 0.50579 to 0.49766, saving model to folds3.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7208 - loss: 0.4874 - val_accuracy: 0.7138 - val_loss: 0.4977 - learning_rate: 0.0100\n",
      "Epoch 6/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7234 - loss: 0.4852 \n",
      "Epoch 6: val_loss improved from 0.49766 to 0.49463, saving model to folds3.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7235 - loss: 0.4848 - val_accuracy: 0.7041 - val_loss: 0.4946 - learning_rate: 0.0100\n",
      "Epoch 7/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7337 - loss: 0.4798 \n",
      "Epoch 7: val_loss did not improve from 0.49463\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7314 - loss: 0.4815 - val_accuracy: 0.7023 - val_loss: 0.4964 - learning_rate: 0.0100\n",
      "Epoch 8/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7259 - loss: 0.4806 \n",
      "Epoch 8: val_loss improved from 0.49463 to 0.48627, saving model to folds3.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7255 - loss: 0.4815 - val_accuracy: 0.7181 - val_loss: 0.4863 - learning_rate: 0.0100\n",
      "Epoch 9/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7288 - loss: 0.4748 \n",
      "Epoch 9: val_loss did not improve from 0.48627\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7292 - loss: 0.4750 - val_accuracy: 0.7145 - val_loss: 0.4873 - learning_rate: 0.0100\n",
      "Epoch 10/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7303 - loss: 0.4751 \n",
      "Epoch 10: val_loss improved from 0.48627 to 0.47876, saving model to folds3.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7308 - loss: 0.4746 - val_accuracy: 0.7224 - val_loss: 0.4788 - learning_rate: 0.0100\n",
      "Epoch 11/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7384 - loss: 0.4636 \n",
      "Epoch 11: val_loss did not improve from 0.47876\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7361 - loss: 0.4671 - val_accuracy: 0.7212 - val_loss: 0.4908 - learning_rate: 0.0100\n",
      "Epoch 12/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7406 - loss: 0.4695 \n",
      "Epoch 12: val_loss improved from 0.47876 to 0.47700, saving model to folds3.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7401 - loss: 0.4701 - val_accuracy: 0.7315 - val_loss: 0.4770 - learning_rate: 0.0100\n",
      "Epoch 13/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7441 - loss: 0.4594 \n",
      "Epoch 13: val_loss improved from 0.47700 to 0.47295, saving model to folds3.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7443 - loss: 0.4611 - val_accuracy: 0.7230 - val_loss: 0.4729 - learning_rate: 0.0100\n",
      "Epoch 14/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7517 - loss: 0.4604 \n",
      "Epoch 14: val_loss did not improve from 0.47295\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7488 - loss: 0.4619 - val_accuracy: 0.7248 - val_loss: 0.4810 - learning_rate: 0.0100\n",
      "Epoch 15/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7509 - loss: 0.4609 \n",
      "Epoch 15: val_loss did not improve from 0.47295\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7508 - loss: 0.4607 - val_accuracy: 0.7254 - val_loss: 0.4754 - learning_rate: 0.0100\n",
      "Epoch 16/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7473 - loss: 0.4640 \n",
      "Epoch 16: val_loss did not improve from 0.47295\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7476 - loss: 0.4637 - val_accuracy: 0.7218 - val_loss: 0.4782 - learning_rate: 0.0100\n",
      "Epoch 17/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7447 - loss: 0.4593 \n",
      "Epoch 17: val_loss did not improve from 0.47295\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7466 - loss: 0.4605 - val_accuracy: 0.7322 - val_loss: 0.4731 - learning_rate: 0.0100\n",
      "Epoch 18/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7601 - loss: 0.4560 \n",
      "Epoch 18: val_loss did not improve from 0.47295\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7592 - loss: 0.4570 - val_accuracy: 0.7151 - val_loss: 0.4798 - learning_rate: 0.0100\n",
      "Epoch 19/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7484 - loss: 0.4655 \n",
      "Epoch 19: val_loss improved from 0.47295 to 0.47095, saving model to folds3.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7494 - loss: 0.4645 - val_accuracy: 0.7389 - val_loss: 0.4709 - learning_rate: 0.0100\n",
      "Epoch 20/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7479 - loss: 0.4625 \n",
      "Epoch 20: val_loss improved from 0.47095 to 0.46699, saving model to folds3.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7488 - loss: 0.4614 - val_accuracy: 0.7328 - val_loss: 0.4670 - learning_rate: 0.0100\n",
      "Epoch 21/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7588 - loss: 0.4480 \n",
      "Epoch 21: val_loss improved from 0.46699 to 0.46323, saving model to folds3.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7578 - loss: 0.4497 - val_accuracy: 0.7462 - val_loss: 0.4632 - learning_rate: 0.0100\n",
      "Epoch 22/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7556 - loss: 0.4476  \n",
      "Epoch 22: val_loss improved from 0.46323 to 0.46178, saving model to folds3.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7570 - loss: 0.4479 - val_accuracy: 0.7444 - val_loss: 0.4618 - learning_rate: 0.0100\n",
      "Epoch 23/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7617 - loss: 0.4535 \n",
      "Epoch 23: val_loss did not improve from 0.46178\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7618 - loss: 0.4529 - val_accuracy: 0.7486 - val_loss: 0.4671 - learning_rate: 0.0100\n",
      "Epoch 24/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7518 - loss: 0.4489 \n",
      "Epoch 24: val_loss did not improve from 0.46178\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7508 - loss: 0.4513 - val_accuracy: 0.7315 - val_loss: 0.4683 - learning_rate: 0.0100\n",
      "Epoch 25/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7464 - loss: 0.4591  \n",
      "Epoch 25: val_loss did not improve from 0.46178\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7492 - loss: 0.4566 - val_accuracy: 0.7090 - val_loss: 0.4748 - learning_rate: 0.0100\n",
      "Epoch 26/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7441 - loss: 0.4560 \n",
      "Epoch 26: val_loss did not improve from 0.46178\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7502 - loss: 0.4543 - val_accuracy: 0.7352 - val_loss: 0.4712 - learning_rate: 0.0100\n",
      "Epoch 27/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7504 - loss: 0.4577 \n",
      "Epoch 27: val_loss did not improve from 0.46178\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7524 - loss: 0.4565 - val_accuracy: 0.7523 - val_loss: 0.4646 - learning_rate: 0.0100\n",
      "Epoch 28/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7685 - loss: 0.4441  \n",
      "Epoch 28: val_loss did not improve from 0.46178\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7687 - loss: 0.4445 - val_accuracy: 0.7395 - val_loss: 0.4686 - learning_rate: 0.0100\n",
      "Epoch 29/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7559 - loss: 0.4547\n",
      "Epoch 29: val_loss did not improve from 0.46178\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7563 - loss: 0.4545 - val_accuracy: 0.7431 - val_loss: 0.4647 - learning_rate: 0.0100\n",
      "Epoch 30/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7577 - loss: 0.4499 \n",
      "Epoch 30: val_loss improved from 0.46178 to 0.46112, saving model to folds3.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7604 - loss: 0.4482 - val_accuracy: 0.7395 - val_loss: 0.4611 - learning_rate: 0.0100\n",
      "Epoch 31/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7673 - loss: 0.4433 \n",
      "Epoch 31: val_loss improved from 0.46112 to 0.45690, saving model to folds3.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7689 - loss: 0.4426 - val_accuracy: 0.7572 - val_loss: 0.4569 - learning_rate: 0.0100\n",
      "Epoch 32/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7728 - loss: 0.4346 \n",
      "Epoch 32: val_loss improved from 0.45690 to 0.45352, saving model to folds3.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7710 - loss: 0.4364 - val_accuracy: 0.7505 - val_loss: 0.4535 - learning_rate: 0.0100\n",
      "Epoch 33/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7705 - loss: 0.4429 \n",
      "Epoch 33: val_loss did not improve from 0.45352\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7702 - loss: 0.4431 - val_accuracy: 0.7456 - val_loss: 0.4640 - learning_rate: 0.0100\n",
      "Epoch 34/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7525 - loss: 0.4594 \n",
      "Epoch 34: val_loss did not improve from 0.45352\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7542 - loss: 0.4564 - val_accuracy: 0.7132 - val_loss: 0.4705 - learning_rate: 0.0100\n",
      "Epoch 35/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7554 - loss: 0.4448 \n",
      "Epoch 35: val_loss did not improve from 0.45352\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7579 - loss: 0.4442 - val_accuracy: 0.7498 - val_loss: 0.4537 - learning_rate: 0.0100\n",
      "Epoch 36/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7800 - loss: 0.4353  \n",
      "Epoch 36: val_loss did not improve from 0.45352\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7785 - loss: 0.4364 - val_accuracy: 0.7553 - val_loss: 0.4586 - learning_rate: 0.0100\n",
      "Epoch 37/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7769 - loss: 0.4376 \n",
      "Epoch 37: val_loss did not improve from 0.45352\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7760 - loss: 0.4374 - val_accuracy: 0.7401 - val_loss: 0.4596 - learning_rate: 0.0100\n",
      "Epoch 38/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7750 - loss: 0.4298 \n",
      "Epoch 38: val_loss improved from 0.45352 to 0.44741, saving model to folds3.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7743 - loss: 0.4317 - val_accuracy: 0.7663 - val_loss: 0.4474 - learning_rate: 0.0100\n",
      "Epoch 39/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7835 - loss: 0.4268 \n",
      "Epoch 39: val_loss did not improve from 0.44741\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7811 - loss: 0.4294 - val_accuracy: 0.7578 - val_loss: 0.4494 - learning_rate: 0.0100\n",
      "Epoch 40/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7770 - loss: 0.4284 \n",
      "Epoch 40: val_loss did not improve from 0.44741\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7773 - loss: 0.4288 - val_accuracy: 0.7718 - val_loss: 0.4476 - learning_rate: 0.0100\n",
      "Epoch 41/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7719 - loss: 0.4449\n",
      "Epoch 41: val_loss improved from 0.44741 to 0.44722, saving model to folds3.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.7722 - loss: 0.4442 - val_accuracy: 0.7608 - val_loss: 0.4472 - learning_rate: 0.0100\n",
      "Epoch 42/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7703 - loss: 0.4411\n",
      "Epoch 42: val_loss did not improve from 0.44722\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7702 - loss: 0.4411 - val_accuracy: 0.7572 - val_loss: 0.4661 - learning_rate: 0.0100\n",
      "Epoch 43/300\n",
      "\u001b[1m13/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7627 - loss: 0.4420\n",
      "Epoch 43: val_loss did not improve from 0.44722\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.7631 - loss: 0.4417 - val_accuracy: 0.7468 - val_loss: 0.4522 - learning_rate: 0.0100\n",
      "Epoch 44/300\n",
      "\u001b[1m12/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7829 - loss: 0.4285\n",
      "Epoch 44: val_loss improved from 0.44722 to 0.44320, saving model to folds3.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.7804 - loss: 0.4300 - val_accuracy: 0.7535 - val_loss: 0.4432 - learning_rate: 0.0100\n",
      "Epoch 45/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7787 - loss: 0.4298\n",
      "Epoch 45: val_loss did not improve from 0.44320\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7786 - loss: 0.4300 - val_accuracy: 0.7608 - val_loss: 0.4441 - learning_rate: 0.0100\n",
      "Epoch 46/300\n",
      "\u001b[1m12/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7839 - loss: 0.4242\n",
      "Epoch 46: val_loss improved from 0.44320 to 0.44298, saving model to folds3.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.7835 - loss: 0.4248 - val_accuracy: 0.7712 - val_loss: 0.4430 - learning_rate: 0.0100\n",
      "Epoch 47/300\n",
      "\u001b[1m12/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7854 - loss: 0.4263\n",
      "Epoch 47: val_loss improved from 0.44298 to 0.44104, saving model to folds3.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.7854 - loss: 0.4261 - val_accuracy: 0.7675 - val_loss: 0.4410 - learning_rate: 0.0100\n",
      "Epoch 48/300\n",
      "\u001b[1m12/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7819 - loss: 0.4258\n",
      "Epoch 48: val_loss did not improve from 0.44104\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.7821 - loss: 0.4256 - val_accuracy: 0.7590 - val_loss: 0.4460 - learning_rate: 0.0100\n",
      "Epoch 49/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7870 - loss: 0.4206\n",
      "Epoch 49: val_loss did not improve from 0.44104\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.7857 - loss: 0.4217 - val_accuracy: 0.7419 - val_loss: 0.4542 - learning_rate: 0.0100\n",
      "Epoch 50/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7704 - loss: 0.4282\n",
      "Epoch 50: val_loss improved from 0.44104 to 0.44092, saving model to folds3.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.7727 - loss: 0.4277 - val_accuracy: 0.7767 - val_loss: 0.4409 - learning_rate: 0.0100\n",
      "Epoch 51/300\n",
      "\u001b[1m13/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7838 - loss: 0.4285 \n",
      "Epoch 51: val_loss did not improve from 0.44092\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.7831 - loss: 0.4282 - val_accuracy: 0.7498 - val_loss: 0.4428 - learning_rate: 0.0100\n",
      "Epoch 52/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7815 - loss: 0.4224 \n",
      "Epoch 52: val_loss did not improve from 0.44092\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7818 - loss: 0.4230 - val_accuracy: 0.7437 - val_loss: 0.4483 - learning_rate: 0.0100\n",
      "Epoch 53/300\n",
      "\u001b[1m 7/15\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7807 - loss: 0.4216 \n",
      "Epoch 53: val_loss did not improve from 0.44092\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7824 - loss: 0.4220 - val_accuracy: 0.7541 - val_loss: 0.4418 - learning_rate: 0.0100\n",
      "Epoch 54/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7783 - loss: 0.4230 \n",
      "Epoch 54: val_loss improved from 0.44092 to 0.43445, saving model to folds3.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7775 - loss: 0.4242 - val_accuracy: 0.7651 - val_loss: 0.4345 - learning_rate: 0.0100\n",
      "Epoch 55/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7880 - loss: 0.4139 \n",
      "Epoch 55: val_loss did not improve from 0.43445\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7876 - loss: 0.4155 - val_accuracy: 0.7657 - val_loss: 0.4374 - learning_rate: 0.0100\n",
      "Epoch 56/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7741 - loss: 0.4281 \n",
      "Epoch 56: val_loss did not improve from 0.43445\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7775 - loss: 0.4257 - val_accuracy: 0.7633 - val_loss: 0.4379 - learning_rate: 0.0100\n",
      "Epoch 57/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7760 - loss: 0.4320  \n",
      "Epoch 57: val_loss did not improve from 0.43445\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7787 - loss: 0.4277 - val_accuracy: 0.7645 - val_loss: 0.4436 - learning_rate: 0.0100\n",
      "Epoch 58/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7914 - loss: 0.4168  \n",
      "Epoch 58: val_loss improved from 0.43445 to 0.43234, saving model to folds3.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7924 - loss: 0.4181 - val_accuracy: 0.7761 - val_loss: 0.4323 - learning_rate: 0.0100\n",
      "Epoch 59/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7899 - loss: 0.4132 \n",
      "Epoch 59: val_loss did not improve from 0.43234\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7883 - loss: 0.4148 - val_accuracy: 0.7584 - val_loss: 0.4441 - learning_rate: 0.0100\n",
      "Epoch 60/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7971 - loss: 0.4162 \n",
      "Epoch 60: val_loss improved from 0.43234 to 0.42978, saving model to folds3.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7951 - loss: 0.4163 - val_accuracy: 0.7834 - val_loss: 0.4298 - learning_rate: 0.0100\n",
      "Epoch 61/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7886 - loss: 0.4170 \n",
      "Epoch 61: val_loss did not improve from 0.42978\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7896 - loss: 0.4159 - val_accuracy: 0.7645 - val_loss: 0.4367 - learning_rate: 0.0100\n",
      "Epoch 62/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7898 - loss: 0.4098  \n",
      "Epoch 62: val_loss did not improve from 0.42978\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7896 - loss: 0.4112 - val_accuracy: 0.7621 - val_loss: 0.4357 - learning_rate: 0.0100\n",
      "Epoch 63/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7873 - loss: 0.4130 \n",
      "Epoch 63: val_loss did not improve from 0.42978\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7885 - loss: 0.4121 - val_accuracy: 0.7590 - val_loss: 0.4366 - learning_rate: 0.0100\n",
      "Epoch 64/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7938 - loss: 0.4128  \n",
      "Epoch 64: val_loss did not improve from 0.42978\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7947 - loss: 0.4121 - val_accuracy: 0.7492 - val_loss: 0.4415 - learning_rate: 0.0100\n",
      "Epoch 65/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7806 - loss: 0.4223 \n",
      "Epoch 65: val_loss did not improve from 0.42978\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7828 - loss: 0.4197 - val_accuracy: 0.7736 - val_loss: 0.4321 - learning_rate: 0.0100\n",
      "Epoch 66/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7939 - loss: 0.4155 \n",
      "Epoch 66: val_loss did not improve from 0.42978\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7927 - loss: 0.4133 - val_accuracy: 0.7584 - val_loss: 0.4433 - learning_rate: 0.0100\n",
      "Epoch 67/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7923 - loss: 0.4089 \n",
      "Epoch 67: val_loss did not improve from 0.42978\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7921 - loss: 0.4096 - val_accuracy: 0.7633 - val_loss: 0.4304 - learning_rate: 0.0100\n",
      "Epoch 68/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7841 - loss: 0.4096 \n",
      "Epoch 68: val_loss did not improve from 0.42978\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7865 - loss: 0.4087 - val_accuracy: 0.7688 - val_loss: 0.4307 - learning_rate: 0.0100\n",
      "Epoch 69/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7856 - loss: 0.4108 \n",
      "Epoch 69: val_loss did not improve from 0.42978\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7823 - loss: 0.4124 - val_accuracy: 0.7328 - val_loss: 0.4452 - learning_rate: 0.0100\n",
      "Epoch 70/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7779 - loss: 0.4107 \n",
      "Epoch 70: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
      "\n",
      "Epoch 70: val_loss did not improve from 0.42978\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7835 - loss: 0.4098 - val_accuracy: 0.7480 - val_loss: 0.4342 - learning_rate: 0.0100\n",
      "Epoch 71/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7832 - loss: 0.4009 \n",
      "Epoch 71: val_loss did not improve from 0.42978\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7865 - loss: 0.4017 - val_accuracy: 0.7761 - val_loss: 0.4325 - learning_rate: 0.0050\n",
      "Epoch 72/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8058 - loss: 0.4041 \n",
      "Epoch 72: val_loss improved from 0.42978 to 0.42221, saving model to folds3.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8033 - loss: 0.4034 - val_accuracy: 0.7761 - val_loss: 0.4222 - learning_rate: 0.0050\n",
      "Epoch 73/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7904 - loss: 0.4033 \n",
      "Epoch 73: val_loss did not improve from 0.42221\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7913 - loss: 0.4030 - val_accuracy: 0.7517 - val_loss: 0.4375 - learning_rate: 0.0050\n",
      "Epoch 74/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7886 - loss: 0.4094 \n",
      "Epoch 74: val_loss did not improve from 0.42221\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7914 - loss: 0.4078 - val_accuracy: 0.7639 - val_loss: 0.4297 - learning_rate: 0.0050\n",
      "Epoch 75/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7908 - loss: 0.4059 \n",
      "Epoch 75: val_loss did not improve from 0.42221\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7920 - loss: 0.4047 - val_accuracy: 0.7669 - val_loss: 0.4264 - learning_rate: 0.0050\n",
      "Epoch 76/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7977 - loss: 0.3956 \n",
      "Epoch 76: val_loss did not improve from 0.42221\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7952 - loss: 0.3974 - val_accuracy: 0.7712 - val_loss: 0.4237 - learning_rate: 0.0050\n",
      "Epoch 77/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7959 - loss: 0.3986 \n",
      "Epoch 77: val_loss did not improve from 0.42221\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7953 - loss: 0.3992 - val_accuracy: 0.7669 - val_loss: 0.4235 - learning_rate: 0.0050\n",
      "Epoch 78/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8036 - loss: 0.3889 \n",
      "Epoch 78: val_loss did not improve from 0.42221\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7996 - loss: 0.3939 - val_accuracy: 0.7572 - val_loss: 0.4296 - learning_rate: 0.0050\n",
      "Epoch 79/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7886 - loss: 0.3997  \n",
      "Epoch 79: val_loss did not improve from 0.42221\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7910 - loss: 0.3994 - val_accuracy: 0.7743 - val_loss: 0.4241 - learning_rate: 0.0050\n",
      "Epoch 80/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7937 - loss: 0.4053 \n",
      "Epoch 80: val_loss did not improve from 0.42221\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7945 - loss: 0.4036 - val_accuracy: 0.7657 - val_loss: 0.4257 - learning_rate: 0.0050\n",
      "Epoch 81/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7889 - loss: 0.4088  \n",
      "Epoch 81: val_loss improved from 0.42221 to 0.42038, saving model to folds3.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7927 - loss: 0.4049 - val_accuracy: 0.7767 - val_loss: 0.4204 - learning_rate: 0.0050\n",
      "Epoch 82/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8080 - loss: 0.3896 \n",
      "Epoch 82: val_loss did not improve from 0.42038\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8056 - loss: 0.3929 - val_accuracy: 0.7761 - val_loss: 0.4208 - learning_rate: 0.0050\n",
      "Epoch 83/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7994 - loss: 0.3994 \n",
      "Epoch 83: val_loss did not improve from 0.42038\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7986 - loss: 0.4003 - val_accuracy: 0.7779 - val_loss: 0.4232 - learning_rate: 0.0050\n",
      "Epoch 84/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7973 - loss: 0.3950 \n",
      "Epoch 84: val_loss did not improve from 0.42038\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7974 - loss: 0.3961 - val_accuracy: 0.7682 - val_loss: 0.4231 - learning_rate: 0.0050\n",
      "Epoch 85/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7907 - loss: 0.3980 \n",
      "Epoch 85: val_loss did not improve from 0.42038\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7929 - loss: 0.3976 - val_accuracy: 0.7767 - val_loss: 0.4226 - learning_rate: 0.0050\n",
      "Epoch 86/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7962 - loss: 0.3998 \n",
      "Epoch 86: val_loss did not improve from 0.42038\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7959 - loss: 0.3991 - val_accuracy: 0.7743 - val_loss: 0.4272 - learning_rate: 0.0050\n",
      "Epoch 87/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8048 - loss: 0.4029 \n",
      "Epoch 87: val_loss did not improve from 0.42038\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8045 - loss: 0.4012 - val_accuracy: 0.7779 - val_loss: 0.4229 - learning_rate: 0.0050\n",
      "Epoch 88/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7965 - loss: 0.4008 \n",
      "Epoch 88: val_loss did not improve from 0.42038\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7983 - loss: 0.3990 - val_accuracy: 0.7706 - val_loss: 0.4213 - learning_rate: 0.0050\n",
      "Epoch 89/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7891 - loss: 0.3989 \n",
      "Epoch 89: val_loss did not improve from 0.42038\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7914 - loss: 0.3982 - val_accuracy: 0.7602 - val_loss: 0.4217 - learning_rate: 0.0050\n",
      "Epoch 90/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7925 - loss: 0.3890  \n",
      "Epoch 90: val_loss improved from 0.42038 to 0.41553, saving model to folds3.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7930 - loss: 0.3921 - val_accuracy: 0.7852 - val_loss: 0.4155 - learning_rate: 0.0050\n",
      "Epoch 91/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7954 - loss: 0.4004 \n",
      "Epoch 91: val_loss did not improve from 0.41553\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7950 - loss: 0.3994 - val_accuracy: 0.7834 - val_loss: 0.4188 - learning_rate: 0.0050\n",
      "Epoch 92/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8001 - loss: 0.3995 \n",
      "Epoch 92: val_loss improved from 0.41553 to 0.41327, saving model to folds3.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7993 - loss: 0.3983 - val_accuracy: 0.7852 - val_loss: 0.4133 - learning_rate: 0.0050\n",
      "Epoch 93/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7991 - loss: 0.3892 \n",
      "Epoch 93: val_loss did not improve from 0.41327\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7981 - loss: 0.3916 - val_accuracy: 0.7804 - val_loss: 0.4177 - learning_rate: 0.0050\n",
      "Epoch 94/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8011 - loss: 0.3976 \n",
      "Epoch 94: val_loss did not improve from 0.41327\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8010 - loss: 0.3966 - val_accuracy: 0.7718 - val_loss: 0.4177 - learning_rate: 0.0050\n",
      "Epoch 95/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8005 - loss: 0.4008 \n",
      "Epoch 95: val_loss did not improve from 0.41327\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8013 - loss: 0.3978 - val_accuracy: 0.7755 - val_loss: 0.4175 - learning_rate: 0.0050\n",
      "Epoch 96/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8011 - loss: 0.3889  \n",
      "Epoch 96: val_loss did not improve from 0.41327\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8005 - loss: 0.3904 - val_accuracy: 0.7669 - val_loss: 0.4223 - learning_rate: 0.0050\n",
      "Epoch 97/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8022 - loss: 0.3937 \n",
      "Epoch 97: val_loss did not improve from 0.41327\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8009 - loss: 0.3964 - val_accuracy: 0.7816 - val_loss: 0.4155 - learning_rate: 0.0050\n",
      "Epoch 98/300\n",
      "\u001b[1m12/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8078 - loss: 0.3901\n",
      "Epoch 98: val_loss did not improve from 0.41327\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.8067 - loss: 0.3908 - val_accuracy: 0.7743 - val_loss: 0.4176 - learning_rate: 0.0050\n",
      "Epoch 99/300\n",
      "\u001b[1m12/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7902 - loss: 0.3961\n",
      "Epoch 99: val_loss did not improve from 0.41327\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.7900 - loss: 0.3962 - val_accuracy: 0.7779 - val_loss: 0.4136 - learning_rate: 0.0050\n",
      "Epoch 100/300\n",
      "\u001b[1m12/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7984 - loss: 0.3935\n",
      "Epoch 100: val_loss did not improve from 0.41327\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.7985 - loss: 0.3935 - val_accuracy: 0.7694 - val_loss: 0.4188 - learning_rate: 0.0050\n",
      "Epoch 101/300\n",
      "\u001b[1m12/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7931 - loss: 0.3959\n",
      "Epoch 101: val_loss did not improve from 0.41327\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.7938 - loss: 0.3952 - val_accuracy: 0.7822 - val_loss: 0.4141 - learning_rate: 0.0050\n",
      "Epoch 102/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8059 - loss: 0.3929\n",
      "Epoch 102: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "\n",
      "Epoch 102: val_loss did not improve from 0.41327\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.8030 - loss: 0.3933 - val_accuracy: 0.7663 - val_loss: 0.4182 - learning_rate: 0.0050\n",
      "Epoch 103/300\n",
      "\u001b[1m12/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7958 - loss: 0.3966\n",
      "Epoch 103: val_loss did not improve from 0.41327\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.7971 - loss: 0.3951 - val_accuracy: 0.7840 - val_loss: 0.4133 - learning_rate: 0.0025\n",
      "Epoch 104/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8085 - loss: 0.3828\n",
      "Epoch 104: val_loss did not improve from 0.41327\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8070 - loss: 0.3848 - val_accuracy: 0.7767 - val_loss: 0.4146 - learning_rate: 0.0025\n",
      "Epoch 105/300\n",
      "\u001b[1m13/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7985 - loss: 0.3870 \n",
      "Epoch 105: val_loss did not improve from 0.41327\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.7986 - loss: 0.3878 - val_accuracy: 0.7688 - val_loss: 0.4159 - learning_rate: 0.0025\n",
      "Epoch 106/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7931 - loss: 0.3939\n",
      "Epoch 106: val_loss did not improve from 0.41327\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.7948 - loss: 0.3928 - val_accuracy: 0.7712 - val_loss: 0.4137 - learning_rate: 0.0025\n",
      "Epoch 107/300\n",
      "\u001b[1m12/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7898 - loss: 0.3949\n",
      "Epoch 107: val_loss did not improve from 0.41327\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.7922 - loss: 0.3937 - val_accuracy: 0.7749 - val_loss: 0.4137 - learning_rate: 0.0025\n",
      "Epoch 108/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8119 - loss: 0.3817\n",
      "Epoch 108: val_loss improved from 0.41327 to 0.41127, saving model to folds3.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.8115 - loss: 0.3822 - val_accuracy: 0.7804 - val_loss: 0.4113 - learning_rate: 0.0025\n",
      "Epoch 109/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8044 - loss: 0.3843 \n",
      "Epoch 109: val_loss did not improve from 0.41127\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8025 - loss: 0.3856 - val_accuracy: 0.7743 - val_loss: 0.4126 - learning_rate: 0.0025\n",
      "Epoch 110/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8012 - loss: 0.3914 \n",
      "Epoch 110: val_loss improved from 0.41127 to 0.40993, saving model to folds3.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8013 - loss: 0.3901 - val_accuracy: 0.7877 - val_loss: 0.4099 - learning_rate: 0.0025\n",
      "Epoch 111/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7988 - loss: 0.3945 \n",
      "Epoch 111: val_loss did not improve from 0.40993\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7985 - loss: 0.3923 - val_accuracy: 0.7797 - val_loss: 0.4127 - learning_rate: 0.0025\n",
      "Epoch 112/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8028 - loss: 0.3861  \n",
      "Epoch 112: val_loss did not improve from 0.40993\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8018 - loss: 0.3876 - val_accuracy: 0.7858 - val_loss: 0.4102 - learning_rate: 0.0025\n",
      "Epoch 113/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7967 - loss: 0.3934 \n",
      "Epoch 113: val_loss did not improve from 0.40993\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7974 - loss: 0.3921 - val_accuracy: 0.7834 - val_loss: 0.4114 - learning_rate: 0.0025\n",
      "Epoch 114/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8069 - loss: 0.3831 \n",
      "Epoch 114: val_loss improved from 0.40993 to 0.40868, saving model to folds3.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8044 - loss: 0.3854 - val_accuracy: 0.7865 - val_loss: 0.4087 - learning_rate: 0.0025\n",
      "Epoch 115/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8012 - loss: 0.3939  \n",
      "Epoch 115: val_loss did not improve from 0.40868\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8005 - loss: 0.3931 - val_accuracy: 0.7828 - val_loss: 0.4124 - learning_rate: 0.0025\n",
      "Epoch 116/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8045 - loss: 0.3837 \n",
      "Epoch 116: val_loss improved from 0.40868 to 0.40775, saving model to folds3.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8020 - loss: 0.3858 - val_accuracy: 0.7871 - val_loss: 0.4077 - learning_rate: 0.0025\n",
      "Epoch 117/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8059 - loss: 0.3817  \n",
      "Epoch 117: val_loss did not improve from 0.40775\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8026 - loss: 0.3848 - val_accuracy: 0.7779 - val_loss: 0.4102 - learning_rate: 0.0025\n",
      "Epoch 118/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8024 - loss: 0.3822 \n",
      "Epoch 118: val_loss did not improve from 0.40775\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8008 - loss: 0.3850 - val_accuracy: 0.7773 - val_loss: 0.4129 - learning_rate: 0.0025\n",
      "Epoch 119/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7958 - loss: 0.3875 \n",
      "Epoch 119: val_loss did not improve from 0.40775\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7978 - loss: 0.3871 - val_accuracy: 0.7785 - val_loss: 0.4103 - learning_rate: 0.0025\n",
      "Epoch 120/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7949 - loss: 0.3918 \n",
      "Epoch 120: val_loss did not improve from 0.40775\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7965 - loss: 0.3907 - val_accuracy: 0.7956 - val_loss: 0.4090 - learning_rate: 0.0025\n",
      "Epoch 121/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8050 - loss: 0.3905 \n",
      "Epoch 121: val_loss did not improve from 0.40775\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8041 - loss: 0.3893 - val_accuracy: 0.7724 - val_loss: 0.4149 - learning_rate: 0.0025\n",
      "Epoch 122/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7973 - loss: 0.3876 \n",
      "Epoch 122: val_loss did not improve from 0.40775\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7977 - loss: 0.3878 - val_accuracy: 0.7785 - val_loss: 0.4103 - learning_rate: 0.0025\n",
      "Epoch 123/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7922 - loss: 0.3849 \n",
      "Epoch 123: val_loss did not improve from 0.40775\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7959 - loss: 0.3857 - val_accuracy: 0.7785 - val_loss: 0.4099 - learning_rate: 0.0025\n",
      "Epoch 124/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7996 - loss: 0.3866 \n",
      "Epoch 124: val_loss did not improve from 0.40775\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8003 - loss: 0.3866 - val_accuracy: 0.7822 - val_loss: 0.4086 - learning_rate: 0.0025\n",
      "Epoch 125/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7898 - loss: 0.3949 \n",
      "Epoch 125: val_loss did not improve from 0.40775\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7926 - loss: 0.3923 - val_accuracy: 0.7688 - val_loss: 0.4138 - learning_rate: 0.0025\n",
      "Epoch 126/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8028 - loss: 0.3745 \n",
      "Epoch 126: val_loss improved from 0.40775 to 0.40639, saving model to folds3.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8017 - loss: 0.3783 - val_accuracy: 0.7816 - val_loss: 0.4064 - learning_rate: 0.0025\n",
      "Epoch 127/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8027 - loss: 0.3820 \n",
      "Epoch 127: val_loss did not improve from 0.40639\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8013 - loss: 0.3836 - val_accuracy: 0.7804 - val_loss: 0.4079 - learning_rate: 0.0025\n",
      "Epoch 128/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8016 - loss: 0.3827  \n",
      "Epoch 128: val_loss did not improve from 0.40639\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8002 - loss: 0.3840 - val_accuracy: 0.7871 - val_loss: 0.4098 - learning_rate: 0.0025\n",
      "Epoch 129/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8029 - loss: 0.3858\n",
      "Epoch 129: val_loss did not improve from 0.40639\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8028 - loss: 0.3858 - val_accuracy: 0.7895 - val_loss: 0.4083 - learning_rate: 0.0025\n",
      "Epoch 130/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8047 - loss: 0.3824 \n",
      "Epoch 130: val_loss did not improve from 0.40639\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8034 - loss: 0.3836 - val_accuracy: 0.7889 - val_loss: 0.4087 - learning_rate: 0.0025\n",
      "Epoch 131/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8050 - loss: 0.3909 \n",
      "Epoch 131: val_loss did not improve from 0.40639\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8038 - loss: 0.3891 - val_accuracy: 0.7828 - val_loss: 0.4082 - learning_rate: 0.0025\n",
      "Epoch 132/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8006 - loss: 0.3869 \n",
      "Epoch 132: val_loss did not improve from 0.40639\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8007 - loss: 0.3867 - val_accuracy: 0.7804 - val_loss: 0.4090 - learning_rate: 0.0025\n",
      "Epoch 133/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8062 - loss: 0.3816 \n",
      "Epoch 133: val_loss did not improve from 0.40639\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8045 - loss: 0.3831 - val_accuracy: 0.7834 - val_loss: 0.4100 - learning_rate: 0.0025\n",
      "Epoch 134/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8098 - loss: 0.3774  \n",
      "Epoch 134: val_loss did not improve from 0.40639\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8065 - loss: 0.3800 - val_accuracy: 0.7858 - val_loss: 0.4069 - learning_rate: 0.0025\n",
      "Epoch 135/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7976 - loss: 0.3883 \n",
      "Epoch 135: val_loss did not improve from 0.40639\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7989 - loss: 0.3870 - val_accuracy: 0.7907 - val_loss: 0.4069 - learning_rate: 0.0025\n",
      "Epoch 136/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8149 - loss: 0.3782 \n",
      "Epoch 136: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
      "\n",
      "Epoch 136: val_loss did not improve from 0.40639\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8105 - loss: 0.3805 - val_accuracy: 0.7846 - val_loss: 0.4070 - learning_rate: 0.0025\n",
      "Epoch 137/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7989 - loss: 0.3852 \n",
      "Epoch 137: val_loss did not improve from 0.40639\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7996 - loss: 0.3843 - val_accuracy: 0.7791 - val_loss: 0.4092 - learning_rate: 0.0012\n",
      "Epoch 138/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8016 - loss: 0.3840 \n",
      "Epoch 138: val_loss did not improve from 0.40639\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8014 - loss: 0.3840 - val_accuracy: 0.7688 - val_loss: 0.4145 - learning_rate: 0.0012\n",
      "Epoch 139/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7933 - loss: 0.3917 \n",
      "Epoch 139: val_loss did not improve from 0.40639\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7976 - loss: 0.3895 - val_accuracy: 0.7749 - val_loss: 0.4087 - learning_rate: 0.0012\n",
      "Epoch 140/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7938 - loss: 0.3839 \n",
      "Epoch 140: val_loss did not improve from 0.40639\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7958 - loss: 0.3839 - val_accuracy: 0.7767 - val_loss: 0.4088 - learning_rate: 0.0012\n",
      "Epoch 141/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7952 - loss: 0.3849 \n",
      "Epoch 141: val_loss improved from 0.40639 to 0.40620, saving model to folds3.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.7967 - loss: 0.3844 - val_accuracy: 0.7871 - val_loss: 0.4062 - learning_rate: 0.0012\n",
      "Epoch 142/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8060 - loss: 0.3769 \n",
      "Epoch 142: val_loss did not improve from 0.40620\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8034 - loss: 0.3795 - val_accuracy: 0.7816 - val_loss: 0.4070 - learning_rate: 0.0012\n",
      "Epoch 143/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8100 - loss: 0.3790 \n",
      "Epoch 143: val_loss did not improve from 0.40620\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8070 - loss: 0.3810 - val_accuracy: 0.7846 - val_loss: 0.4073 - learning_rate: 0.0012\n",
      "Epoch 144/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8082 - loss: 0.3815 \n",
      "Epoch 144: val_loss did not improve from 0.40620\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8072 - loss: 0.3819 - val_accuracy: 0.7797 - val_loss: 0.4078 - learning_rate: 0.0012\n",
      "Epoch 145/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8051 - loss: 0.3798 \n",
      "Epoch 145: val_loss improved from 0.40620 to 0.40599, saving model to folds3.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8015 - loss: 0.3816 - val_accuracy: 0.7846 - val_loss: 0.4060 - learning_rate: 0.0012\n",
      "Epoch 146/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8037 - loss: 0.3806 \n",
      "Epoch 146: val_loss did not improve from 0.40599\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8028 - loss: 0.3822 - val_accuracy: 0.7761 - val_loss: 0.4076 - learning_rate: 0.0012\n",
      "Epoch 147/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7961 - loss: 0.3877 \n",
      "Epoch 147: val_loss did not improve from 0.40599\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7970 - loss: 0.3860 - val_accuracy: 0.7919 - val_loss: 0.4061 - learning_rate: 0.0012\n",
      "Epoch 148/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8026 - loss: 0.3914 \n",
      "Epoch 148: val_loss improved from 0.40599 to 0.40555, saving model to folds3.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8023 - loss: 0.3884 - val_accuracy: 0.7938 - val_loss: 0.4056 - learning_rate: 0.0012\n",
      "Epoch 149/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7957 - loss: 0.3912 \n",
      "Epoch 149: val_loss did not improve from 0.40555\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7969 - loss: 0.3891 - val_accuracy: 0.7913 - val_loss: 0.4063 - learning_rate: 0.0012\n",
      "Epoch 150/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8065 - loss: 0.3762 \n",
      "Epoch 150: val_loss improved from 0.40555 to 0.40470, saving model to folds3.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8035 - loss: 0.3786 - val_accuracy: 0.7913 - val_loss: 0.4047 - learning_rate: 0.0012\n",
      "Epoch 151/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8067 - loss: 0.3856 \n",
      "Epoch 151: val_loss did not improve from 0.40470\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8034 - loss: 0.3851 - val_accuracy: 0.7858 - val_loss: 0.4056 - learning_rate: 0.0012\n",
      "Epoch 152/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7958 - loss: 0.3877  \n",
      "Epoch 152: val_loss did not improve from 0.40470\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7979 - loss: 0.3858 - val_accuracy: 0.7767 - val_loss: 0.4083 - learning_rate: 0.0012\n",
      "Epoch 153/300\n",
      "\u001b[1m12/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7985 - loss: 0.3810\n",
      "Epoch 153: val_loss did not improve from 0.40470\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7989 - loss: 0.3812 - val_accuracy: 0.7755 - val_loss: 0.4091 - learning_rate: 0.0012\n",
      "Epoch 154/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8004 - loss: 0.3823\n",
      "Epoch 154: val_loss did not improve from 0.40470\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8004 - loss: 0.3824 - val_accuracy: 0.7767 - val_loss: 0.4099 - learning_rate: 0.0012\n",
      "Epoch 155/300\n",
      "\u001b[1m12/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8001 - loss: 0.3805\n",
      "Epoch 155: val_loss did not improve from 0.40470\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.7999 - loss: 0.3812 - val_accuracy: 0.7871 - val_loss: 0.4059 - learning_rate: 0.0012\n",
      "Epoch 156/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7998 - loss: 0.3855\n",
      "Epoch 156: val_loss did not improve from 0.40470\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7996 - loss: 0.3852 - val_accuracy: 0.7932 - val_loss: 0.4047 - learning_rate: 0.0012\n",
      "Epoch 157/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8058 - loss: 0.3847\n",
      "Epoch 157: val_loss did not improve from 0.40470\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.8056 - loss: 0.3845 - val_accuracy: 0.7724 - val_loss: 0.4086 - learning_rate: 0.0012\n",
      "Epoch 158/300\n",
      "\u001b[1m12/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7997 - loss: 0.3754\n",
      "Epoch 158: val_loss did not improve from 0.40470\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.7994 - loss: 0.3772 - val_accuracy: 0.7779 - val_loss: 0.4061 - learning_rate: 0.0012\n",
      "Epoch 159/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8004 - loss: 0.3773\n",
      "Epoch 159: val_loss did not improve from 0.40470\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.7995 - loss: 0.3793 - val_accuracy: 0.7865 - val_loss: 0.4048 - learning_rate: 0.0012\n",
      "Epoch 160/300\n",
      "\u001b[1m12/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8058 - loss: 0.3797\n",
      "Epoch 160: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "\n",
      "Epoch 160: val_loss did not improve from 0.40470\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.8039 - loss: 0.3806 - val_accuracy: 0.7828 - val_loss: 0.4073 - learning_rate: 0.0012\n",
      "Epoch 161/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7951 - loss: 0.3914\n",
      "Epoch 161: val_loss did not improve from 0.40470\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.7960 - loss: 0.3888 - val_accuracy: 0.7865 - val_loss: 0.4054 - learning_rate: 6.2500e-04\n",
      "Epoch 162/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8057 - loss: 0.3829\n",
      "Epoch 162: val_loss did not improve from 0.40470\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.8056 - loss: 0.3829 - val_accuracy: 0.7773 - val_loss: 0.4064 - learning_rate: 6.2500e-04\n",
      "Epoch 163/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7918 - loss: 0.3843\n",
      "Epoch 163: val_loss did not improve from 0.40470\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.7947 - loss: 0.3833 - val_accuracy: 0.7804 - val_loss: 0.4059 - learning_rate: 6.2500e-04\n",
      "Epoch 164/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8041 - loss: 0.3786\n",
      "Epoch 164: val_loss did not improve from 0.40470\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.8026 - loss: 0.3797 - val_accuracy: 0.7749 - val_loss: 0.4066 - learning_rate: 6.2500e-04\n",
      "Epoch 165/300\n",
      "\u001b[1m12/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7946 - loss: 0.3849\n",
      "Epoch 165: val_loss did not improve from 0.40470\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.7960 - loss: 0.3842 - val_accuracy: 0.7773 - val_loss: 0.4067 - learning_rate: 6.2500e-04\n",
      "Epoch 166/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7929 - loss: 0.3808 \n",
      "Epoch 166: val_loss did not improve from 0.40470\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7955 - loss: 0.3811 - val_accuracy: 0.7828 - val_loss: 0.4051 - learning_rate: 6.2500e-04\n",
      "Epoch 167/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7926 - loss: 0.3859 \n",
      "Epoch 167: val_loss did not improve from 0.40470\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7949 - loss: 0.3830 - val_accuracy: 0.7816 - val_loss: 0.4059 - learning_rate: 6.2500e-04\n",
      "Epoch 168/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8035 - loss: 0.3797 \n",
      "Epoch 168: val_loss improved from 0.40470 to 0.40443, saving model to folds3.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8010 - loss: 0.3804 - val_accuracy: 0.7871 - val_loss: 0.4044 - learning_rate: 6.2500e-04\n",
      "Epoch 169/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8053 - loss: 0.3795 \n",
      "Epoch 169: val_loss did not improve from 0.40443\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8036 - loss: 0.3798 - val_accuracy: 0.7822 - val_loss: 0.4054 - learning_rate: 6.2500e-04\n",
      "Epoch 170/300\n",
      "\u001b[1m13/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8022 - loss: 0.3802\n",
      "Epoch 170: val_loss did not improve from 0.40443\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8024 - loss: 0.3805 - val_accuracy: 0.7767 - val_loss: 0.4069 - learning_rate: 6.2500e-04\n",
      "Epoch 171/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7919 - loss: 0.3900 \n",
      "Epoch 171: val_loss did not improve from 0.40443\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7952 - loss: 0.3860 - val_accuracy: 0.7828 - val_loss: 0.4048 - learning_rate: 6.2500e-04\n",
      "Epoch 172/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7958 - loss: 0.3897 \n",
      "Epoch 172: val_loss did not improve from 0.40443\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7976 - loss: 0.3871 - val_accuracy: 0.7804 - val_loss: 0.4055 - learning_rate: 6.2500e-04\n",
      "Epoch 173/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7958 - loss: 0.3867 \n",
      "Epoch 173: val_loss did not improve from 0.40443\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7972 - loss: 0.3846 - val_accuracy: 0.7810 - val_loss: 0.4056 - learning_rate: 6.2500e-04\n",
      "Epoch 174/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7988 - loss: 0.3792  \n",
      "Epoch 174: val_loss did not improve from 0.40443\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7981 - loss: 0.3803 - val_accuracy: 0.7871 - val_loss: 0.4047 - learning_rate: 6.2500e-04\n",
      "Epoch 175/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7989 - loss: 0.3879  \n",
      "Epoch 175: val_loss did not improve from 0.40443\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7993 - loss: 0.3857 - val_accuracy: 0.7761 - val_loss: 0.4064 - learning_rate: 6.2500e-04\n",
      "Epoch 176/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7966 - loss: 0.3827 \n",
      "Epoch 176: val_loss improved from 0.40443 to 0.40438, saving model to folds3.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7975 - loss: 0.3823 - val_accuracy: 0.7883 - val_loss: 0.4044 - learning_rate: 6.2500e-04\n",
      "Epoch 177/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8055 - loss: 0.3802 \n",
      "Epoch 177: val_loss did not improve from 0.40438\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8034 - loss: 0.3811 - val_accuracy: 0.7761 - val_loss: 0.4069 - learning_rate: 6.2500e-04\n",
      "Epoch 178/300\n",
      "\u001b[1m 7/15\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7995 - loss: 0.3823 \n",
      "Epoch 178: ReduceLROnPlateau reducing learning rate to 0.0003124999930150807.\n",
      "\n",
      "Epoch 178: val_loss did not improve from 0.40438\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8013 - loss: 0.3819 - val_accuracy: 0.7816 - val_loss: 0.4055 - learning_rate: 6.2500e-04\n",
      "Epoch 179/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8019 - loss: 0.3807\n",
      "Epoch 179: val_loss did not improve from 0.40438\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8017 - loss: 0.3807 - val_accuracy: 0.7804 - val_loss: 0.4054 - learning_rate: 3.1250e-04\n",
      "Epoch 180/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7957 - loss: 0.3818  \n",
      "Epoch 180: val_loss did not improve from 0.40438\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7976 - loss: 0.3813 - val_accuracy: 0.7828 - val_loss: 0.4049 - learning_rate: 3.1250e-04\n",
      "Epoch 181/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8056 - loss: 0.3767 \n",
      "Epoch 181: val_loss did not improve from 0.40438\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8022 - loss: 0.3788 - val_accuracy: 0.7816 - val_loss: 0.4053 - learning_rate: 3.1250e-04\n",
      "Epoch 182/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7984 - loss: 0.3796 \n",
      "Epoch 182: val_loss did not improve from 0.40438\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7993 - loss: 0.3801 - val_accuracy: 0.7840 - val_loss: 0.4048 - learning_rate: 3.1250e-04\n",
      "Epoch 183/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7980 - loss: 0.3847 \n",
      "Epoch 183: val_loss did not improve from 0.40438\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7987 - loss: 0.3836 - val_accuracy: 0.7816 - val_loss: 0.4047 - learning_rate: 3.1250e-04\n",
      "Epoch 184/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8000 - loss: 0.3784 \n",
      "Epoch 184: val_loss did not improve from 0.40438\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7992 - loss: 0.3802 - val_accuracy: 0.7779 - val_loss: 0.4056 - learning_rate: 3.1250e-04\n",
      "Epoch 185/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7927 - loss: 0.3863 \n",
      "Epoch 185: val_loss did not improve from 0.40438\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7947 - loss: 0.3838 - val_accuracy: 0.7871 - val_loss: 0.4047 - learning_rate: 3.1250e-04\n",
      "Epoch 186/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7980 - loss: 0.3868 \n",
      "Epoch 186: val_loss did not improve from 0.40438\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7989 - loss: 0.3852 - val_accuracy: 0.7804 - val_loss: 0.4051 - learning_rate: 3.1250e-04\n",
      "Epoch 187/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8052 - loss: 0.3802 \n",
      "Epoch 187: val_loss did not improve from 0.40438\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8041 - loss: 0.3799 - val_accuracy: 0.7791 - val_loss: 0.4054 - learning_rate: 3.1250e-04\n",
      "Epoch 188/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8026 - loss: 0.3837 \n",
      "Epoch 188: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "\n",
      "Epoch 188: val_loss did not improve from 0.40438\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8010 - loss: 0.3827 - val_accuracy: 0.7791 - val_loss: 0.4056 - learning_rate: 3.1250e-04\n",
      "Epoch 189/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8032 - loss: 0.3772\n",
      "Epoch 189: val_loss did not improve from 0.40438\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8028 - loss: 0.3776 - val_accuracy: 0.7828 - val_loss: 0.4047 - learning_rate: 1.5625e-04\n",
      "Epoch 190/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7985 - loss: 0.3826 \n",
      "Epoch 190: val_loss did not improve from 0.40438\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7995 - loss: 0.3818 - val_accuracy: 0.7828 - val_loss: 0.4045 - learning_rate: 1.5625e-04\n",
      "Epoch 191/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7965 - loss: 0.3822  \n",
      "Epoch 191: val_loss did not improve from 0.40438\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7984 - loss: 0.3815 - val_accuracy: 0.7834 - val_loss: 0.4049 - learning_rate: 1.5625e-04\n",
      "Epoch 192/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7973 - loss: 0.3816  \n",
      "Epoch 192: val_loss did not improve from 0.40438\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7974 - loss: 0.3813 - val_accuracy: 0.7791 - val_loss: 0.4054 - learning_rate: 1.5625e-04\n",
      "Epoch 193/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7986 - loss: 0.3810\n",
      "Epoch 193: val_loss did not improve from 0.40438\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7987 - loss: 0.3810 - val_accuracy: 0.7840 - val_loss: 0.4046 - learning_rate: 1.5625e-04\n",
      "Epoch 194/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7978 - loss: 0.3779  \n",
      "Epoch 194: val_loss did not improve from 0.40438\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7984 - loss: 0.3790 - val_accuracy: 0.7797 - val_loss: 0.4053 - learning_rate: 1.5625e-04\n",
      "Epoch 195/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7902 - loss: 0.3851 \n",
      "Epoch 195: val_loss did not improve from 0.40438\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7936 - loss: 0.3829 - val_accuracy: 0.7822 - val_loss: 0.4052 - learning_rate: 1.5625e-04\n",
      "Epoch 196/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8002 - loss: 0.3792 \n",
      "Epoch 196: val_loss did not improve from 0.40438\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8006 - loss: 0.3799 - val_accuracy: 0.7840 - val_loss: 0.4048 - learning_rate: 1.5625e-04\n",
      "Epoch 197/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7985 - loss: 0.3801 \n",
      "Epoch 197: val_loss did not improve from 0.40438\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7992 - loss: 0.3802 - val_accuracy: 0.7804 - val_loss: 0.4051 - learning_rate: 1.5625e-04\n",
      "Epoch 198/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7946 - loss: 0.3785  \n",
      "Epoch 198: ReduceLROnPlateau reducing learning rate to 7.812499825377017e-05.\n",
      "\n",
      "Epoch 198: val_loss did not improve from 0.40438\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7957 - loss: 0.3789 - val_accuracy: 0.7804 - val_loss: 0.4052 - learning_rate: 1.5625e-04\n",
      "Epoch 199/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8010 - loss: 0.3807 \n",
      "Epoch 199: val_loss did not improve from 0.40438\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8002 - loss: 0.3806 - val_accuracy: 0.7822 - val_loss: 0.4049 - learning_rate: 7.8125e-05\n",
      "Epoch 200/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7986 - loss: 0.3842 \n",
      "Epoch 200: val_loss did not improve from 0.40438\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7995 - loss: 0.3821 - val_accuracy: 0.7822 - val_loss: 0.4050 - learning_rate: 7.8125e-05\n",
      "Epoch 201/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7972 - loss: 0.3830 \n",
      "Epoch 201: val_loss did not improve from 0.40438\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7984 - loss: 0.3819 - val_accuracy: 0.7822 - val_loss: 0.4050 - learning_rate: 7.8125e-05\n",
      "Epoch 202/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8028 - loss: 0.3807 \n",
      "Epoch 202: val_loss did not improve from 0.40438\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8019 - loss: 0.3807 - val_accuracy: 0.7834 - val_loss: 0.4048 - learning_rate: 7.8125e-05\n",
      "Epoch 203/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7966 - loss: 0.3815\n",
      "Epoch 203: val_loss did not improve from 0.40438\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7968 - loss: 0.3815 - val_accuracy: 0.7797 - val_loss: 0.4052 - learning_rate: 7.8125e-05\n",
      "Epoch 204/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7946 - loss: 0.3872  \n",
      "Epoch 204: val_loss did not improve from 0.40438\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7962 - loss: 0.3846 - val_accuracy: 0.7816 - val_loss: 0.4050 - learning_rate: 7.8125e-05\n",
      "Epoch 205/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8040 - loss: 0.3809 \n",
      "Epoch 205: val_loss did not improve from 0.40438\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8029 - loss: 0.3800 - val_accuracy: 0.7834 - val_loss: 0.4046 - learning_rate: 7.8125e-05\n",
      "Epoch 206/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8035 - loss: 0.3773 \n",
      "Epoch 206: val_loss did not improve from 0.40438\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8022 - loss: 0.3786 - val_accuracy: 0.7822 - val_loss: 0.4049 - learning_rate: 7.8125e-05\n",
      "Epoch 207/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8014 - loss: 0.3817\n",
      "Epoch 207: val_loss did not improve from 0.40438\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8012 - loss: 0.3816 - val_accuracy: 0.7822 - val_loss: 0.4049 - learning_rate: 7.8125e-05\n",
      "Epoch 208/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7975 - loss: 0.3819\n",
      "Epoch 208: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "\n",
      "Epoch 208: val_loss did not improve from 0.40438\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.7977 - loss: 0.3818 - val_accuracy: 0.7828 - val_loss: 0.4049 - learning_rate: 7.8125e-05\n",
      "Epoch 209/300\n",
      "\u001b[1m13/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8025 - loss: 0.3756 \n",
      "Epoch 209: val_loss did not improve from 0.40438\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8018 - loss: 0.3765 - val_accuracy: 0.7828 - val_loss: 0.4049 - learning_rate: 3.9062e-05\n",
      "Epoch 210/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7999 - loss: 0.3762\n",
      "Epoch 210: val_loss did not improve from 0.40438\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.8002 - loss: 0.3772 - val_accuracy: 0.7828 - val_loss: 0.4049 - learning_rate: 3.9062e-05\n",
      "Epoch 211/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8089 - loss: 0.3747\n",
      "Epoch 211: val_loss did not improve from 0.40438\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.8084 - loss: 0.3751 - val_accuracy: 0.7828 - val_loss: 0.4050 - learning_rate: 3.9062e-05\n",
      "Epoch 212/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7985 - loss: 0.3845\n",
      "Epoch 212: val_loss did not improve from 0.40438\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.7988 - loss: 0.3835 - val_accuracy: 0.7828 - val_loss: 0.4050 - learning_rate: 3.9062e-05\n",
      "Epoch 213/300\n",
      "\u001b[1m12/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8043 - loss: 0.3753\n",
      "Epoch 213: val_loss did not improve from 0.40438\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.8033 - loss: 0.3766 - val_accuracy: 0.7828 - val_loss: 0.4049 - learning_rate: 3.9062e-05\n",
      "Epoch 214/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7986 - loss: 0.3824\n",
      "Epoch 214: val_loss did not improve from 0.40438\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.7987 - loss: 0.3823 - val_accuracy: 0.7834 - val_loss: 0.4048 - learning_rate: 3.9062e-05\n",
      "Epoch 215/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8039 - loss: 0.3738\n",
      "Epoch 215: val_loss did not improve from 0.40438\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.8037 - loss: 0.3742 - val_accuracy: 0.7846 - val_loss: 0.4048 - learning_rate: 3.9062e-05\n",
      "Epoch 216/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7990 - loss: 0.3837  \n",
      "Epoch 216: val_loss did not improve from 0.40438\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7991 - loss: 0.3826 - val_accuracy: 0.7804 - val_loss: 0.4050 - learning_rate: 3.9062e-05\n",
      "Epoch 217/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8019 - loss: 0.3787 \n",
      "Epoch 217: val_loss did not improve from 0.40438\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8014 - loss: 0.3791 - val_accuracy: 0.7834 - val_loss: 0.4048 - learning_rate: 3.9062e-05\n",
      "Epoch 218/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8018 - loss: 0.3782 \n",
      "Epoch 218: ReduceLROnPlateau reducing learning rate to 1.9531249563442543e-05.\n",
      "\n",
      "Epoch 218: val_loss did not improve from 0.40438\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8016 - loss: 0.3788 - val_accuracy: 0.7828 - val_loss: 0.4048 - learning_rate: 3.9062e-05\n",
      "Epoch 219/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8019 - loss: 0.3788 \n",
      "Epoch 219: val_loss did not improve from 0.40438\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8017 - loss: 0.3790 - val_accuracy: 0.7840 - val_loss: 0.4048 - learning_rate: 1.9531e-05\n",
      "Epoch 220/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8040 - loss: 0.3779\n",
      "Epoch 220: val_loss did not improve from 0.40438\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8038 - loss: 0.3781 - val_accuracy: 0.7822 - val_loss: 0.4049 - learning_rate: 1.9531e-05\n",
      "Epoch 221/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7904 - loss: 0.3898 \n",
      "Epoch 221: val_loss did not improve from 0.40438\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7944 - loss: 0.3859 - val_accuracy: 0.7834 - val_loss: 0.4048 - learning_rate: 1.9531e-05\n",
      "Epoch 222/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8046 - loss: 0.3762 \n",
      "Epoch 222: val_loss did not improve from 0.40438\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8025 - loss: 0.3783 - val_accuracy: 0.7828 - val_loss: 0.4049 - learning_rate: 1.9531e-05\n",
      "Epoch 223/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8051 - loss: 0.3727  \n",
      "Epoch 223: val_loss did not improve from 0.40438\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8041 - loss: 0.3750 - val_accuracy: 0.7816 - val_loss: 0.4049 - learning_rate: 1.9531e-05\n",
      "Epoch 224/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7953 - loss: 0.3815 \n",
      "Epoch 224: val_loss did not improve from 0.40438\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7976 - loss: 0.3807 - val_accuracy: 0.7828 - val_loss: 0.4049 - learning_rate: 1.9531e-05\n",
      "Epoch 225/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7950 - loss: 0.3913 \n",
      "Epoch 225: val_loss did not improve from 0.40438\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7973 - loss: 0.3868 - val_accuracy: 0.7834 - val_loss: 0.4048 - learning_rate: 1.9531e-05\n",
      "Epoch 226/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7986 - loss: 0.3790 \n",
      "Epoch 226: val_loss did not improve from 0.40438\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7997 - loss: 0.3796 - val_accuracy: 0.7840 - val_loss: 0.4048 - learning_rate: 1.9531e-05\n",
      "Epoch 227/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8012 - loss: 0.3742 \n",
      "Epoch 227: val_loss did not improve from 0.40438\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8012 - loss: 0.3766 - val_accuracy: 0.7834 - val_loss: 0.4048 - learning_rate: 1.9531e-05\n",
      "Epoch 228/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7967 - loss: 0.3887 \n",
      "Epoch 228: ReduceLROnPlateau reducing learning rate to 9.765624781721272e-06.\n",
      "\n",
      "Epoch 228: val_loss did not improve from 0.40438\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7983 - loss: 0.3858 - val_accuracy: 0.7840 - val_loss: 0.4048 - learning_rate: 1.9531e-05\n",
      "Epoch 229/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7887 - loss: 0.3922 \n",
      "Epoch 229: val_loss did not improve from 0.40438\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7941 - loss: 0.3868 - val_accuracy: 0.7840 - val_loss: 0.4048 - learning_rate: 9.7656e-06\n",
      "Epoch 230/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8073 - loss: 0.3723 \n",
      "Epoch 230: val_loss did not improve from 0.40438\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8038 - loss: 0.3769 - val_accuracy: 0.7840 - val_loss: 0.4048 - learning_rate: 9.7656e-06\n",
      "Epoch 231/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8135 - loss: 0.3711 \n",
      "Epoch 231: val_loss did not improve from 0.40438\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8081 - loss: 0.3749 - val_accuracy: 0.7840 - val_loss: 0.4048 - learning_rate: 9.7656e-06\n",
      "Epoch 232/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7933 - loss: 0.3891 \n",
      "Epoch 232: val_loss did not improve from 0.40438\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7968 - loss: 0.3849 - val_accuracy: 0.7834 - val_loss: 0.4048 - learning_rate: 9.7656e-06\n",
      "Epoch 233/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8005 - loss: 0.3771 \n",
      "Epoch 233: val_loss did not improve from 0.40438\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8010 - loss: 0.3784 - val_accuracy: 0.7834 - val_loss: 0.4047 - learning_rate: 9.7656e-06\n",
      "Epoch 234/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8065 - loss: 0.3713 \n",
      "Epoch 234: val_loss did not improve from 0.40438\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8039 - loss: 0.3757 - val_accuracy: 0.7840 - val_loss: 0.4048 - learning_rate: 9.7656e-06\n",
      "Epoch 235/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7993 - loss: 0.3831 \n",
      "Epoch 235: val_loss did not improve from 0.40438\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7995 - loss: 0.3824 - val_accuracy: 0.7840 - val_loss: 0.4048 - learning_rate: 9.7656e-06\n",
      "Epoch 236/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8117 - loss: 0.3676 \n",
      "Epoch 236: val_loss did not improve from 0.40438\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8068 - loss: 0.3730 - val_accuracy: 0.7834 - val_loss: 0.4048 - learning_rate: 9.7656e-06\n",
      "Epoch 236: early stopping\n",
      "Restoring model weights from the end of the best epoch: 176.\n",
      " Computing LSTM feature importance...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:01<00:00,  6.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- > Fold 5 < ---------------\n",
      "Epoch 1/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5625 - loss: 0.7694\n",
      "Epoch 1: val_loss improved from inf to 0.53509, saving model to folds4.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step - accuracy: 0.6093 - loss: 0.7025 - val_accuracy: 0.7126 - val_loss: 0.5351 - learning_rate: 0.0100\n",
      "Epoch 2/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7153 - loss: 0.5300 \n",
      "Epoch 2: val_loss improved from 0.53509 to 0.51880, saving model to folds4.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7153 - loss: 0.5296 - val_accuracy: 0.7175 - val_loss: 0.5188 - learning_rate: 0.0100\n",
      "Epoch 3/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7201 - loss: 0.5149 \n",
      "Epoch 3: val_loss improved from 0.51880 to 0.50204, saving model to folds4.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7198 - loss: 0.5115 - val_accuracy: 0.7151 - val_loss: 0.5020 - learning_rate: 0.0100\n",
      "Epoch 4/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7141 - loss: 0.4957 \n",
      "Epoch 4: val_loss improved from 0.50204 to 0.49479, saving model to folds4.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7175 - loss: 0.4933 - val_accuracy: 0.7181 - val_loss: 0.4948 - learning_rate: 0.0100\n",
      "Epoch 5/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7227 - loss: 0.4961 \n",
      "Epoch 5: val_loss improved from 0.49479 to 0.48354, saving model to folds4.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.7234 - loss: 0.4944 - val_accuracy: 0.7328 - val_loss: 0.4835 - learning_rate: 0.0100\n",
      "Epoch 6/300\n",
      "\u001b[1m13/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7367 - loss: 0.4735\n",
      "Epoch 6: val_loss improved from 0.48354 to 0.47900, saving model to folds4.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.7353 - loss: 0.4750 - val_accuracy: 0.7285 - val_loss: 0.4790 - learning_rate: 0.0100\n",
      "Epoch 7/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7345 - loss: 0.4846\n",
      "Epoch 7: val_loss improved from 0.47900 to 0.47880, saving model to folds4.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.7362 - loss: 0.4821 - val_accuracy: 0.7261 - val_loss: 0.4788 - learning_rate: 0.0100\n",
      "Epoch 8/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7400 - loss: 0.4727\n",
      "Epoch 8: val_loss improved from 0.47880 to 0.47559, saving model to folds4.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.7399 - loss: 0.4727 - val_accuracy: 0.7334 - val_loss: 0.4756 - learning_rate: 0.0100\n",
      "Epoch 9/300\n",
      "\u001b[1m13/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7418 - loss: 0.4723\n",
      "Epoch 9: val_loss did not improve from 0.47559\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.7413 - loss: 0.4719 - val_accuracy: 0.7242 - val_loss: 0.4854 - learning_rate: 0.0100\n",
      "Epoch 10/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7435 - loss: 0.4712\n",
      "Epoch 10: val_loss improved from 0.47559 to 0.46967, saving model to folds4.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.7428 - loss: 0.4720 - val_accuracy: 0.7413 - val_loss: 0.4697 - learning_rate: 0.0100\n",
      "Epoch 11/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7404 - loss: 0.4669\n",
      "Epoch 11: val_loss improved from 0.46967 to 0.46793, saving model to folds4.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.7405 - loss: 0.4664 - val_accuracy: 0.7334 - val_loss: 0.4679 - learning_rate: 0.0100\n",
      "Epoch 12/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7442 - loss: 0.4649\n",
      "Epoch 12: val_loss improved from 0.46793 to 0.46099, saving model to folds4.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.7447 - loss: 0.4650 - val_accuracy: 0.7431 - val_loss: 0.4610 - learning_rate: 0.0100\n",
      "Epoch 13/300\n",
      "\u001b[1m12/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7637 - loss: 0.4480 \n",
      "Epoch 13: val_loss did not improve from 0.46099\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.7604 - loss: 0.4508 - val_accuracy: 0.7383 - val_loss: 0.4691 - learning_rate: 0.0100\n",
      "Epoch 14/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7472 - loss: 0.4578\n",
      "Epoch 14: val_loss did not improve from 0.46099\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.7454 - loss: 0.4603 - val_accuracy: 0.7425 - val_loss: 0.4684 - learning_rate: 0.0100\n",
      "Epoch 15/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7463 - loss: 0.4666 \n",
      "Epoch 15: val_loss did not improve from 0.46099\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7451 - loss: 0.4662 - val_accuracy: 0.7370 - val_loss: 0.4685 - learning_rate: 0.0100\n",
      "Epoch 16/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7482 - loss: 0.4596 \n",
      "Epoch 16: val_loss did not improve from 0.46099\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7484 - loss: 0.4597 - val_accuracy: 0.7261 - val_loss: 0.4662 - learning_rate: 0.0100\n",
      "Epoch 17/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7320 - loss: 0.4698 \n",
      "Epoch 17: val_loss did not improve from 0.46099\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7363 - loss: 0.4675 - val_accuracy: 0.7444 - val_loss: 0.4629 - learning_rate: 0.0100\n",
      "Epoch 18/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7525 - loss: 0.4580 \n",
      "Epoch 18: val_loss improved from 0.46099 to 0.45499, saving model to folds4.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7533 - loss: 0.4565 - val_accuracy: 0.7621 - val_loss: 0.4550 - learning_rate: 0.0100\n",
      "Epoch 19/300\n",
      "\u001b[1m 7/15\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7615 - loss: 0.4494 \n",
      "Epoch 19: val_loss did not improve from 0.45499\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7571 - loss: 0.4531 - val_accuracy: 0.7547 - val_loss: 0.4603 - learning_rate: 0.0100\n",
      "Epoch 20/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7488 - loss: 0.4591 \n",
      "Epoch 20: val_loss improved from 0.45499 to 0.44961, saving model to folds4.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7526 - loss: 0.4566 - val_accuracy: 0.7541 - val_loss: 0.4496 - learning_rate: 0.0100\n",
      "Epoch 21/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7521 - loss: 0.4561 \n",
      "Epoch 21: val_loss did not improve from 0.44961\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7532 - loss: 0.4552 - val_accuracy: 0.7553 - val_loss: 0.4632 - learning_rate: 0.0100\n",
      "Epoch 22/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7570 - loss: 0.4556 \n",
      "Epoch 22: val_loss did not improve from 0.44961\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7603 - loss: 0.4522 - val_accuracy: 0.7376 - val_loss: 0.4601 - learning_rate: 0.0100\n",
      "Epoch 23/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7652 - loss: 0.4388 \n",
      "Epoch 23: val_loss did not improve from 0.44961\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7632 - loss: 0.4438 - val_accuracy: 0.7529 - val_loss: 0.4656 - learning_rate: 0.0100\n",
      "Epoch 24/300\n",
      "\u001b[1m 7/15\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7650 - loss: 0.4436 \n",
      "Epoch 24: val_loss did not improve from 0.44961\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7606 - loss: 0.4483 - val_accuracy: 0.7517 - val_loss: 0.4605 - learning_rate: 0.0100\n",
      "Epoch 25/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7578 - loss: 0.4535 \n",
      "Epoch 25: val_loss did not improve from 0.44961\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7594 - loss: 0.4512 - val_accuracy: 0.7614 - val_loss: 0.4499 - learning_rate: 0.0100\n",
      "Epoch 26/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7608 - loss: 0.4451 \n",
      "Epoch 26: val_loss did not improve from 0.44961\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7612 - loss: 0.4464 - val_accuracy: 0.7541 - val_loss: 0.4568 - learning_rate: 0.0100\n",
      "Epoch 27/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7588 - loss: 0.4503 \n",
      "Epoch 27: val_loss did not improve from 0.44961\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7583 - loss: 0.4503 - val_accuracy: 0.7437 - val_loss: 0.4545 - learning_rate: 0.0100\n",
      "Epoch 28/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7652 - loss: 0.4377 \n",
      "Epoch 28: val_loss improved from 0.44961 to 0.44750, saving model to folds4.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7663 - loss: 0.4394 - val_accuracy: 0.7492 - val_loss: 0.4475 - learning_rate: 0.0100\n",
      "Epoch 29/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7626 - loss: 0.4361  \n",
      "Epoch 29: val_loss improved from 0.44750 to 0.44620, saving model to folds4.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7632 - loss: 0.4369 - val_accuracy: 0.7444 - val_loss: 0.4462 - learning_rate: 0.0100\n",
      "Epoch 30/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7610 - loss: 0.4441 \n",
      "Epoch 30: val_loss improved from 0.44620 to 0.44468, saving model to folds4.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7624 - loss: 0.4437 - val_accuracy: 0.7712 - val_loss: 0.4447 - learning_rate: 0.0100\n",
      "Epoch 31/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7679 - loss: 0.4404 \n",
      "Epoch 31: val_loss did not improve from 0.44468\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7693 - loss: 0.4389 - val_accuracy: 0.7743 - val_loss: 0.4482 - learning_rate: 0.0100\n",
      "Epoch 32/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7696 - loss: 0.4419 \n",
      "Epoch 32: val_loss did not improve from 0.44468\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7685 - loss: 0.4406 - val_accuracy: 0.7602 - val_loss: 0.4456 - learning_rate: 0.0100\n",
      "Epoch 33/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7576 - loss: 0.4455 \n",
      "Epoch 33: val_loss did not improve from 0.44468\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7591 - loss: 0.4429 - val_accuracy: 0.7541 - val_loss: 0.4557 - learning_rate: 0.0100\n",
      "Epoch 34/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7665 - loss: 0.4412  \n",
      "Epoch 34: val_loss improved from 0.44468 to 0.44289, saving model to folds4.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7663 - loss: 0.4397 - val_accuracy: 0.7517 - val_loss: 0.4429 - learning_rate: 0.0100\n",
      "Epoch 35/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7694 - loss: 0.4296 \n",
      "Epoch 35: val_loss improved from 0.44289 to 0.43799, saving model to folds4.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7691 - loss: 0.4317 - val_accuracy: 0.7718 - val_loss: 0.4380 - learning_rate: 0.0100\n",
      "Epoch 36/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7803 - loss: 0.4241 \n",
      "Epoch 36: val_loss did not improve from 0.43799\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7766 - loss: 0.4277 - val_accuracy: 0.7431 - val_loss: 0.4448 - learning_rate: 0.0100\n",
      "Epoch 37/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7610 - loss: 0.4343 \n",
      "Epoch 37: val_loss improved from 0.43799 to 0.43495, saving model to folds4.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7642 - loss: 0.4337 - val_accuracy: 0.7706 - val_loss: 0.4349 - learning_rate: 0.0100\n",
      "Epoch 38/300\n",
      "\u001b[1m 7/15\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7778 - loss: 0.4254 \n",
      "Epoch 38: val_loss did not improve from 0.43495\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7748 - loss: 0.4270 - val_accuracy: 0.7468 - val_loss: 0.4454 - learning_rate: 0.0100\n",
      "Epoch 39/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7721 - loss: 0.4303 \n",
      "Epoch 39: val_loss did not improve from 0.43495\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7718 - loss: 0.4302 - val_accuracy: 0.7535 - val_loss: 0.4366 - learning_rate: 0.0100\n",
      "Epoch 40/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7590 - loss: 0.4399 \n",
      "Epoch 40: val_loss did not improve from 0.43495\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7604 - loss: 0.4378 - val_accuracy: 0.7559 - val_loss: 0.4435 - learning_rate: 0.0100\n",
      "Epoch 41/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7554 - loss: 0.4417 \n",
      "Epoch 41: val_loss improved from 0.43495 to 0.43375, saving model to folds4.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7579 - loss: 0.4411 - val_accuracy: 0.7730 - val_loss: 0.4337 - learning_rate: 0.0100\n",
      "Epoch 42/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7777 - loss: 0.4251 \n",
      "Epoch 42: val_loss improved from 0.43375 to 0.42958, saving model to folds4.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7766 - loss: 0.4254 - val_accuracy: 0.7749 - val_loss: 0.4296 - learning_rate: 0.0100\n",
      "Epoch 43/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7793 - loss: 0.4174\n",
      "Epoch 43: val_loss improved from 0.42958 to 0.42653, saving model to folds4.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7789 - loss: 0.4178 - val_accuracy: 0.7773 - val_loss: 0.4265 - learning_rate: 0.0100\n",
      "Epoch 44/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7781 - loss: 0.4193 \n",
      "Epoch 44: val_loss did not improve from 0.42653\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7776 - loss: 0.4206 - val_accuracy: 0.7749 - val_loss: 0.4293 - learning_rate: 0.0100\n",
      "Epoch 45/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7863 - loss: 0.4161 \n",
      "Epoch 45: val_loss did not improve from 0.42653\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7809 - loss: 0.4197 - val_accuracy: 0.7669 - val_loss: 0.4303 - learning_rate: 0.0100\n",
      "Epoch 46/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7798 - loss: 0.4232 \n",
      "Epoch 46: val_loss did not improve from 0.42653\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7804 - loss: 0.4234 - val_accuracy: 0.7584 - val_loss: 0.4385 - learning_rate: 0.0100\n",
      "Epoch 47/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7699 - loss: 0.4223 \n",
      "Epoch 47: val_loss did not improve from 0.42653\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7706 - loss: 0.4227 - val_accuracy: 0.7596 - val_loss: 0.4300 - learning_rate: 0.0100\n",
      "Epoch 48/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7732 - loss: 0.4208  \n",
      "Epoch 48: val_loss did not improve from 0.42653\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7746 - loss: 0.4198 - val_accuracy: 0.7657 - val_loss: 0.4320 - learning_rate: 0.0100\n",
      "Epoch 49/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7802 - loss: 0.4109 \n",
      "Epoch 49: val_loss did not improve from 0.42653\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7771 - loss: 0.4159 - val_accuracy: 0.7657 - val_loss: 0.4326 - learning_rate: 0.0100\n",
      "Epoch 50/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7701 - loss: 0.4322 \n",
      "Epoch 50: val_loss did not improve from 0.42653\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7704 - loss: 0.4294 - val_accuracy: 0.7749 - val_loss: 0.4269 - learning_rate: 0.0100\n",
      "Epoch 51/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7820 - loss: 0.4145 \n",
      "Epoch 51: val_loss did not improve from 0.42653\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7813 - loss: 0.4161 - val_accuracy: 0.7743 - val_loss: 0.4267 - learning_rate: 0.0100\n",
      "Epoch 52/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7722 - loss: 0.4252 \n",
      "Epoch 52: val_loss did not improve from 0.42653\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7734 - loss: 0.4234 - val_accuracy: 0.7572 - val_loss: 0.4281 - learning_rate: 0.0100\n",
      "Epoch 53/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7713 - loss: 0.4207 \n",
      "Epoch 53: val_loss improved from 0.42653 to 0.42391, saving model to folds4.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.7722 - loss: 0.4208 - val_accuracy: 0.7828 - val_loss: 0.4239 - learning_rate: 0.0100\n",
      "Epoch 54/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7902 - loss: 0.4105 \n",
      "Epoch 54: val_loss did not improve from 0.42391\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7865 - loss: 0.4135 - val_accuracy: 0.7639 - val_loss: 0.4276 - learning_rate: 0.0100\n",
      "Epoch 55/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7707 - loss: 0.4197  \n",
      "Epoch 55: val_loss did not improve from 0.42391\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7712 - loss: 0.4189 - val_accuracy: 0.7694 - val_loss: 0.4280 - learning_rate: 0.0100\n",
      "Epoch 56/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7829 - loss: 0.4211 \n",
      "Epoch 56: val_loss did not improve from 0.42391\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7828 - loss: 0.4193 - val_accuracy: 0.7816 - val_loss: 0.4283 - learning_rate: 0.0100\n",
      "Epoch 57/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7791 - loss: 0.4220 \n",
      "Epoch 57: val_loss improved from 0.42391 to 0.42098, saving model to folds4.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7788 - loss: 0.4202 - val_accuracy: 0.7718 - val_loss: 0.4210 - learning_rate: 0.0100\n",
      "Epoch 58/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7636 - loss: 0.4217 \n",
      "Epoch 58: val_loss did not improve from 0.42098\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7654 - loss: 0.4226 - val_accuracy: 0.7633 - val_loss: 0.4300 - learning_rate: 0.0100\n",
      "Epoch 59/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7708 - loss: 0.4211 \n",
      "Epoch 59: val_loss did not improve from 0.42098\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7729 - loss: 0.4190 - val_accuracy: 0.7566 - val_loss: 0.4342 - learning_rate: 0.0100\n",
      "Epoch 60/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7756 - loss: 0.4165 \n",
      "Epoch 60: val_loss did not improve from 0.42098\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7765 - loss: 0.4166 - val_accuracy: 0.7621 - val_loss: 0.4314 - learning_rate: 0.0100\n",
      "Epoch 61/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7790 - loss: 0.4136 \n",
      "Epoch 61: val_loss improved from 0.42098 to 0.42056, saving model to folds4.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.7797 - loss: 0.4145 - val_accuracy: 0.7785 - val_loss: 0.4206 - learning_rate: 0.0100\n",
      "Epoch 62/300\n",
      "\u001b[1m13/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7886 - loss: 0.4062\n",
      "Epoch 62: val_loss did not improve from 0.42056\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.7875 - loss: 0.4079 - val_accuracy: 0.7657 - val_loss: 0.4224 - learning_rate: 0.0100\n",
      "Epoch 63/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7810 - loss: 0.4140\n",
      "Epoch 63: val_loss did not improve from 0.42056\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.7804 - loss: 0.4142 - val_accuracy: 0.7804 - val_loss: 0.4230 - learning_rate: 0.0100\n",
      "Epoch 64/300\n",
      "\u001b[1m12/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7725 - loss: 0.4220\n",
      "Epoch 64: val_loss did not improve from 0.42056\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.7747 - loss: 0.4202 - val_accuracy: 0.7718 - val_loss: 0.4210 - learning_rate: 0.0100\n",
      "Epoch 65/300\n",
      "\u001b[1m12/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7698 - loss: 0.4164\n",
      "Epoch 65: val_loss did not improve from 0.42056\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.7700 - loss: 0.4169 - val_accuracy: 0.7718 - val_loss: 0.4272 - learning_rate: 0.0100\n",
      "Epoch 66/300\n",
      "\u001b[1m13/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7733 - loss: 0.4144\n",
      "Epoch 66: val_loss did not improve from 0.42056\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.7740 - loss: 0.4145 - val_accuracy: 0.7688 - val_loss: 0.4230 - learning_rate: 0.0100\n",
      "Epoch 67/300\n",
      "\u001b[1m13/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7801 - loss: 0.4103\n",
      "Epoch 67: val_loss did not improve from 0.42056\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7802 - loss: 0.4107 - val_accuracy: 0.7773 - val_loss: 0.4222 - learning_rate: 0.0100\n",
      "Epoch 68/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7718 - loss: 0.4170\n",
      "Epoch 68: val_loss did not improve from 0.42056\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.7723 - loss: 0.4167 - val_accuracy: 0.7779 - val_loss: 0.4237 - learning_rate: 0.0100\n",
      "Epoch 69/300\n",
      "\u001b[1m13/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7826 - loss: 0.4152\n",
      "Epoch 69: val_loss did not improve from 0.42056\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.7828 - loss: 0.4147 - val_accuracy: 0.7700 - val_loss: 0.4227 - learning_rate: 0.0100\n",
      "Epoch 70/300\n",
      "\u001b[1m13/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7816 - loss: 0.4124\n",
      "Epoch 70: val_loss improved from 0.42056 to 0.41737, saving model to folds4.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.7821 - loss: 0.4117 - val_accuracy: 0.7785 - val_loss: 0.4174 - learning_rate: 0.0100\n",
      "Epoch 71/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7964 - loss: 0.3964 \n",
      "Epoch 71: val_loss did not improve from 0.41737\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7933 - loss: 0.4006 - val_accuracy: 0.7682 - val_loss: 0.4243 - learning_rate: 0.0100\n",
      "Epoch 72/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7793 - loss: 0.4157 \n",
      "Epoch 72: val_loss did not improve from 0.41737\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7795 - loss: 0.4149 - val_accuracy: 0.7712 - val_loss: 0.4224 - learning_rate: 0.0100\n",
      "Epoch 73/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7735 - loss: 0.4118 \n",
      "Epoch 73: val_loss did not improve from 0.41737\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7766 - loss: 0.4120 - val_accuracy: 0.7755 - val_loss: 0.4308 - learning_rate: 0.0100\n",
      "Epoch 74/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7862 - loss: 0.4155 \n",
      "Epoch 74: val_loss did not improve from 0.41737\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7868 - loss: 0.4141 - val_accuracy: 0.7749 - val_loss: 0.4233 - learning_rate: 0.0100\n",
      "Epoch 75/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7874 - loss: 0.4077 \n",
      "Epoch 75: val_loss improved from 0.41737 to 0.41692, saving model to folds4.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7858 - loss: 0.4089 - val_accuracy: 0.7834 - val_loss: 0.4169 - learning_rate: 0.0100\n",
      "Epoch 76/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7878 - loss: 0.4090 \n",
      "Epoch 76: val_loss did not improve from 0.41692\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7849 - loss: 0.4106 - val_accuracy: 0.7773 - val_loss: 0.4193 - learning_rate: 0.0100\n",
      "Epoch 77/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7805 - loss: 0.4077 \n",
      "Epoch 77: val_loss did not improve from 0.41692\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7803 - loss: 0.4077 - val_accuracy: 0.7791 - val_loss: 0.4225 - learning_rate: 0.0100\n",
      "Epoch 78/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7850 - loss: 0.4133 \n",
      "Epoch 78: val_loss did not improve from 0.41692\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7842 - loss: 0.4115 - val_accuracy: 0.7749 - val_loss: 0.4304 - learning_rate: 0.0100\n",
      "Epoch 79/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7909 - loss: 0.4096  \n",
      "Epoch 79: val_loss did not improve from 0.41692\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7888 - loss: 0.4101 - val_accuracy: 0.7852 - val_loss: 0.4184 - learning_rate: 0.0100\n",
      "Epoch 80/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7866 - loss: 0.4111 \n",
      "Epoch 80: val_loss did not improve from 0.41692\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7854 - loss: 0.4121 - val_accuracy: 0.7755 - val_loss: 0.4322 - learning_rate: 0.0100\n",
      "Epoch 81/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7719 - loss: 0.4252  \n",
      "Epoch 81: val_loss improved from 0.41692 to 0.41650, saving model to folds4.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7757 - loss: 0.4194 - val_accuracy: 0.7877 - val_loss: 0.4165 - learning_rate: 0.0100\n",
      "Epoch 82/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7790 - loss: 0.4205 \n",
      "Epoch 82: val_loss did not improve from 0.41650\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7822 - loss: 0.4171 - val_accuracy: 0.7822 - val_loss: 0.4172 - learning_rate: 0.0100\n",
      "Epoch 83/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7825 - loss: 0.4061 \n",
      "Epoch 83: val_loss improved from 0.41650 to 0.41550, saving model to folds4.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7837 - loss: 0.4078 - val_accuracy: 0.7810 - val_loss: 0.4155 - learning_rate: 0.0100\n",
      "Epoch 84/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7890 - loss: 0.4091 \n",
      "Epoch 84: val_loss did not improve from 0.41550\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7892 - loss: 0.4092 - val_accuracy: 0.7755 - val_loss: 0.4199 - learning_rate: 0.0100\n",
      "Epoch 85/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7916 - loss: 0.4018\n",
      "Epoch 85: val_loss improved from 0.41550 to 0.41382, saving model to folds4.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7914 - loss: 0.4022 - val_accuracy: 0.7889 - val_loss: 0.4138 - learning_rate: 0.0100\n",
      "Epoch 86/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7885 - loss: 0.4044 \n",
      "Epoch 86: val_loss did not improve from 0.41382\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7873 - loss: 0.4060 - val_accuracy: 0.7743 - val_loss: 0.4280 - learning_rate: 0.0100\n",
      "Epoch 87/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7918 - loss: 0.4149 \n",
      "Epoch 87: val_loss did not improve from 0.41382\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7913 - loss: 0.4121 - val_accuracy: 0.7688 - val_loss: 0.4203 - learning_rate: 0.0100\n",
      "Epoch 88/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7926 - loss: 0.4019 \n",
      "Epoch 88: val_loss did not improve from 0.41382\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7901 - loss: 0.4061 - val_accuracy: 0.7804 - val_loss: 0.4155 - learning_rate: 0.0100\n",
      "Epoch 89/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7947 - loss: 0.3993 \n",
      "Epoch 89: val_loss did not improve from 0.41382\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7901 - loss: 0.4035 - val_accuracy: 0.7736 - val_loss: 0.4205 - learning_rate: 0.0100\n",
      "Epoch 90/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7826 - loss: 0.4099\n",
      "Epoch 90: val_loss did not improve from 0.41382\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7831 - loss: 0.4098 - val_accuracy: 0.7828 - val_loss: 0.4141 - learning_rate: 0.0100\n",
      "Epoch 91/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7935 - loss: 0.4050 \n",
      "Epoch 91: val_loss did not improve from 0.41382\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7916 - loss: 0.4055 - val_accuracy: 0.7712 - val_loss: 0.4181 - learning_rate: 0.0100\n",
      "Epoch 92/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7871 - loss: 0.4089 \n",
      "Epoch 92: val_loss did not improve from 0.41382\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7888 - loss: 0.4073 - val_accuracy: 0.7736 - val_loss: 0.4218 - learning_rate: 0.0100\n",
      "Epoch 93/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7866 - loss: 0.4090 \n",
      "Epoch 93: val_loss improved from 0.41382 to 0.40776, saving model to folds4.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7891 - loss: 0.4059 - val_accuracy: 0.7968 - val_loss: 0.4078 - learning_rate: 0.0100\n",
      "Epoch 94/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7988 - loss: 0.3921 \n",
      "Epoch 94: val_loss did not improve from 0.40776\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7984 - loss: 0.3939 - val_accuracy: 0.7773 - val_loss: 0.4104 - learning_rate: 0.0100\n",
      "Epoch 95/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7863 - loss: 0.4049\n",
      "Epoch 95: val_loss did not improve from 0.40776\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7868 - loss: 0.4046 - val_accuracy: 0.7962 - val_loss: 0.4094 - learning_rate: 0.0100\n",
      "Epoch 96/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8004 - loss: 0.4019 \n",
      "Epoch 96: val_loss did not improve from 0.40776\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7973 - loss: 0.4022 - val_accuracy: 0.7895 - val_loss: 0.4079 - learning_rate: 0.0100\n",
      "Epoch 97/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7887 - loss: 0.4035 \n",
      "Epoch 97: val_loss did not improve from 0.40776\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7908 - loss: 0.4012 - val_accuracy: 0.7883 - val_loss: 0.4163 - learning_rate: 0.0100\n",
      "Epoch 98/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8020 - loss: 0.3962 \n",
      "Epoch 98: val_loss improved from 0.40776 to 0.40712, saving model to folds4.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8001 - loss: 0.3977 - val_accuracy: 0.7944 - val_loss: 0.4071 - learning_rate: 0.0100\n",
      "Epoch 99/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7997 - loss: 0.3992  \n",
      "Epoch 99: val_loss did not improve from 0.40712\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7987 - loss: 0.3997 - val_accuracy: 0.7749 - val_loss: 0.4190 - learning_rate: 0.0100\n",
      "Epoch 100/300\n",
      "\u001b[1m 7/15\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7837 - loss: 0.4066 \n",
      "Epoch 100: val_loss did not improve from 0.40712\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7869 - loss: 0.4038 - val_accuracy: 0.7816 - val_loss: 0.4158 - learning_rate: 0.0100\n",
      "Epoch 101/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7895 - loss: 0.4039 \n",
      "Epoch 101: val_loss did not improve from 0.40712\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7867 - loss: 0.4079 - val_accuracy: 0.7907 - val_loss: 0.4106 - learning_rate: 0.0100\n",
      "Epoch 102/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7913 - loss: 0.3990 \n",
      "Epoch 102: val_loss did not improve from 0.40712\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7914 - loss: 0.3996 - val_accuracy: 0.7743 - val_loss: 0.4108 - learning_rate: 0.0100\n",
      "Epoch 103/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7840 - loss: 0.4082 \n",
      "Epoch 103: val_loss improved from 0.40712 to 0.40490, saving model to folds4.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7869 - loss: 0.4050 - val_accuracy: 0.7743 - val_loss: 0.4049 - learning_rate: 0.0100\n",
      "Epoch 104/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7996 - loss: 0.3894 \n",
      "Epoch 104: val_loss improved from 0.40490 to 0.40428, saving model to folds4.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7975 - loss: 0.3908 - val_accuracy: 0.7877 - val_loss: 0.4043 - learning_rate: 0.0100\n",
      "Epoch 105/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7946 - loss: 0.3972\n",
      "Epoch 105: val_loss did not improve from 0.40428\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7946 - loss: 0.3972 - val_accuracy: 0.8005 - val_loss: 0.4057 - learning_rate: 0.0100\n",
      "Epoch 106/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8043 - loss: 0.3892 \n",
      "Epoch 106: val_loss did not improve from 0.40428\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8023 - loss: 0.3903 - val_accuracy: 0.7718 - val_loss: 0.4068 - learning_rate: 0.0100\n",
      "Epoch 107/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7919 - loss: 0.3878 \n",
      "Epoch 107: val_loss improved from 0.40428 to 0.40347, saving model to folds4.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7905 - loss: 0.3913 - val_accuracy: 0.7865 - val_loss: 0.4035 - learning_rate: 0.0100\n",
      "Epoch 108/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8027 - loss: 0.3890 \n",
      "Epoch 108: val_loss did not improve from 0.40347\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7998 - loss: 0.3912 - val_accuracy: 0.7956 - val_loss: 0.4085 - learning_rate: 0.0100\n",
      "Epoch 109/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7934 - loss: 0.4016 \n",
      "Epoch 109: val_loss did not improve from 0.40347\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7892 - loss: 0.4037 - val_accuracy: 0.7791 - val_loss: 0.4134 - learning_rate: 0.0100\n",
      "Epoch 110/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7951 - loss: 0.4009  \n",
      "Epoch 110: val_loss did not improve from 0.40347\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7931 - loss: 0.4017 - val_accuracy: 0.7590 - val_loss: 0.4212 - learning_rate: 0.0100\n",
      "Epoch 111/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7835 - loss: 0.4091 \n",
      "Epoch 111: val_loss did not improve from 0.40347\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7842 - loss: 0.4072 - val_accuracy: 0.7816 - val_loss: 0.4101 - learning_rate: 0.0100\n",
      "Epoch 112/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7853 - loss: 0.4038 \n",
      "Epoch 112: val_loss did not improve from 0.40347\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7885 - loss: 0.4003 - val_accuracy: 0.7755 - val_loss: 0.4054 - learning_rate: 0.0100\n",
      "Epoch 113/300\n",
      "\u001b[1m 7/15\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7922 - loss: 0.3935 \n",
      "Epoch 113: val_loss did not improve from 0.40347\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7920 - loss: 0.3943 - val_accuracy: 0.7865 - val_loss: 0.4067 - learning_rate: 0.0100\n",
      "Epoch 114/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7867 - loss: 0.3996 \n",
      "Epoch 114: val_loss improved from 0.40347 to 0.40029, saving model to folds4.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7887 - loss: 0.3974 - val_accuracy: 0.7901 - val_loss: 0.4003 - learning_rate: 0.0100\n",
      "Epoch 115/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7848 - loss: 0.3931\n",
      "Epoch 115: val_loss did not improve from 0.40029\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7849 - loss: 0.3935 - val_accuracy: 0.7895 - val_loss: 0.4114 - learning_rate: 0.0100\n",
      "Epoch 116/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7871 - loss: 0.4013 \n",
      "Epoch 116: val_loss did not improve from 0.40029\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7882 - loss: 0.4024 - val_accuracy: 0.7791 - val_loss: 0.4060 - learning_rate: 0.0100\n",
      "Epoch 117/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7897 - loss: 0.4056 \n",
      "Epoch 117: val_loss did not improve from 0.40029\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7924 - loss: 0.4018 - val_accuracy: 0.7755 - val_loss: 0.4107 - learning_rate: 0.0100\n",
      "Epoch 118/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7899 - loss: 0.4008 \n",
      "Epoch 118: val_loss did not improve from 0.40029\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7897 - loss: 0.4011 - val_accuracy: 0.7871 - val_loss: 0.4151 - learning_rate: 0.0100\n",
      "Epoch 119/300\n",
      "\u001b[1m13/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7940 - loss: 0.4008 \n",
      "Epoch 119: val_loss did not improve from 0.40029\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.7939 - loss: 0.4007 - val_accuracy: 0.7816 - val_loss: 0.4130 - learning_rate: 0.0100\n",
      "Epoch 120/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7955 - loss: 0.3997\n",
      "Epoch 120: val_loss did not improve from 0.40029\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.7956 - loss: 0.3995 - val_accuracy: 0.7926 - val_loss: 0.4081 - learning_rate: 0.0100\n",
      "Epoch 121/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8017 - loss: 0.3972\n",
      "Epoch 121: val_loss did not improve from 0.40029\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.8020 - loss: 0.3955 - val_accuracy: 0.7956 - val_loss: 0.4030 - learning_rate: 0.0100\n",
      "Epoch 122/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8122 - loss: 0.3829\n",
      "Epoch 122: val_loss did not improve from 0.40029\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.8101 - loss: 0.3845 - val_accuracy: 0.7901 - val_loss: 0.4023 - learning_rate: 0.0100\n",
      "Epoch 123/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7971 - loss: 0.3903\n",
      "Epoch 123: val_loss did not improve from 0.40029\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.7973 - loss: 0.3905 - val_accuracy: 0.7907 - val_loss: 0.4022 - learning_rate: 0.0100\n",
      "Epoch 124/300\n",
      "\u001b[1m13/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8040 - loss: 0.3866\n",
      "Epoch 124: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
      "\n",
      "Epoch 124: val_loss did not improve from 0.40029\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8026 - loss: 0.3880 - val_accuracy: 0.7901 - val_loss: 0.4019 - learning_rate: 0.0100\n",
      "Epoch 125/300\n",
      "\u001b[1m12/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8020 - loss: 0.3897\n",
      "Epoch 125: val_loss improved from 0.40029 to 0.39889, saving model to folds4.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.8015 - loss: 0.3898 - val_accuracy: 0.7913 - val_loss: 0.3989 - learning_rate: 0.0050\n",
      "Epoch 126/300\n",
      "\u001b[1m12/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7973 - loss: 0.3870\n",
      "Epoch 126: val_loss improved from 0.39889 to 0.39471, saving model to folds4.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.7980 - loss: 0.3870 - val_accuracy: 0.7797 - val_loss: 0.3947 - learning_rate: 0.0050\n",
      "Epoch 127/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8005 - loss: 0.3757 \n",
      "Epoch 127: val_loss did not improve from 0.39471\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8014 - loss: 0.3799 - val_accuracy: 0.7871 - val_loss: 0.4002 - learning_rate: 0.0050\n",
      "Epoch 128/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7923 - loss: 0.3963 \n",
      "Epoch 128: val_loss did not improve from 0.39471\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7970 - loss: 0.3911 - val_accuracy: 0.7822 - val_loss: 0.3979 - learning_rate: 0.0050\n",
      "Epoch 129/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8007 - loss: 0.3871 \n",
      "Epoch 129: val_loss did not improve from 0.39471\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8001 - loss: 0.3868 - val_accuracy: 0.8005 - val_loss: 0.3960 - learning_rate: 0.0050\n",
      "Epoch 130/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7913 - loss: 0.3963 \n",
      "Epoch 130: val_loss did not improve from 0.39471\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7941 - loss: 0.3914 - val_accuracy: 0.7913 - val_loss: 0.3976 - learning_rate: 0.0050\n",
      "Epoch 131/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8014 - loss: 0.3818 \n",
      "Epoch 131: val_loss improved from 0.39471 to 0.39375, saving model to folds4.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8003 - loss: 0.3829 - val_accuracy: 0.7950 - val_loss: 0.3937 - learning_rate: 0.0050\n",
      "Epoch 132/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8023 - loss: 0.3777 \n",
      "Epoch 132: val_loss did not improve from 0.39375\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8012 - loss: 0.3798 - val_accuracy: 0.7913 - val_loss: 0.3960 - learning_rate: 0.0050\n",
      "Epoch 133/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7951 - loss: 0.3919\n",
      "Epoch 133: val_loss improved from 0.39375 to 0.39152, saving model to folds4.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7955 - loss: 0.3914 - val_accuracy: 0.7993 - val_loss: 0.3915 - learning_rate: 0.0050\n",
      "Epoch 134/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8109 - loss: 0.3743 \n",
      "Epoch 134: val_loss did not improve from 0.39152\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8074 - loss: 0.3774 - val_accuracy: 0.7944 - val_loss: 0.3956 - learning_rate: 0.0050\n",
      "Epoch 135/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8016 - loss: 0.3803 \n",
      "Epoch 135: val_loss did not improve from 0.39152\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8016 - loss: 0.3811 - val_accuracy: 0.7926 - val_loss: 0.3927 - learning_rate: 0.0050\n",
      "Epoch 136/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7968 - loss: 0.3861 \n",
      "Epoch 136: val_loss did not improve from 0.39152\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7975 - loss: 0.3852 - val_accuracy: 0.7846 - val_loss: 0.3952 - learning_rate: 0.0050\n",
      "Epoch 137/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7998 - loss: 0.3785 \n",
      "Epoch 137: val_loss did not improve from 0.39152\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8002 - loss: 0.3800 - val_accuracy: 0.7865 - val_loss: 0.3928 - learning_rate: 0.0050\n",
      "Epoch 138/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7914 - loss: 0.3876\n",
      "Epoch 138: val_loss did not improve from 0.39152\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7919 - loss: 0.3873 - val_accuracy: 0.8011 - val_loss: 0.3958 - learning_rate: 0.0050\n",
      "Epoch 139/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7941 - loss: 0.3871 \n",
      "Epoch 139: val_loss did not improve from 0.39152\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7980 - loss: 0.3849 - val_accuracy: 0.7974 - val_loss: 0.3916 - learning_rate: 0.0050\n",
      "Epoch 140/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7977 - loss: 0.3845\n",
      "Epoch 140: val_loss did not improve from 0.39152\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7979 - loss: 0.3843 - val_accuracy: 0.7871 - val_loss: 0.3923 - learning_rate: 0.0050\n",
      "Epoch 141/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8000 - loss: 0.3890 \n",
      "Epoch 141: val_loss did not improve from 0.39152\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8013 - loss: 0.3874 - val_accuracy: 0.7907 - val_loss: 0.3921 - learning_rate: 0.0050\n",
      "Epoch 142/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8009 - loss: 0.3800 \n",
      "Epoch 142: val_loss did not improve from 0.39152\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8001 - loss: 0.3820 - val_accuracy: 0.7846 - val_loss: 0.3982 - learning_rate: 0.0050\n",
      "Epoch 143/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7976 - loss: 0.3854 \n",
      "Epoch 143: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "\n",
      "Epoch 143: val_loss did not improve from 0.39152\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7977 - loss: 0.3842 - val_accuracy: 0.7907 - val_loss: 0.3929 - learning_rate: 0.0050\n",
      "Epoch 144/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8025 - loss: 0.3831 \n",
      "Epoch 144: val_loss improved from 0.39152 to 0.39044, saving model to folds4.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8028 - loss: 0.3810 - val_accuracy: 0.7932 - val_loss: 0.3904 - learning_rate: 0.0025\n",
      "Epoch 145/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8050 - loss: 0.3751  \n",
      "Epoch 145: val_loss did not improve from 0.39044\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8041 - loss: 0.3765 - val_accuracy: 0.7993 - val_loss: 0.3910 - learning_rate: 0.0025\n",
      "Epoch 146/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8093 - loss: 0.3738 \n",
      "Epoch 146: val_loss improved from 0.39044 to 0.39006, saving model to folds4.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8064 - loss: 0.3761 - val_accuracy: 0.7993 - val_loss: 0.3901 - learning_rate: 0.0025\n",
      "Epoch 147/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8150 - loss: 0.3684 \n",
      "Epoch 147: val_loss did not improve from 0.39006\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8110 - loss: 0.3719 - val_accuracy: 0.7901 - val_loss: 0.3921 - learning_rate: 0.0025\n",
      "Epoch 148/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8062 - loss: 0.3753  \n",
      "Epoch 148: val_loss did not improve from 0.39006\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8047 - loss: 0.3763 - val_accuracy: 0.7980 - val_loss: 0.3927 - learning_rate: 0.0025\n",
      "Epoch 149/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8009 - loss: 0.3887 \n",
      "Epoch 149: val_loss improved from 0.39006 to 0.38965, saving model to folds4.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8027 - loss: 0.3840 - val_accuracy: 0.7956 - val_loss: 0.3897 - learning_rate: 0.0025\n",
      "Epoch 150/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8057 - loss: 0.3747 \n",
      "Epoch 150: val_loss improved from 0.38965 to 0.38942, saving model to folds4.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8046 - loss: 0.3756 - val_accuracy: 0.7938 - val_loss: 0.3894 - learning_rate: 0.0025\n",
      "Epoch 151/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8046 - loss: 0.3753 \n",
      "Epoch 151: val_loss did not improve from 0.38942\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8036 - loss: 0.3763 - val_accuracy: 0.7974 - val_loss: 0.3895 - learning_rate: 0.0025\n",
      "Epoch 152/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8008 - loss: 0.3780\n",
      "Epoch 152: val_loss did not improve from 0.38942\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8008 - loss: 0.3779 - val_accuracy: 0.7938 - val_loss: 0.3900 - learning_rate: 0.0025\n",
      "Epoch 153/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8091 - loss: 0.3732 \n",
      "Epoch 153: val_loss improved from 0.38942 to 0.38897, saving model to folds4.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8067 - loss: 0.3760 - val_accuracy: 0.7956 - val_loss: 0.3890 - learning_rate: 0.0025\n",
      "Epoch 154/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8004 - loss: 0.3829 \n",
      "Epoch 154: val_loss did not improve from 0.38897\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8022 - loss: 0.3804 - val_accuracy: 0.7956 - val_loss: 0.3912 - learning_rate: 0.0025\n",
      "Epoch 155/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8060 - loss: 0.3707 \n",
      "Epoch 155: val_loss improved from 0.38897 to 0.38782, saving model to folds4.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8045 - loss: 0.3735 - val_accuracy: 0.7999 - val_loss: 0.3878 - learning_rate: 0.0025\n",
      "Epoch 156/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8082 - loss: 0.3738 \n",
      "Epoch 156: val_loss improved from 0.38782 to 0.38706, saving model to folds4.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8063 - loss: 0.3747 - val_accuracy: 0.8048 - val_loss: 0.3871 - learning_rate: 0.0025\n",
      "Epoch 157/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8004 - loss: 0.3828\n",
      "Epoch 157: val_loss did not improve from 0.38706\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8006 - loss: 0.3824 - val_accuracy: 0.7987 - val_loss: 0.3892 - learning_rate: 0.0025\n",
      "Epoch 158/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8083 - loss: 0.3701 \n",
      "Epoch 158: val_loss did not improve from 0.38706\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8068 - loss: 0.3722 - val_accuracy: 0.7968 - val_loss: 0.3883 - learning_rate: 0.0025\n",
      "Epoch 159/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8008 - loss: 0.3796 \n",
      "Epoch 159: val_loss did not improve from 0.38706\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8015 - loss: 0.3789 - val_accuracy: 0.8011 - val_loss: 0.3880 - learning_rate: 0.0025\n",
      "Epoch 160/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8068 - loss: 0.3781 \n",
      "Epoch 160: val_loss did not improve from 0.38706\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8063 - loss: 0.3776 - val_accuracy: 0.7950 - val_loss: 0.3889 - learning_rate: 0.0025\n",
      "Epoch 161/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8101 - loss: 0.3727 \n",
      "Epoch 161: val_loss did not improve from 0.38706\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8086 - loss: 0.3735 - val_accuracy: 0.8011 - val_loss: 0.3885 - learning_rate: 0.0025\n",
      "Epoch 162/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8017 - loss: 0.3847  \n",
      "Epoch 162: val_loss did not improve from 0.38706\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8037 - loss: 0.3803 - val_accuracy: 0.7944 - val_loss: 0.3873 - learning_rate: 0.0025\n",
      "Epoch 163/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8034 - loss: 0.3753 \n",
      "Epoch 163: val_loss did not improve from 0.38706\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8041 - loss: 0.3751 - val_accuracy: 0.7980 - val_loss: 0.3873 - learning_rate: 0.0025\n",
      "Epoch 164/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8121 - loss: 0.3680  \n",
      "Epoch 164: val_loss did not improve from 0.38706\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8096 - loss: 0.3705 - val_accuracy: 0.7962 - val_loss: 0.3872 - learning_rate: 0.0025\n",
      "Epoch 165/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8032 - loss: 0.3737  \n",
      "Epoch 165: val_loss did not improve from 0.38706\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8038 - loss: 0.3740 - val_accuracy: 0.7944 - val_loss: 0.3871 - learning_rate: 0.0025\n",
      "Epoch 166/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8040 - loss: 0.3722  \n",
      "Epoch 166: val_loss improved from 0.38706 to 0.38510, saving model to folds4.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8045 - loss: 0.3737 - val_accuracy: 0.7968 - val_loss: 0.3851 - learning_rate: 0.0025\n",
      "Epoch 167/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8040 - loss: 0.3742 \n",
      "Epoch 167: val_loss did not improve from 0.38510\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8036 - loss: 0.3748 - val_accuracy: 0.7956 - val_loss: 0.3869 - learning_rate: 0.0025\n",
      "Epoch 168/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8115 - loss: 0.3723 \n",
      "Epoch 168: val_loss did not improve from 0.38510\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8097 - loss: 0.3730 - val_accuracy: 0.7987 - val_loss: 0.3864 - learning_rate: 0.0025\n",
      "Epoch 169/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8037 - loss: 0.3753 \n",
      "Epoch 169: val_loss improved from 0.38510 to 0.38443, saving model to folds4.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8054 - loss: 0.3746 - val_accuracy: 0.8029 - val_loss: 0.3844 - learning_rate: 0.0025\n",
      "Epoch 170/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8110 - loss: 0.3759 \n",
      "Epoch 170: val_loss did not improve from 0.38443\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8088 - loss: 0.3754 - val_accuracy: 0.7938 - val_loss: 0.3869 - learning_rate: 0.0025\n",
      "Epoch 171/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8071 - loss: 0.3706 \n",
      "Epoch 171: val_loss did not improve from 0.38443\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8065 - loss: 0.3731 - val_accuracy: 0.8005 - val_loss: 0.3854 - learning_rate: 0.0025\n",
      "Epoch 172/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8057 - loss: 0.3789\n",
      "Epoch 172: val_loss did not improve from 0.38443\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.8062 - loss: 0.3777 - val_accuracy: 0.7999 - val_loss: 0.3849 - learning_rate: 0.0025\n",
      "Epoch 173/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8083 - loss: 0.3716\n",
      "Epoch 173: val_loss did not improve from 0.38443\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8082 - loss: 0.3718 - val_accuracy: 0.7938 - val_loss: 0.3875 - learning_rate: 0.0025\n",
      "Epoch 174/300\n",
      "\u001b[1m12/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8059 - loss: 0.3754\n",
      "Epoch 174: val_loss improved from 0.38443 to 0.38388, saving model to folds4.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.8057 - loss: 0.3752 - val_accuracy: 0.8041 - val_loss: 0.3839 - learning_rate: 0.0025\n",
      "Epoch 175/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7952 - loss: 0.3859\n",
      "Epoch 175: val_loss did not improve from 0.38388\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.7979 - loss: 0.3831 - val_accuracy: 0.7999 - val_loss: 0.3855 - learning_rate: 0.0025\n",
      "Epoch 176/300\n",
      "\u001b[1m12/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8082 - loss: 0.3745\n",
      "Epoch 176: val_loss did not improve from 0.38388\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.8078 - loss: 0.3752 - val_accuracy: 0.7950 - val_loss: 0.3864 - learning_rate: 0.0025\n",
      "Epoch 177/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8033 - loss: 0.3836\n",
      "Epoch 177: val_loss did not improve from 0.38388\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.8044 - loss: 0.3813 - val_accuracy: 0.7877 - val_loss: 0.3885 - learning_rate: 0.0025\n",
      "Epoch 178/300\n",
      "\u001b[1m12/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8036 - loss: 0.3739\n",
      "Epoch 178: val_loss did not improve from 0.38388\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8043 - loss: 0.3744 - val_accuracy: 0.7987 - val_loss: 0.3871 - learning_rate: 0.0025\n",
      "Epoch 179/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8123 - loss: 0.3780\n",
      "Epoch 179: val_loss did not improve from 0.38388\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8113 - loss: 0.3774 - val_accuracy: 0.7950 - val_loss: 0.3860 - learning_rate: 0.0025\n",
      "Epoch 180/300\n",
      "\u001b[1m12/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8027 - loss: 0.3781\n",
      "Epoch 180: val_loss did not improve from 0.38388\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.8034 - loss: 0.3773 - val_accuracy: 0.7999 - val_loss: 0.3874 - learning_rate: 0.0025\n",
      "Epoch 181/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8083 - loss: 0.3747\n",
      "Epoch 181: val_loss improved from 0.38388 to 0.38383, saving model to folds4.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.8082 - loss: 0.3743 - val_accuracy: 0.8023 - val_loss: 0.3838 - learning_rate: 0.0025\n",
      "Epoch 182/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8043 - loss: 0.3713  \n",
      "Epoch 182: val_loss did not improve from 0.38383\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8053 - loss: 0.3722 - val_accuracy: 0.7944 - val_loss: 0.3843 - learning_rate: 0.0025\n",
      "Epoch 183/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8076 - loss: 0.3697\n",
      "Epoch 183: val_loss did not improve from 0.38383\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8076 - loss: 0.3699 - val_accuracy: 0.7944 - val_loss: 0.3847 - learning_rate: 0.0025\n",
      "Epoch 184/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8017 - loss: 0.3800 \n",
      "Epoch 184: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
      "\n",
      "Epoch 184: val_loss did not improve from 0.38383\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8042 - loss: 0.3766 - val_accuracy: 0.8017 - val_loss: 0.3846 - learning_rate: 0.0025\n",
      "Epoch 185/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8094 - loss: 0.3760 \n",
      "Epoch 185: val_loss improved from 0.38383 to 0.38360, saving model to folds4.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8083 - loss: 0.3752 - val_accuracy: 0.7987 - val_loss: 0.3836 - learning_rate: 0.0012\n",
      "Epoch 186/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8059 - loss: 0.3742 \n",
      "Epoch 186: val_loss did not improve from 0.38360\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8070 - loss: 0.3739 - val_accuracy: 0.7999 - val_loss: 0.3837 - learning_rate: 0.0012\n",
      "Epoch 187/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8099 - loss: 0.3638 \n",
      "Epoch 187: val_loss improved from 0.38360 to 0.38352, saving model to folds4.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8092 - loss: 0.3669 - val_accuracy: 0.8011 - val_loss: 0.3835 - learning_rate: 0.0012\n",
      "Epoch 188/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8051 - loss: 0.3737 \n",
      "Epoch 188: val_loss did not improve from 0.38352\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8053 - loss: 0.3735 - val_accuracy: 0.8035 - val_loss: 0.3837 - learning_rate: 0.0012\n",
      "Epoch 189/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8084 - loss: 0.3728  \n",
      "Epoch 189: val_loss improved from 0.38352 to 0.38256, saving model to folds4.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8079 - loss: 0.3726 - val_accuracy: 0.7974 - val_loss: 0.3826 - learning_rate: 0.0012\n",
      "Epoch 190/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8048 - loss: 0.3731 \n",
      "Epoch 190: val_loss did not improve from 0.38256\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8059 - loss: 0.3732 - val_accuracy: 0.7987 - val_loss: 0.3827 - learning_rate: 0.0012\n",
      "Epoch 191/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8030 - loss: 0.3754 \n",
      "Epoch 191: val_loss improved from 0.38256 to 0.38251, saving model to folds4.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8051 - loss: 0.3738 - val_accuracy: 0.7956 - val_loss: 0.3825 - learning_rate: 0.0012\n",
      "Epoch 192/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8088 - loss: 0.3700\n",
      "Epoch 192: val_loss did not improve from 0.38251\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8088 - loss: 0.3701 - val_accuracy: 0.8011 - val_loss: 0.3829 - learning_rate: 0.0012\n",
      "Epoch 193/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8068 - loss: 0.3680 \n",
      "Epoch 193: val_loss improved from 0.38251 to 0.38181, saving model to folds4.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8072 - loss: 0.3690 - val_accuracy: 0.8041 - val_loss: 0.3818 - learning_rate: 0.0012\n",
      "Epoch 194/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8069 - loss: 0.3750 \n",
      "Epoch 194: val_loss did not improve from 0.38181\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8085 - loss: 0.3736 - val_accuracy: 0.7993 - val_loss: 0.3834 - learning_rate: 0.0012\n",
      "Epoch 195/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8034 - loss: 0.3770 \n",
      "Epoch 195: val_loss did not improve from 0.38181\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8049 - loss: 0.3746 - val_accuracy: 0.7993 - val_loss: 0.3826 - learning_rate: 0.0012\n",
      "Epoch 196/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8211 - loss: 0.3610 \n",
      "Epoch 196: val_loss did not improve from 0.38181\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8166 - loss: 0.3650 - val_accuracy: 0.8029 - val_loss: 0.3822 - learning_rate: 0.0012\n",
      "Epoch 197/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8115 - loss: 0.3658\n",
      "Epoch 197: val_loss did not improve from 0.38181\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8113 - loss: 0.3661 - val_accuracy: 0.8023 - val_loss: 0.3823 - learning_rate: 0.0012\n",
      "Epoch 198/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8034 - loss: 0.3796 \n",
      "Epoch 198: val_loss did not improve from 0.38181\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8061 - loss: 0.3768 - val_accuracy: 0.7980 - val_loss: 0.3837 - learning_rate: 0.0012\n",
      "Epoch 199/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7983 - loss: 0.3815 \n",
      "Epoch 199: val_loss did not improve from 0.38181\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8014 - loss: 0.3781 - val_accuracy: 0.8017 - val_loss: 0.3826 - learning_rate: 0.0012\n",
      "Epoch 200/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8071 - loss: 0.3725 \n",
      "Epoch 200: val_loss did not improve from 0.38181\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8078 - loss: 0.3717 - val_accuracy: 0.8023 - val_loss: 0.3834 - learning_rate: 0.0012\n",
      "Epoch 201/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8112 - loss: 0.3674 \n",
      "Epoch 201: val_loss did not improve from 0.38181\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8112 - loss: 0.3689 - val_accuracy: 0.8011 - val_loss: 0.3836 - learning_rate: 0.0012\n",
      "Epoch 202/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8091 - loss: 0.3691\n",
      "Epoch 202: val_loss improved from 0.38181 to 0.38169, saving model to folds4.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8091 - loss: 0.3694 - val_accuracy: 0.8005 - val_loss: 0.3817 - learning_rate: 0.0012\n",
      "Epoch 203/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8073 - loss: 0.3676 \n",
      "Epoch 203: val_loss did not improve from 0.38169\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8079 - loss: 0.3697 - val_accuracy: 0.7968 - val_loss: 0.3828 - learning_rate: 0.0012\n",
      "Epoch 204/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8131 - loss: 0.3658 \n",
      "Epoch 204: val_loss did not improve from 0.38169\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8120 - loss: 0.3677 - val_accuracy: 0.7987 - val_loss: 0.3820 - learning_rate: 0.0012\n",
      "Epoch 205/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8047 - loss: 0.3718 \n",
      "Epoch 205: val_loss did not improve from 0.38169\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8064 - loss: 0.3710 - val_accuracy: 0.7993 - val_loss: 0.3823 - learning_rate: 0.0012\n",
      "Epoch 206/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8063 - loss: 0.3749 \n",
      "Epoch 206: val_loss did not improve from 0.38169\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8075 - loss: 0.3730 - val_accuracy: 0.7987 - val_loss: 0.3832 - learning_rate: 0.0012\n",
      "Epoch 207/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8116 - loss: 0.3681\n",
      "Epoch 207: val_loss improved from 0.38169 to 0.38118, saving model to folds4.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8114 - loss: 0.3683 - val_accuracy: 0.7993 - val_loss: 0.3812 - learning_rate: 0.0012\n",
      "Epoch 208/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8092 - loss: 0.3677 \n",
      "Epoch 208: val_loss did not improve from 0.38118\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8080 - loss: 0.3694 - val_accuracy: 0.8023 - val_loss: 0.3817 - learning_rate: 0.0012\n",
      "Epoch 209/300\n",
      "\u001b[1m 7/15\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8045 - loss: 0.3755 \n",
      "Epoch 209: val_loss did not improve from 0.38118\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8072 - loss: 0.3721 - val_accuracy: 0.7993 - val_loss: 0.3831 - learning_rate: 0.0012\n",
      "Epoch 210/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8135 - loss: 0.3636 \n",
      "Epoch 210: val_loss did not improve from 0.38118\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8123 - loss: 0.3662 - val_accuracy: 0.7999 - val_loss: 0.3816 - learning_rate: 0.0012\n",
      "Epoch 211/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8065 - loss: 0.3717 \n",
      "Epoch 211: val_loss did not improve from 0.38118\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8077 - loss: 0.3715 - val_accuracy: 0.7938 - val_loss: 0.3822 - learning_rate: 0.0012\n",
      "Epoch 212/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8057 - loss: 0.3711\n",
      "Epoch 212: val_loss did not improve from 0.38118\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8059 - loss: 0.3711 - val_accuracy: 0.7999 - val_loss: 0.3815 - learning_rate: 0.0012\n",
      "Epoch 213/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8054 - loss: 0.3774 \n",
      "Epoch 213: val_loss improved from 0.38118 to 0.38118, saving model to folds4.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8066 - loss: 0.3746 - val_accuracy: 0.8023 - val_loss: 0.3812 - learning_rate: 0.0012\n",
      "Epoch 214/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8066 - loss: 0.3753  \n",
      "Epoch 214: val_loss did not improve from 0.38118\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8086 - loss: 0.3738 - val_accuracy: 0.8035 - val_loss: 0.3822 - learning_rate: 0.0012\n",
      "Epoch 215/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8083 - loss: 0.3690 \n",
      "Epoch 215: val_loss did not improve from 0.38118\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8091 - loss: 0.3690 - val_accuracy: 0.7999 - val_loss: 0.3812 - learning_rate: 0.0012\n",
      "Epoch 216/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8042 - loss: 0.3793 \n",
      "Epoch 216: val_loss did not improve from 0.38118\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8077 - loss: 0.3751 - val_accuracy: 0.7987 - val_loss: 0.3815 - learning_rate: 0.0012\n",
      "Epoch 217/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8001 - loss: 0.3743 \n",
      "Epoch 217: val_loss improved from 0.38118 to 0.38095, saving model to folds4.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8038 - loss: 0.3733 - val_accuracy: 0.8023 - val_loss: 0.3810 - learning_rate: 0.0012\n",
      "Epoch 218/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8131 - loss: 0.3604 \n",
      "Epoch 218: val_loss did not improve from 0.38095\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8118 - loss: 0.3648 - val_accuracy: 0.8017 - val_loss: 0.3815 - learning_rate: 0.0012\n",
      "Epoch 219/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8116 - loss: 0.3684 \n",
      "Epoch 219: val_loss improved from 0.38095 to 0.38089, saving model to folds4.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8104 - loss: 0.3691 - val_accuracy: 0.8041 - val_loss: 0.3809 - learning_rate: 0.0012\n",
      "Epoch 220/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8147 - loss: 0.3677 \n",
      "Epoch 220: val_loss did not improve from 0.38089\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8131 - loss: 0.3689 - val_accuracy: 0.7987 - val_loss: 0.3820 - learning_rate: 0.0012\n",
      "Epoch 221/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7962 - loss: 0.3860  \n",
      "Epoch 221: val_loss did not improve from 0.38089\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8019 - loss: 0.3800 - val_accuracy: 0.7962 - val_loss: 0.3825 - learning_rate: 0.0012\n",
      "Epoch 222/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7999 - loss: 0.3729 \n",
      "Epoch 222: val_loss did not improve from 0.38089\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8028 - loss: 0.3724 - val_accuracy: 0.8054 - val_loss: 0.3810 - learning_rate: 0.0012\n",
      "Epoch 223/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8077 - loss: 0.3704 \n",
      "Epoch 223: val_loss did not improve from 0.38089\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8086 - loss: 0.3712 - val_accuracy: 0.7987 - val_loss: 0.3812 - learning_rate: 0.0012\n",
      "Epoch 224/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8089 - loss: 0.3702  \n",
      "Epoch 224: val_loss improved from 0.38089 to 0.38034, saving model to folds4.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8089 - loss: 0.3704 - val_accuracy: 0.8023 - val_loss: 0.3803 - learning_rate: 0.0012\n",
      "Epoch 225/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8126 - loss: 0.3642 \n",
      "Epoch 225: val_loss did not improve from 0.38034\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8114 - loss: 0.3663 - val_accuracy: 0.7999 - val_loss: 0.3812 - learning_rate: 0.0012\n",
      "Epoch 226/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8063 - loss: 0.3723 \n",
      "Epoch 226: val_loss did not improve from 0.38034\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8067 - loss: 0.3713 - val_accuracy: 0.7987 - val_loss: 0.3818 - learning_rate: 0.0012\n",
      "Epoch 227/300\n",
      "\u001b[1m12/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8160 - loss: 0.3673\n",
      "Epoch 227: val_loss did not improve from 0.38034\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.8152 - loss: 0.3679 - val_accuracy: 0.7950 - val_loss: 0.3817 - learning_rate: 0.0012\n",
      "Epoch 228/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8044 - loss: 0.3693\n",
      "Epoch 228: val_loss did not improve from 0.38034\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.8056 - loss: 0.3697 - val_accuracy: 0.8017 - val_loss: 0.3817 - learning_rate: 0.0012\n",
      "Epoch 229/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8079 - loss: 0.3673\n",
      "Epoch 229: val_loss did not improve from 0.38034\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.8080 - loss: 0.3685 - val_accuracy: 0.8041 - val_loss: 0.3827 - learning_rate: 0.0012\n",
      "Epoch 230/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8089 - loss: 0.3664\n",
      "Epoch 230: val_loss did not improve from 0.38034\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.8082 - loss: 0.3677 - val_accuracy: 0.8041 - val_loss: 0.3812 - learning_rate: 0.0012\n",
      "Epoch 231/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8143 - loss: 0.3636\n",
      "Epoch 231: val_loss did not improve from 0.38034\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.8122 - loss: 0.3656 - val_accuracy: 0.7968 - val_loss: 0.3829 - learning_rate: 0.0012\n",
      "Epoch 232/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8113 - loss: 0.3773\n",
      "Epoch 232: val_loss did not improve from 0.38034\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.8113 - loss: 0.3770 - val_accuracy: 0.7944 - val_loss: 0.3819 - learning_rate: 0.0012\n",
      "Epoch 233/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8073 - loss: 0.3704\n",
      "Epoch 233: val_loss did not improve from 0.38034\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.8074 - loss: 0.3704 - val_accuracy: 0.7974 - val_loss: 0.3804 - learning_rate: 0.0012\n",
      "Epoch 234/300\n",
      "\u001b[1m12/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8108 - loss: 0.3706\n",
      "Epoch 234: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "\n",
      "Epoch 234: val_loss did not improve from 0.38034\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.8104 - loss: 0.3703 - val_accuracy: 0.7980 - val_loss: 0.3812 - learning_rate: 0.0012\n",
      "Epoch 235/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8158 - loss: 0.3636\n",
      "Epoch 235: val_loss improved from 0.38034 to 0.38000, saving model to folds4.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.8140 - loss: 0.3651 - val_accuracy: 0.8054 - val_loss: 0.3800 - learning_rate: 6.2500e-04\n",
      "Epoch 236/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8173 - loss: 0.3624\n",
      "Epoch 236: val_loss improved from 0.38000 to 0.37940, saving model to folds4.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.8153 - loss: 0.3642 - val_accuracy: 0.8011 - val_loss: 0.3794 - learning_rate: 6.2500e-04\n",
      "Epoch 237/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8141 - loss: 0.3670  \n",
      "Epoch 237: val_loss did not improve from 0.37940\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8124 - loss: 0.3674 - val_accuracy: 0.8054 - val_loss: 0.3797 - learning_rate: 6.2500e-04\n",
      "Epoch 238/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8115 - loss: 0.3696 \n",
      "Epoch 238: val_loss did not improve from 0.37940\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8100 - loss: 0.3699 - val_accuracy: 0.7999 - val_loss: 0.3797 - learning_rate: 6.2500e-04\n",
      "Epoch 239/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8180 - loss: 0.3622 \n",
      "Epoch 239: val_loss did not improve from 0.37940\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8145 - loss: 0.3654 - val_accuracy: 0.8066 - val_loss: 0.3794 - learning_rate: 6.2500e-04\n",
      "Epoch 240/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8094 - loss: 0.3717\n",
      "Epoch 240: val_loss did not improve from 0.37940\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8094 - loss: 0.3715 - val_accuracy: 0.7987 - val_loss: 0.3797 - learning_rate: 6.2500e-04\n",
      "Epoch 241/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8050 - loss: 0.3731\n",
      "Epoch 241: val_loss did not improve from 0.37940\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8053 - loss: 0.3728 - val_accuracy: 0.8054 - val_loss: 0.3801 - learning_rate: 6.2500e-04\n",
      "Epoch 242/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8042 - loss: 0.3747 \n",
      "Epoch 242: val_loss improved from 0.37940 to 0.37936, saving model to folds4.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8067 - loss: 0.3720 - val_accuracy: 0.8041 - val_loss: 0.3794 - learning_rate: 6.2500e-04\n",
      "Epoch 243/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8141 - loss: 0.3618 \n",
      "Epoch 243: val_loss did not improve from 0.37936\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8125 - loss: 0.3644 - val_accuracy: 0.8041 - val_loss: 0.3795 - learning_rate: 6.2500e-04\n",
      "Epoch 244/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8072 - loss: 0.3719 \n",
      "Epoch 244: val_loss did not improve from 0.37936\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8076 - loss: 0.3709 - val_accuracy: 0.7999 - val_loss: 0.3799 - learning_rate: 6.2500e-04\n",
      "Epoch 245/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8138 - loss: 0.3596 \n",
      "Epoch 245: val_loss did not improve from 0.37936\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8121 - loss: 0.3638 - val_accuracy: 0.8023 - val_loss: 0.3799 - learning_rate: 6.2500e-04\n",
      "Epoch 246/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8099 - loss: 0.3603 \n",
      "Epoch 246: ReduceLROnPlateau reducing learning rate to 0.0003124999930150807.\n",
      "\n",
      "Epoch 246: val_loss did not improve from 0.37936\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8092 - loss: 0.3640 - val_accuracy: 0.8035 - val_loss: 0.3796 - learning_rate: 6.2500e-04\n",
      "Epoch 247/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8084 - loss: 0.3702\n",
      "Epoch 247: val_loss did not improve from 0.37936\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8085 - loss: 0.3701 - val_accuracy: 0.8035 - val_loss: 0.3795 - learning_rate: 3.1250e-04\n",
      "Epoch 248/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8136 - loss: 0.3652\n",
      "Epoch 248: val_loss did not improve from 0.37936\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8132 - loss: 0.3656 - val_accuracy: 0.8023 - val_loss: 0.3795 - learning_rate: 3.1250e-04\n",
      "Epoch 249/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8193 - loss: 0.3635 \n",
      "Epoch 249: val_loss did not improve from 0.37936\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8146 - loss: 0.3657 - val_accuracy: 0.7993 - val_loss: 0.3795 - learning_rate: 3.1250e-04\n",
      "Epoch 250/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8038 - loss: 0.3749 \n",
      "Epoch 250: val_loss improved from 0.37936 to 0.37928, saving model to folds4.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8058 - loss: 0.3731 - val_accuracy: 0.8011 - val_loss: 0.3793 - learning_rate: 3.1250e-04\n",
      "Epoch 251/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8127 - loss: 0.3675 \n",
      "Epoch 251: val_loss did not improve from 0.37928\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8107 - loss: 0.3683 - val_accuracy: 0.8023 - val_loss: 0.3794 - learning_rate: 3.1250e-04\n",
      "Epoch 252/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8053 - loss: 0.3691 \n",
      "Epoch 252: val_loss improved from 0.37928 to 0.37920, saving model to folds4.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8081 - loss: 0.3677 - val_accuracy: 0.8017 - val_loss: 0.3792 - learning_rate: 3.1250e-04\n",
      "Epoch 253/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8093 - loss: 0.3668 \n",
      "Epoch 253: val_loss did not improve from 0.37920\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8095 - loss: 0.3676 - val_accuracy: 0.7999 - val_loss: 0.3795 - learning_rate: 3.1250e-04\n",
      "Epoch 254/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8128 - loss: 0.3606 \n",
      "Epoch 254: val_loss did not improve from 0.37920\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8107 - loss: 0.3647 - val_accuracy: 0.8048 - val_loss: 0.3794 - learning_rate: 3.1250e-04\n",
      "Epoch 255/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7991 - loss: 0.3722 \n",
      "Epoch 255: val_loss did not improve from 0.37920\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8043 - loss: 0.3696 - val_accuracy: 0.8023 - val_loss: 0.3795 - learning_rate: 3.1250e-04\n",
      "Epoch 256/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8087 - loss: 0.3737 \n",
      "Epoch 256: val_loss improved from 0.37920 to 0.37905, saving model to folds4.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8079 - loss: 0.3720 - val_accuracy: 0.8017 - val_loss: 0.3791 - learning_rate: 3.1250e-04\n",
      "Epoch 257/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8075 - loss: 0.3686 \n",
      "Epoch 257: val_loss did not improve from 0.37905\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8094 - loss: 0.3679 - val_accuracy: 0.8017 - val_loss: 0.3793 - learning_rate: 3.1250e-04\n",
      "Epoch 258/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8128 - loss: 0.3620  \n",
      "Epoch 258: val_loss did not improve from 0.37905\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8117 - loss: 0.3639 - val_accuracy: 0.8041 - val_loss: 0.3792 - learning_rate: 3.1250e-04\n",
      "Epoch 259/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8138 - loss: 0.3610 \n",
      "Epoch 259: val_loss did not improve from 0.37905\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8111 - loss: 0.3651 - val_accuracy: 0.8023 - val_loss: 0.3793 - learning_rate: 3.1250e-04\n",
      "Epoch 260/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8047 - loss: 0.3787 \n",
      "Epoch 260: val_loss did not improve from 0.37905\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8069 - loss: 0.3746 - val_accuracy: 0.8048 - val_loss: 0.3791 - learning_rate: 3.1250e-04\n",
      "Epoch 261/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8073 - loss: 0.3720 \n",
      "Epoch 261: val_loss did not improve from 0.37905\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8080 - loss: 0.3702 - val_accuracy: 0.8023 - val_loss: 0.3792 - learning_rate: 3.1250e-04\n",
      "Epoch 262/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8141 - loss: 0.3680 \n",
      "Epoch 262: val_loss did not improve from 0.37905\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8124 - loss: 0.3677 - val_accuracy: 0.8011 - val_loss: 0.3792 - learning_rate: 3.1250e-04\n",
      "Epoch 263/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8079 - loss: 0.3678 \n",
      "Epoch 263: val_loss did not improve from 0.37905\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8089 - loss: 0.3677 - val_accuracy: 0.7980 - val_loss: 0.3793 - learning_rate: 3.1250e-04\n",
      "Epoch 264/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8091 - loss: 0.3689 \n",
      "Epoch 264: val_loss improved from 0.37905 to 0.37904, saving model to folds4.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8092 - loss: 0.3681 - val_accuracy: 0.8054 - val_loss: 0.3790 - learning_rate: 3.1250e-04\n",
      "Epoch 265/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8084 - loss: 0.3646 \n",
      "Epoch 265: val_loss did not improve from 0.37904\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8090 - loss: 0.3663 - val_accuracy: 0.8029 - val_loss: 0.3793 - learning_rate: 3.1250e-04\n",
      "Epoch 266/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8097 - loss: 0.3611 \n",
      "Epoch 266: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "\n",
      "Epoch 266: val_loss did not improve from 0.37904\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8102 - loss: 0.3639 - val_accuracy: 0.7993 - val_loss: 0.3792 - learning_rate: 3.1250e-04\n",
      "Epoch 267/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8094 - loss: 0.3696 \n",
      "Epoch 267: val_loss did not improve from 0.37904\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8099 - loss: 0.3677 - val_accuracy: 0.8029 - val_loss: 0.3791 - learning_rate: 1.5625e-04\n",
      "Epoch 268/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8127 - loss: 0.3639 \n",
      "Epoch 268: val_loss improved from 0.37904 to 0.37894, saving model to folds4.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8114 - loss: 0.3656 - val_accuracy: 0.8060 - val_loss: 0.3789 - learning_rate: 1.5625e-04\n",
      "Epoch 269/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8235 - loss: 0.3596 \n",
      "Epoch 269: val_loss did not improve from 0.37894\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8184 - loss: 0.3627 - val_accuracy: 0.8048 - val_loss: 0.3791 - learning_rate: 1.5625e-04\n",
      "Epoch 270/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8235 - loss: 0.3506  \n",
      "Epoch 270: val_loss did not improve from 0.37894\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8173 - loss: 0.3587 - val_accuracy: 0.8011 - val_loss: 0.3791 - learning_rate: 1.5625e-04\n",
      "Epoch 271/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8134 - loss: 0.3655 \n",
      "Epoch 271: val_loss did not improve from 0.37894\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8118 - loss: 0.3672 - val_accuracy: 0.8023 - val_loss: 0.3791 - learning_rate: 1.5625e-04\n",
      "Epoch 272/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8111 - loss: 0.3642 \n",
      "Epoch 272: val_loss did not improve from 0.37894\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8102 - loss: 0.3659 - val_accuracy: 0.8017 - val_loss: 0.3791 - learning_rate: 1.5625e-04\n",
      "Epoch 273/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8126 - loss: 0.3670 \n",
      "Epoch 273: val_loss improved from 0.37894 to 0.37891, saving model to folds4.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8109 - loss: 0.3678 - val_accuracy: 0.8048 - val_loss: 0.3789 - learning_rate: 1.5625e-04\n",
      "Epoch 274/300\n",
      "\u001b[1m12/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8120 - loss: 0.3652 \n",
      "Epoch 274: val_loss did not improve from 0.37891\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8117 - loss: 0.3657 - val_accuracy: 0.8011 - val_loss: 0.3790 - learning_rate: 1.5625e-04\n",
      "Epoch 275/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8153 - loss: 0.3629  \n",
      "Epoch 275: val_loss did not improve from 0.37891\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8138 - loss: 0.3641 - val_accuracy: 0.8023 - val_loss: 0.3793 - learning_rate: 1.5625e-04\n",
      "Epoch 276/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8138 - loss: 0.3656 \n",
      "Epoch 276: val_loss did not improve from 0.37891\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8123 - loss: 0.3665 - val_accuracy: 0.8029 - val_loss: 0.3790 - learning_rate: 1.5625e-04\n",
      "Epoch 277/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8002 - loss: 0.3784 \n",
      "Epoch 277: val_loss did not improve from 0.37891\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8045 - loss: 0.3743 - val_accuracy: 0.8005 - val_loss: 0.3790 - learning_rate: 1.5625e-04\n",
      "Epoch 278/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8101 - loss: 0.3721 \n",
      "Epoch 278: ReduceLROnPlateau reducing learning rate to 7.812499825377017e-05.\n",
      "\n",
      "Epoch 278: val_loss did not improve from 0.37891\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8095 - loss: 0.3712 - val_accuracy: 0.8029 - val_loss: 0.3790 - learning_rate: 1.5625e-04\n",
      "Epoch 279/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8112 - loss: 0.3674\n",
      "Epoch 279: val_loss did not improve from 0.37891\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8111 - loss: 0.3674 - val_accuracy: 0.8011 - val_loss: 0.3791 - learning_rate: 7.8125e-05\n",
      "Epoch 280/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8094 - loss: 0.3725 \n",
      "Epoch 280: val_loss did not improve from 0.37891\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8100 - loss: 0.3701 - val_accuracy: 0.7993 - val_loss: 0.3791 - learning_rate: 7.8125e-05\n",
      "Epoch 281/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8027 - loss: 0.3724 \n",
      "Epoch 281: val_loss did not improve from 0.37891\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8063 - loss: 0.3698 - val_accuracy: 0.8011 - val_loss: 0.3790 - learning_rate: 7.8125e-05\n",
      "Epoch 282/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8104 - loss: 0.3638\n",
      "Epoch 282: val_loss did not improve from 0.37891\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.8104 - loss: 0.3651 - val_accuracy: 0.8029 - val_loss: 0.3790 - learning_rate: 7.8125e-05\n",
      "Epoch 283/300\n",
      "\u001b[1m12/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8044 - loss: 0.3745\n",
      "Epoch 283: val_loss did not improve from 0.37891\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.8057 - loss: 0.3729 - val_accuracy: 0.8017 - val_loss: 0.3790 - learning_rate: 7.8125e-05\n",
      "Epoch 284/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8150 - loss: 0.3651\n",
      "Epoch 284: val_loss did not improve from 0.37891\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.8144 - loss: 0.3654 - val_accuracy: 0.8029 - val_loss: 0.3790 - learning_rate: 7.8125e-05\n",
      "Epoch 285/300\n",
      "\u001b[1m12/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8084 - loss: 0.3691\n",
      "Epoch 285: val_loss did not improve from 0.37891\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8088 - loss: 0.3687 - val_accuracy: 0.8023 - val_loss: 0.3789 - learning_rate: 7.8125e-05\n",
      "Epoch 286/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8111 - loss: 0.3678\n",
      "Epoch 286: val_loss did not improve from 0.37891\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.8108 - loss: 0.3676 - val_accuracy: 0.8017 - val_loss: 0.3790 - learning_rate: 7.8125e-05\n",
      "Epoch 287/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8131 - loss: 0.3620\n",
      "Epoch 287: val_loss did not improve from 0.37891\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.8129 - loss: 0.3624 - val_accuracy: 0.8011 - val_loss: 0.3790 - learning_rate: 7.8125e-05\n",
      "Epoch 288/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8127 - loss: 0.3681\n",
      "Epoch 288: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "\n",
      "Epoch 288: val_loss did not improve from 0.37891\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.8121 - loss: 0.3677 - val_accuracy: 0.8035 - val_loss: 0.3789 - learning_rate: 7.8125e-05\n",
      "Epoch 289/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8105 - loss: 0.3669\n",
      "Epoch 289: val_loss did not improve from 0.37891\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.8100 - loss: 0.3673 - val_accuracy: 0.8041 - val_loss: 0.3789 - learning_rate: 3.9062e-05\n",
      "Epoch 290/300\n",
      "\u001b[1m12/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8091 - loss: 0.3686\n",
      "Epoch 290: val_loss did not improve from 0.37891\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.8093 - loss: 0.3683 - val_accuracy: 0.8035 - val_loss: 0.3789 - learning_rate: 3.9062e-05\n",
      "Epoch 291/300\n",
      "\u001b[1m12/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8084 - loss: 0.3677\n",
      "Epoch 291: val_loss improved from 0.37891 to 0.37886, saving model to folds4.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.8088 - loss: 0.3675 - val_accuracy: 0.8023 - val_loss: 0.3789 - learning_rate: 3.9062e-05\n",
      "Epoch 292/300\n",
      "\u001b[1m12/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8118 - loss: 0.3638\n",
      "Epoch 292: val_loss did not improve from 0.37886\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.8116 - loss: 0.3645 - val_accuracy: 0.8023 - val_loss: 0.3789 - learning_rate: 3.9062e-05\n",
      "Epoch 293/300\n",
      "\u001b[1m13/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8045 - loss: 0.3725\n",
      "Epoch 293: val_loss did not improve from 0.37886\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8056 - loss: 0.3715 - val_accuracy: 0.8023 - val_loss: 0.3789 - learning_rate: 3.9062e-05\n",
      "Epoch 294/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8164 - loss: 0.3587 \n",
      "Epoch 294: val_loss did not improve from 0.37886\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8141 - loss: 0.3618 - val_accuracy: 0.8029 - val_loss: 0.3789 - learning_rate: 3.9062e-05\n",
      "Epoch 295/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8118 - loss: 0.3665\n",
      "Epoch 295: val_loss did not improve from 0.37886\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8117 - loss: 0.3665 - val_accuracy: 0.8029 - val_loss: 0.3789 - learning_rate: 3.9062e-05\n",
      "Epoch 296/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8125 - loss: 0.3664  \n",
      "Epoch 296: val_loss did not improve from 0.37886\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8119 - loss: 0.3667 - val_accuracy: 0.8041 - val_loss: 0.3789 - learning_rate: 3.9062e-05\n",
      "Epoch 297/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8087 - loss: 0.3637 \n",
      "Epoch 297: val_loss did not improve from 0.37886\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8096 - loss: 0.3654 - val_accuracy: 0.8005 - val_loss: 0.3789 - learning_rate: 3.9062e-05\n",
      "Epoch 298/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8074 - loss: 0.3733 \n",
      "Epoch 298: ReduceLROnPlateau reducing learning rate to 1.9531249563442543e-05.\n",
      "\n",
      "Epoch 298: val_loss did not improve from 0.37886\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8095 - loss: 0.3703 - val_accuracy: 0.8011 - val_loss: 0.3789 - learning_rate: 3.9062e-05\n",
      "Epoch 299/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8121 - loss: 0.3629\n",
      "Epoch 299: val_loss did not improve from 0.37886\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8120 - loss: 0.3631 - val_accuracy: 0.8023 - val_loss: 0.3789 - learning_rate: 1.9531e-05\n",
      "Epoch 300/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8091 - loss: 0.3695  \n",
      "Epoch 300: val_loss did not improve from 0.37886\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8095 - loss: 0.3684 - val_accuracy: 0.8023 - val_loss: 0.3789 - learning_rate: 1.9531e-05\n",
      "Restoring model weights from the end of the best epoch: 291.\n",
      " Computing LSTM feature importance...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:01<00:00,  5.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- > Fold 6 < ---------------\n",
      "Epoch 1/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6145 - loss: 0.6440\n",
      "Epoch 1: val_loss improved from inf to 0.53386, saving model to folds5.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - accuracy: 0.6476 - loss: 0.6149 - val_accuracy: 0.7108 - val_loss: 0.5339 - learning_rate: 0.0100\n",
      "Epoch 2/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7192 - loss: 0.5169 \n",
      "Epoch 2: val_loss improved from 0.53386 to 0.52147, saving model to folds5.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7177 - loss: 0.5160 - val_accuracy: 0.7023 - val_loss: 0.5215 - learning_rate: 0.0100\n",
      "Epoch 3/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7004 - loss: 0.5045 \n",
      "Epoch 3: val_loss improved from 0.52147 to 0.50790, saving model to folds5.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7033 - loss: 0.5021 - val_accuracy: 0.7120 - val_loss: 0.5079 - learning_rate: 0.0100\n",
      "Epoch 4/300\n",
      "\u001b[1m 7/15\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7237 - loss: 0.4862 \n",
      "Epoch 4: val_loss improved from 0.50790 to 0.50366, saving model to folds5.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7227 - loss: 0.4856 - val_accuracy: 0.7206 - val_loss: 0.5037 - learning_rate: 0.0100\n",
      "Epoch 5/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7191 - loss: 0.4806 \n",
      "Epoch 5: val_loss improved from 0.50366 to 0.48740, saving model to folds5.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7217 - loss: 0.4795 - val_accuracy: 0.7328 - val_loss: 0.4874 - learning_rate: 0.0100\n",
      "Epoch 6/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7205 - loss: 0.4745 \n",
      "Epoch 6: val_loss did not improve from 0.48740\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7246 - loss: 0.4727 - val_accuracy: 0.7334 - val_loss: 0.4879 - learning_rate: 0.0100\n",
      "Epoch 7/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7307 - loss: 0.4699 \n",
      "Epoch 7: val_loss improved from 0.48740 to 0.48405, saving model to folds5.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7319 - loss: 0.4692 - val_accuracy: 0.7285 - val_loss: 0.4840 - learning_rate: 0.0100\n",
      "Epoch 8/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7331 - loss: 0.4642 \n",
      "Epoch 8: val_loss improved from 0.48405 to 0.47222, saving model to folds5.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7325 - loss: 0.4632 - val_accuracy: 0.7462 - val_loss: 0.4722 - learning_rate: 0.0100\n",
      "Epoch 9/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7400 - loss: 0.4582 \n",
      "Epoch 9: val_loss did not improve from 0.47222\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7394 - loss: 0.4583 - val_accuracy: 0.7352 - val_loss: 0.4822 - learning_rate: 0.0100\n",
      "Epoch 10/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7442 - loss: 0.4514 \n",
      "Epoch 10: val_loss did not improve from 0.47222\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7432 - loss: 0.4532 - val_accuracy: 0.7364 - val_loss: 0.4771 - learning_rate: 0.0100\n",
      "Epoch 11/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7311 - loss: 0.4539 \n",
      "Epoch 11: val_loss did not improve from 0.47222\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7323 - loss: 0.4557 - val_accuracy: 0.7322 - val_loss: 0.4764 - learning_rate: 0.0100\n",
      "Epoch 12/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7455 - loss: 0.4489 \n",
      "Epoch 12: val_loss improved from 0.47222 to 0.46657, saving model to folds5.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7438 - loss: 0.4506 - val_accuracy: 0.7267 - val_loss: 0.4666 - learning_rate: 0.0100\n",
      "Epoch 13/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7404 - loss: 0.4505 \n",
      "Epoch 13: val_loss improved from 0.46657 to 0.46267, saving model to folds5.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7430 - loss: 0.4499 - val_accuracy: 0.7505 - val_loss: 0.4627 - learning_rate: 0.0100\n",
      "Epoch 14/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7537 - loss: 0.4484 \n",
      "Epoch 14: val_loss improved from 0.46267 to 0.45808, saving model to folds5.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7512 - loss: 0.4492 - val_accuracy: 0.7450 - val_loss: 0.4581 - learning_rate: 0.0100\n",
      "Epoch 15/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7586 - loss: 0.4403 \n",
      "Epoch 15: val_loss improved from 0.45808 to 0.45317, saving model to folds5.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7563 - loss: 0.4425 - val_accuracy: 0.7468 - val_loss: 0.4532 - learning_rate: 0.0100\n",
      "Epoch 16/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7603 - loss: 0.4329  \n",
      "Epoch 16: val_loss did not improve from 0.45317\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7593 - loss: 0.4354 - val_accuracy: 0.7584 - val_loss: 0.4536 - learning_rate: 0.0100\n",
      "Epoch 17/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7598 - loss: 0.4390 \n",
      "Epoch 17: val_loss did not improve from 0.45317\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7582 - loss: 0.4381 - val_accuracy: 0.7346 - val_loss: 0.4606 - learning_rate: 0.0100\n",
      "Epoch 18/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7479 - loss: 0.4388 \n",
      "Epoch 18: val_loss improved from 0.45317 to 0.44885, saving model to folds5.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7496 - loss: 0.4387 - val_accuracy: 0.7578 - val_loss: 0.4488 - learning_rate: 0.0100\n",
      "Epoch 19/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7578 - loss: 0.4339 \n",
      "Epoch 19: val_loss did not improve from 0.44885\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7583 - loss: 0.4341 - val_accuracy: 0.7389 - val_loss: 0.4529 - learning_rate: 0.0100\n",
      "Epoch 20/300\n",
      "\u001b[1m12/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7647 - loss: 0.4287\n",
      "Epoch 20: val_loss improved from 0.44885 to 0.44022, saving model to folds5.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.7641 - loss: 0.4295 - val_accuracy: 0.7663 - val_loss: 0.4402 - learning_rate: 0.0100\n",
      "Epoch 21/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7745 - loss: 0.4176\n",
      "Epoch 21: val_loss improved from 0.44022 to 0.43620, saving model to folds5.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.7713 - loss: 0.4203 - val_accuracy: 0.7724 - val_loss: 0.4362 - learning_rate: 0.0100\n",
      "Epoch 22/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7715 - loss: 0.4268\n",
      "Epoch 22: val_loss did not improve from 0.43620\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.7715 - loss: 0.4263 - val_accuracy: 0.7346 - val_loss: 0.4612 - learning_rate: 0.0100\n",
      "Epoch 23/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7593 - loss: 0.4290 \n",
      "Epoch 23: val_loss did not improve from 0.43620\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7641 - loss: 0.4274 - val_accuracy: 0.7657 - val_loss: 0.4367 - learning_rate: 0.0100\n",
      "Epoch 24/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7690 - loss: 0.4241\n",
      "Epoch 24: val_loss did not improve from 0.43620\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.7692 - loss: 0.4241 - val_accuracy: 0.7547 - val_loss: 0.4424 - learning_rate: 0.0100\n",
      "Epoch 25/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7597 - loss: 0.4288\n",
      "Epoch 25: val_loss improved from 0.43620 to 0.43186, saving model to folds5.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.7613 - loss: 0.4270 - val_accuracy: 0.7541 - val_loss: 0.4319 - learning_rate: 0.0100\n",
      "Epoch 26/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7662 - loss: 0.4202\n",
      "Epoch 26: val_loss did not improve from 0.43186\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.7672 - loss: 0.4199 - val_accuracy: 0.7529 - val_loss: 0.4396 - learning_rate: 0.0100\n",
      "Epoch 27/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7657 - loss: 0.4190\n",
      "Epoch 27: val_loss improved from 0.43186 to 0.42753, saving model to folds5.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.7687 - loss: 0.4180 - val_accuracy: 0.7718 - val_loss: 0.4275 - learning_rate: 0.0100\n",
      "Epoch 28/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7736 - loss: 0.4138  \n",
      "Epoch 28: val_loss improved from 0.42753 to 0.42281, saving model to folds5.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.7741 - loss: 0.4150 - val_accuracy: 0.7779 - val_loss: 0.4228 - learning_rate: 0.0100\n",
      "Epoch 29/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7826 - loss: 0.4029 \n",
      "Epoch 29: val_loss did not improve from 0.42281\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7809 - loss: 0.4070 - val_accuracy: 0.7645 - val_loss: 0.4317 - learning_rate: 0.0100\n",
      "Epoch 30/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7781 - loss: 0.4082 \n",
      "Epoch 30: val_loss did not improve from 0.42281\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7778 - loss: 0.4086 - val_accuracy: 0.7419 - val_loss: 0.4546 - learning_rate: 0.0100\n",
      "Epoch 31/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7799 - loss: 0.4155 \n",
      "Epoch 31: val_loss improved from 0.42281 to 0.42233, saving model to folds5.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7814 - loss: 0.4128 - val_accuracy: 0.7767 - val_loss: 0.4223 - learning_rate: 0.0100\n",
      "Epoch 32/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7858 - loss: 0.4009 \n",
      "Epoch 32: val_loss did not improve from 0.42233\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7849 - loss: 0.4016 - val_accuracy: 0.7584 - val_loss: 0.4237 - learning_rate: 0.0100\n",
      "Epoch 33/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7843 - loss: 0.3987 \n",
      "Epoch 33: val_loss improved from 0.42233 to 0.42056, saving model to folds5.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7869 - loss: 0.3991 - val_accuracy: 0.7743 - val_loss: 0.4206 - learning_rate: 0.0100\n",
      "Epoch 34/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7872 - loss: 0.3997 \n",
      "Epoch 34: val_loss did not improve from 0.42056\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7876 - loss: 0.3992 - val_accuracy: 0.7633 - val_loss: 0.4284 - learning_rate: 0.0100\n",
      "Epoch 35/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7888 - loss: 0.3950\n",
      "Epoch 35: val_loss improved from 0.42056 to 0.41624, saving model to folds5.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7889 - loss: 0.3950 - val_accuracy: 0.7437 - val_loss: 0.4162 - learning_rate: 0.0100\n",
      "Epoch 36/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7710 - loss: 0.4041 \n",
      "Epoch 36: val_loss improved from 0.41624 to 0.41004, saving model to folds5.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7752 - loss: 0.4021 - val_accuracy: 0.7810 - val_loss: 0.4100 - learning_rate: 0.0100\n",
      "Epoch 37/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7761 - loss: 0.4112 \n",
      "Epoch 37: val_loss did not improve from 0.41004\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7784 - loss: 0.4094 - val_accuracy: 0.7712 - val_loss: 0.4268 - learning_rate: 0.0100\n",
      "Epoch 38/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7766 - loss: 0.4051 \n",
      "Epoch 38: val_loss did not improve from 0.41004\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7779 - loss: 0.4047 - val_accuracy: 0.7682 - val_loss: 0.4247 - learning_rate: 0.0100\n",
      "Epoch 39/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7874 - loss: 0.3986 \n",
      "Epoch 39: val_loss did not improve from 0.41004\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7897 - loss: 0.3960 - val_accuracy: 0.7718 - val_loss: 0.4329 - learning_rate: 0.0100\n",
      "Epoch 40/300\n",
      "\u001b[1m 7/15\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7714 - loss: 0.4149 \n",
      "Epoch 40: val_loss did not improve from 0.41004\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7749 - loss: 0.4121 - val_accuracy: 0.7596 - val_loss: 0.4338 - learning_rate: 0.0100\n",
      "Epoch 41/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7771 - loss: 0.4086 \n",
      "Epoch 41: val_loss improved from 0.41004 to 0.41003, saving model to folds5.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7807 - loss: 0.4028 - val_accuracy: 0.7858 - val_loss: 0.4100 - learning_rate: 0.0100\n",
      "Epoch 42/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7944 - loss: 0.3921 \n",
      "Epoch 42: val_loss did not improve from 0.41003\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7916 - loss: 0.3936 - val_accuracy: 0.7639 - val_loss: 0.4232 - learning_rate: 0.0100\n",
      "Epoch 43/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7804 - loss: 0.4052 \n",
      "Epoch 43: val_loss did not improve from 0.41003\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7824 - loss: 0.4033 - val_accuracy: 0.7682 - val_loss: 0.4364 - learning_rate: 0.0100\n",
      "Epoch 44/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7694 - loss: 0.4142 \n",
      "Epoch 44: val_loss did not improve from 0.41003\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7783 - loss: 0.4081 - val_accuracy: 0.7797 - val_loss: 0.4189 - learning_rate: 0.0100\n",
      "Epoch 45/300\n",
      "\u001b[1m 7/15\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7939 - loss: 0.3941 \n",
      "Epoch 45: val_loss improved from 0.41003 to 0.40913, saving model to folds5.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7913 - loss: 0.3936 - val_accuracy: 0.7755 - val_loss: 0.4091 - learning_rate: 0.0100\n",
      "Epoch 46/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7807 - loss: 0.3906 \n",
      "Epoch 46: val_loss improved from 0.40913 to 0.40605, saving model to folds5.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7845 - loss: 0.3902 - val_accuracy: 0.7907 - val_loss: 0.4061 - learning_rate: 0.0100\n",
      "Epoch 47/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7972 - loss: 0.3868 \n",
      "Epoch 47: val_loss did not improve from 0.40605\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7935 - loss: 0.3873 - val_accuracy: 0.7633 - val_loss: 0.4215 - learning_rate: 0.0100\n",
      "Epoch 48/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7855 - loss: 0.3992 \n",
      "Epoch 48: val_loss improved from 0.40605 to 0.39594, saving model to folds5.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7848 - loss: 0.3983 - val_accuracy: 0.7974 - val_loss: 0.3959 - learning_rate: 0.0100\n",
      "Epoch 49/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7996 - loss: 0.3885 \n",
      "Epoch 49: val_loss did not improve from 0.39594\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7975 - loss: 0.3872 - val_accuracy: 0.7895 - val_loss: 0.3992 - learning_rate: 0.0100\n",
      "Epoch 50/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7993 - loss: 0.3766 \n",
      "Epoch 50: val_loss improved from 0.39594 to 0.39371, saving model to folds5.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7993 - loss: 0.3768 - val_accuracy: 0.7968 - val_loss: 0.3937 - learning_rate: 0.0100\n",
      "Epoch 51/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8034 - loss: 0.3806 \n",
      "Epoch 51: val_loss improved from 0.39371 to 0.38840, saving model to folds5.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8017 - loss: 0.3815 - val_accuracy: 0.7938 - val_loss: 0.3884 - learning_rate: 0.0100\n",
      "Epoch 52/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8035 - loss: 0.3767 \n",
      "Epoch 52: val_loss did not improve from 0.38840\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8024 - loss: 0.3774 - val_accuracy: 0.7913 - val_loss: 0.4032 - learning_rate: 0.0100\n",
      "Epoch 53/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8093 - loss: 0.3698 \n",
      "Epoch 53: val_loss did not improve from 0.38840\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8079 - loss: 0.3719 - val_accuracy: 0.7871 - val_loss: 0.4011 - learning_rate: 0.0100\n",
      "Epoch 54/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8007 - loss: 0.3748\n",
      "Epoch 54: val_loss improved from 0.38840 to 0.38657, saving model to folds5.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8008 - loss: 0.3747 - val_accuracy: 0.8103 - val_loss: 0.3866 - learning_rate: 0.0100\n",
      "Epoch 55/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7970 - loss: 0.3798 \n",
      "Epoch 55: val_loss did not improve from 0.38657\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7982 - loss: 0.3793 - val_accuracy: 0.7895 - val_loss: 0.3993 - learning_rate: 0.0100\n",
      "Epoch 56/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8018 - loss: 0.3718 \n",
      "Epoch 56: val_loss did not improve from 0.38657\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8017 - loss: 0.3732 - val_accuracy: 0.7993 - val_loss: 0.3950 - learning_rate: 0.0100\n",
      "Epoch 57/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8038 - loss: 0.3760 \n",
      "Epoch 57: val_loss did not improve from 0.38657\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8047 - loss: 0.3734 - val_accuracy: 0.7871 - val_loss: 0.3916 - learning_rate: 0.0100\n",
      "Epoch 58/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8034 - loss: 0.3724 \n",
      "Epoch 58: val_loss improved from 0.38657 to 0.38098, saving model to folds5.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8059 - loss: 0.3703 - val_accuracy: 0.7944 - val_loss: 0.3810 - learning_rate: 0.0100\n",
      "Epoch 59/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8060 - loss: 0.3676 \n",
      "Epoch 59: val_loss did not improve from 0.38098\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8065 - loss: 0.3673 - val_accuracy: 0.7938 - val_loss: 0.3861 - learning_rate: 0.0100\n",
      "Epoch 60/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8050 - loss: 0.3717 \n",
      "Epoch 60: val_loss improved from 0.38098 to 0.37989, saving model to folds5.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8059 - loss: 0.3683 - val_accuracy: 0.8096 - val_loss: 0.3799 - learning_rate: 0.0100\n",
      "Epoch 61/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8020 - loss: 0.3728 \n",
      "Epoch 61: val_loss did not improve from 0.37989\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8047 - loss: 0.3690 - val_accuracy: 0.7919 - val_loss: 0.3804 - learning_rate: 0.0100\n",
      "Epoch 62/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7948 - loss: 0.3808 \n",
      "Epoch 62: val_loss did not improve from 0.37989\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7995 - loss: 0.3752 - val_accuracy: 0.7913 - val_loss: 0.3905 - learning_rate: 0.0100\n",
      "Epoch 63/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7993 - loss: 0.3770 \n",
      "Epoch 63: val_loss did not improve from 0.37989\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7990 - loss: 0.3759 - val_accuracy: 0.7608 - val_loss: 0.4000 - learning_rate: 0.0100\n",
      "Epoch 64/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7828 - loss: 0.3784  \n",
      "Epoch 64: val_loss did not improve from 0.37989\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7879 - loss: 0.3766 - val_accuracy: 0.7987 - val_loss: 0.3881 - learning_rate: 0.0100\n",
      "Epoch 65/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8131 - loss: 0.3600 \n",
      "Epoch 65: val_loss did not improve from 0.37989\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8115 - loss: 0.3612 - val_accuracy: 0.7999 - val_loss: 0.3879 - learning_rate: 0.0100\n",
      "Epoch 66/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8092 - loss: 0.3654  \n",
      "Epoch 66: val_loss improved from 0.37989 to 0.37367, saving model to folds5.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8100 - loss: 0.3640 - val_accuracy: 0.7974 - val_loss: 0.3737 - learning_rate: 0.0100\n",
      "Epoch 67/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8062 - loss: 0.3610 \n",
      "Epoch 67: val_loss did not improve from 0.37367\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8064 - loss: 0.3624 - val_accuracy: 0.8011 - val_loss: 0.3754 - learning_rate: 0.0100\n",
      "Epoch 68/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8004 - loss: 0.3657 \n",
      "Epoch 68: val_loss did not improve from 0.37367\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8004 - loss: 0.3651 - val_accuracy: 0.8048 - val_loss: 0.3781 - learning_rate: 0.0100\n",
      "Epoch 69/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8147 - loss: 0.3603 \n",
      "Epoch 69: val_loss improved from 0.37367 to 0.37157, saving model to folds5.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8127 - loss: 0.3608 - val_accuracy: 0.7968 - val_loss: 0.3716 - learning_rate: 0.0100\n",
      "Epoch 70/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8076 - loss: 0.3563 \n",
      "Epoch 70: val_loss did not improve from 0.37157\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8097 - loss: 0.3572 - val_accuracy: 0.8041 - val_loss: 0.3731 - learning_rate: 0.0100\n",
      "Epoch 71/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8120 - loss: 0.3592 \n",
      "Epoch 71: val_loss did not improve from 0.37157\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8120 - loss: 0.3592 - val_accuracy: 0.8017 - val_loss: 0.3737 - learning_rate: 0.0100\n",
      "Epoch 72/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8080 - loss: 0.3583 \n",
      "Epoch 72: val_loss did not improve from 0.37157\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8112 - loss: 0.3559 - val_accuracy: 0.7974 - val_loss: 0.3732 - learning_rate: 0.0100\n",
      "Epoch 73/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8150 - loss: 0.3460 \n",
      "Epoch 73: val_loss did not improve from 0.37157\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8134 - loss: 0.3507 - val_accuracy: 0.7950 - val_loss: 0.3771 - learning_rate: 0.0100\n",
      "Epoch 74/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8010 - loss: 0.3661\n",
      "Epoch 74: val_loss did not improve from 0.37157\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.8009 - loss: 0.3664 - val_accuracy: 0.7980 - val_loss: 0.3792 - learning_rate: 0.0100\n",
      "Epoch 75/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8003 - loss: 0.3724\n",
      "Epoch 75: val_loss improved from 0.37157 to 0.36698, saving model to folds5.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.8012 - loss: 0.3713 - val_accuracy: 0.8054 - val_loss: 0.3670 - learning_rate: 0.0100\n",
      "Epoch 76/300\n",
      "\u001b[1m12/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8103 - loss: 0.3585\n",
      "Epoch 76: val_loss did not improve from 0.36698\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.8107 - loss: 0.3581 - val_accuracy: 0.8023 - val_loss: 0.3786 - learning_rate: 0.0100\n",
      "Epoch 77/300\n",
      "\u001b[1m12/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8113 - loss: 0.3621\n",
      "Epoch 77: val_loss did not improve from 0.36698\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.8115 - loss: 0.3616 - val_accuracy: 0.7993 - val_loss: 0.3837 - learning_rate: 0.0100\n",
      "Epoch 78/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8111 - loss: 0.3595\n",
      "Epoch 78: val_loss did not improve from 0.36698\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.8110 - loss: 0.3596 - val_accuracy: 0.7962 - val_loss: 0.3811 - learning_rate: 0.0100\n",
      "Epoch 79/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8178 - loss: 0.3558\n",
      "Epoch 79: val_loss did not improve from 0.36698\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.8167 - loss: 0.3555 - val_accuracy: 0.8060 - val_loss: 0.3745 - learning_rate: 0.0100\n",
      "Epoch 80/300\n",
      "\u001b[1m12/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8151 - loss: 0.3507\n",
      "Epoch 80: val_loss did not improve from 0.36698\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.8140 - loss: 0.3524 - val_accuracy: 0.7944 - val_loss: 0.3760 - learning_rate: 0.0100\n",
      "Epoch 81/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8082 - loss: 0.3589\n",
      "Epoch 81: val_loss did not improve from 0.36698\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8079 - loss: 0.3590 - val_accuracy: 0.7987 - val_loss: 0.3714 - learning_rate: 0.0100\n",
      "Epoch 82/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8141 - loss: 0.3511\n",
      "Epoch 82: val_loss improved from 0.36698 to 0.36142, saving model to folds5.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8142 - loss: 0.3513 - val_accuracy: 0.8109 - val_loss: 0.3614 - learning_rate: 0.0100\n",
      "Epoch 83/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8117 - loss: 0.3557 \n",
      "Epoch 83: val_loss did not improve from 0.36142\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8126 - loss: 0.3524 - val_accuracy: 0.8011 - val_loss: 0.3859 - learning_rate: 0.0100\n",
      "Epoch 84/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8160 - loss: 0.3546 \n",
      "Epoch 84: val_loss did not improve from 0.36142\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8160 - loss: 0.3531 - val_accuracy: 0.8139 - val_loss: 0.3617 - learning_rate: 0.0100\n",
      "Epoch 85/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8089 - loss: 0.3511 \n",
      "Epoch 85: val_loss improved from 0.36142 to 0.36058, saving model to folds5.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8125 - loss: 0.3502 - val_accuracy: 0.8103 - val_loss: 0.3606 - learning_rate: 0.0100\n",
      "Epoch 86/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8242 - loss: 0.3385 \n",
      "Epoch 86: val_loss did not improve from 0.36058\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8242 - loss: 0.3396 - val_accuracy: 0.8041 - val_loss: 0.3625 - learning_rate: 0.0100\n",
      "Epoch 87/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8217 - loss: 0.3408 \n",
      "Epoch 87: val_loss improved from 0.36058 to 0.35942, saving model to folds5.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8207 - loss: 0.3413 - val_accuracy: 0.8090 - val_loss: 0.3594 - learning_rate: 0.0100\n",
      "Epoch 88/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8211 - loss: 0.3443  \n",
      "Epoch 88: val_loss did not improve from 0.35942\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8200 - loss: 0.3451 - val_accuracy: 0.8103 - val_loss: 0.3695 - learning_rate: 0.0100\n",
      "Epoch 89/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8300 - loss: 0.3306 \n",
      "Epoch 89: val_loss did not improve from 0.35942\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8270 - loss: 0.3365 - val_accuracy: 0.8164 - val_loss: 0.3609 - learning_rate: 0.0100\n",
      "Epoch 90/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8203 - loss: 0.3452 \n",
      "Epoch 90: val_loss did not improve from 0.35942\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8198 - loss: 0.3456 - val_accuracy: 0.8084 - val_loss: 0.3689 - learning_rate: 0.0100\n",
      "Epoch 91/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8162 - loss: 0.3531 \n",
      "Epoch 91: val_loss improved from 0.35942 to 0.35925, saving model to folds5.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8165 - loss: 0.3500 - val_accuracy: 0.8127 - val_loss: 0.3593 - learning_rate: 0.0100\n",
      "Epoch 92/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8141 - loss: 0.3526 \n",
      "Epoch 92: val_loss did not improve from 0.35925\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8135 - loss: 0.3541 - val_accuracy: 0.8005 - val_loss: 0.3734 - learning_rate: 0.0100\n",
      "Epoch 93/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8172 - loss: 0.3529 \n",
      "Epoch 93: val_loss did not improve from 0.35925\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8161 - loss: 0.3529 - val_accuracy: 0.8084 - val_loss: 0.3668 - learning_rate: 0.0100\n",
      "Epoch 94/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8104 - loss: 0.3598 \n",
      "Epoch 94: val_loss did not improve from 0.35925\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8128 - loss: 0.3560 - val_accuracy: 0.8090 - val_loss: 0.3681 - learning_rate: 0.0100\n",
      "Epoch 95/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8091 - loss: 0.3503\n",
      "Epoch 95: val_loss did not improve from 0.35925\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8098 - loss: 0.3502 - val_accuracy: 0.8096 - val_loss: 0.3667 - learning_rate: 0.0100\n",
      "Epoch 96/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8284 - loss: 0.3461 \n",
      "Epoch 96: val_loss did not improve from 0.35925\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8240 - loss: 0.3449 - val_accuracy: 0.8176 - val_loss: 0.3644 - learning_rate: 0.0100\n",
      "Epoch 97/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8177 - loss: 0.3537 \n",
      "Epoch 97: val_loss improved from 0.35925 to 0.35565, saving model to folds5.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8188 - loss: 0.3501 - val_accuracy: 0.8200 - val_loss: 0.3556 - learning_rate: 0.0100\n",
      "Epoch 98/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8138 - loss: 0.3452 \n",
      "Epoch 98: val_loss did not improve from 0.35565\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8160 - loss: 0.3455 - val_accuracy: 0.8139 - val_loss: 0.3613 - learning_rate: 0.0100\n",
      "Epoch 99/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8178 - loss: 0.3420 \n",
      "Epoch 99: val_loss did not improve from 0.35565\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8189 - loss: 0.3435 - val_accuracy: 0.8133 - val_loss: 0.3590 - learning_rate: 0.0100\n",
      "Epoch 100/300\n",
      "\u001b[1m13/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8159 - loss: 0.3524\n",
      "Epoch 100: val_loss improved from 0.35565 to 0.35263, saving model to folds5.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8163 - loss: 0.3512 - val_accuracy: 0.8145 - val_loss: 0.3526 - learning_rate: 0.0100\n",
      "Epoch 101/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8202 - loss: 0.3368 \n",
      "Epoch 101: val_loss did not improve from 0.35263\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8211 - loss: 0.3385 - val_accuracy: 0.8157 - val_loss: 0.3550 - learning_rate: 0.0100\n",
      "Epoch 102/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8155 - loss: 0.3523 \n",
      "Epoch 102: val_loss did not improve from 0.35263\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8186 - loss: 0.3477 - val_accuracy: 0.8121 - val_loss: 0.3543 - learning_rate: 0.0100\n",
      "Epoch 103/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8223 - loss: 0.3478 \n",
      "Epoch 103: val_loss improved from 0.35263 to 0.35128, saving model to folds5.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8237 - loss: 0.3455 - val_accuracy: 0.8115 - val_loss: 0.3513 - learning_rate: 0.0100\n",
      "Epoch 104/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8205 - loss: 0.3440 \n",
      "Epoch 104: val_loss did not improve from 0.35128\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8203 - loss: 0.3432 - val_accuracy: 0.8023 - val_loss: 0.3641 - learning_rate: 0.0100\n",
      "Epoch 105/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8197 - loss: 0.3508\n",
      "Epoch 105: val_loss did not improve from 0.35128\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8197 - loss: 0.3507 - val_accuracy: 0.8115 - val_loss: 0.3611 - learning_rate: 0.0100\n",
      "Epoch 106/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8097 - loss: 0.3406 \n",
      "Epoch 106: val_loss improved from 0.35128 to 0.34861, saving model to folds5.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8135 - loss: 0.3406 - val_accuracy: 0.8164 - val_loss: 0.3486 - learning_rate: 0.0100\n",
      "Epoch 107/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8110 - loss: 0.3461  \n",
      "Epoch 107: val_loss did not improve from 0.34861\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8154 - loss: 0.3436 - val_accuracy: 0.8139 - val_loss: 0.3548 - learning_rate: 0.0100\n",
      "Epoch 108/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8116 - loss: 0.3460 \n",
      "Epoch 108: val_loss did not improve from 0.34861\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8161 - loss: 0.3420 - val_accuracy: 0.8096 - val_loss: 0.3607 - learning_rate: 0.0100\n",
      "Epoch 109/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8119 - loss: 0.3492 \n",
      "Epoch 109: val_loss did not improve from 0.34861\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8155 - loss: 0.3452 - val_accuracy: 0.8316 - val_loss: 0.3490 - learning_rate: 0.0100\n",
      "Epoch 110/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8216 - loss: 0.3404 \n",
      "Epoch 110: val_loss did not improve from 0.34861\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8233 - loss: 0.3378 - val_accuracy: 0.8115 - val_loss: 0.3539 - learning_rate: 0.0100\n",
      "Epoch 111/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8296 - loss: 0.3332 \n",
      "Epoch 111: val_loss did not improve from 0.34861\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8273 - loss: 0.3341 - val_accuracy: 0.8035 - val_loss: 0.3603 - learning_rate: 0.0100\n",
      "Epoch 112/300\n",
      "\u001b[1m 7/15\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8218 - loss: 0.3432 \n",
      "Epoch 112: val_loss improved from 0.34861 to 0.34721, saving model to folds5.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8229 - loss: 0.3421 - val_accuracy: 0.8279 - val_loss: 0.3472 - learning_rate: 0.0100\n",
      "Epoch 113/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8315 - loss: 0.3347 \n",
      "Epoch 113: val_loss improved from 0.34721 to 0.34312, saving model to folds5.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8307 - loss: 0.3343 - val_accuracy: 0.8255 - val_loss: 0.3431 - learning_rate: 0.0100\n",
      "Epoch 114/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8265 - loss: 0.3382\n",
      "Epoch 114: val_loss did not improve from 0.34312\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8266 - loss: 0.3379 - val_accuracy: 0.7987 - val_loss: 0.3575 - learning_rate: 0.0100\n",
      "Epoch 115/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8193 - loss: 0.3349 \n",
      "Epoch 115: val_loss did not improve from 0.34312\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8212 - loss: 0.3340 - val_accuracy: 0.8218 - val_loss: 0.3481 - learning_rate: 0.0100\n",
      "Epoch 116/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8275 - loss: 0.3245 \n",
      "Epoch 116: val_loss did not improve from 0.34312\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8267 - loss: 0.3276 - val_accuracy: 0.8072 - val_loss: 0.3483 - learning_rate: 0.0100\n",
      "Epoch 117/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8206 - loss: 0.3419 \n",
      "Epoch 117: val_loss did not improve from 0.34312\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8233 - loss: 0.3392 - val_accuracy: 0.8225 - val_loss: 0.3458 - learning_rate: 0.0100\n",
      "Epoch 118/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8208 - loss: 0.3370 \n",
      "Epoch 118: val_loss did not improve from 0.34312\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8224 - loss: 0.3356 - val_accuracy: 0.8060 - val_loss: 0.3509 - learning_rate: 0.0100\n",
      "Epoch 119/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8237 - loss: 0.3303\n",
      "Epoch 119: val_loss did not improve from 0.34312\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8240 - loss: 0.3303 - val_accuracy: 0.8182 - val_loss: 0.3477 - learning_rate: 0.0100\n",
      "Epoch 120/300\n",
      "\u001b[1m 7/15\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8228 - loss: 0.3320 \n",
      "Epoch 120: val_loss improved from 0.34312 to 0.34307, saving model to folds5.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8239 - loss: 0.3311 - val_accuracy: 0.8273 - val_loss: 0.3431 - learning_rate: 0.0100\n",
      "Epoch 121/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8327 - loss: 0.3252 \n",
      "Epoch 121: val_loss did not improve from 0.34307\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8310 - loss: 0.3257 - val_accuracy: 0.8121 - val_loss: 0.3490 - learning_rate: 0.0100\n",
      "Epoch 122/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8209 - loss: 0.3391 \n",
      "Epoch 122: val_loss improved from 0.34307 to 0.34152, saving model to folds5.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8249 - loss: 0.3347 - val_accuracy: 0.8231 - val_loss: 0.3415 - learning_rate: 0.0100\n",
      "Epoch 123/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8218 - loss: 0.3307 \n",
      "Epoch 123: val_loss did not improve from 0.34152\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8243 - loss: 0.3307 - val_accuracy: 0.8127 - val_loss: 0.3539 - learning_rate: 0.0100\n",
      "Epoch 124/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8270 - loss: 0.3327 \n",
      "Epoch 124: val_loss improved from 0.34152 to 0.34088, saving model to folds5.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8267 - loss: 0.3315 - val_accuracy: 0.8225 - val_loss: 0.3409 - learning_rate: 0.0100\n",
      "Epoch 125/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8244 - loss: 0.3297 \n",
      "Epoch 125: val_loss improved from 0.34088 to 0.33893, saving model to folds5.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8254 - loss: 0.3296 - val_accuracy: 0.8261 - val_loss: 0.3389 - learning_rate: 0.0100\n",
      "Epoch 126/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8267 - loss: 0.3306 \n",
      "Epoch 126: val_loss improved from 0.33893 to 0.33533, saving model to folds5.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8294 - loss: 0.3262 - val_accuracy: 0.8279 - val_loss: 0.3353 - learning_rate: 0.0100\n",
      "Epoch 127/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8346 - loss: 0.3231\n",
      "Epoch 127: val_loss did not improve from 0.33533\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8345 - loss: 0.3230 - val_accuracy: 0.8200 - val_loss: 0.3380 - learning_rate: 0.0100\n",
      "Epoch 128/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8355 - loss: 0.3178\n",
      "Epoch 128: val_loss did not improve from 0.33533\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.8351 - loss: 0.3191 - val_accuracy: 0.8322 - val_loss: 0.3364 - learning_rate: 0.0100\n",
      "Epoch 129/300\n",
      "\u001b[1m12/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8259 - loss: 0.3300\n",
      "Epoch 129: val_loss did not improve from 0.33533\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8274 - loss: 0.3287 - val_accuracy: 0.8365 - val_loss: 0.3383 - learning_rate: 0.0100\n",
      "Epoch 130/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8282 - loss: 0.3287\n",
      "Epoch 130: val_loss improved from 0.33533 to 0.33127, saving model to folds5.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.8285 - loss: 0.3269 - val_accuracy: 0.8279 - val_loss: 0.3313 - learning_rate: 0.0100\n",
      "Epoch 131/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8280 - loss: 0.3265\n",
      "Epoch 131: val_loss did not improve from 0.33127\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.8278 - loss: 0.3272 - val_accuracy: 0.8298 - val_loss: 0.3429 - learning_rate: 0.0100\n",
      "Epoch 132/300\n",
      "\u001b[1m12/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8359 - loss: 0.3195 \n",
      "Epoch 132: val_loss did not improve from 0.33127\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.8346 - loss: 0.3200 - val_accuracy: 0.8353 - val_loss: 0.3360 - learning_rate: 0.0100\n",
      "Epoch 133/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8271 - loss: 0.3345\n",
      "Epoch 133: val_loss did not improve from 0.33127\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.8270 - loss: 0.3334 - val_accuracy: 0.8249 - val_loss: 0.3433 - learning_rate: 0.0100\n",
      "Epoch 134/300\n",
      "\u001b[1m13/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8310 - loss: 0.3402 \n",
      "Epoch 134: val_loss did not improve from 0.33127\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.8306 - loss: 0.3389 - val_accuracy: 0.8304 - val_loss: 0.3366 - learning_rate: 0.0100\n",
      "Epoch 135/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8243 - loss: 0.3333\n",
      "Epoch 135: val_loss improved from 0.33127 to 0.32919, saving model to folds5.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.8256 - loss: 0.3309 - val_accuracy: 0.8267 - val_loss: 0.3292 - learning_rate: 0.0100\n",
      "Epoch 136/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8325 - loss: 0.3142  \n",
      "Epoch 136: val_loss did not improve from 0.32919\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8340 - loss: 0.3172 - val_accuracy: 0.8298 - val_loss: 0.3476 - learning_rate: 0.0100\n",
      "Epoch 137/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8202 - loss: 0.3405 \n",
      "Epoch 137: val_loss did not improve from 0.32919\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8265 - loss: 0.3318 - val_accuracy: 0.8267 - val_loss: 0.3408 - learning_rate: 0.0100\n",
      "Epoch 138/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8434 - loss: 0.3199 \n",
      "Epoch 138: val_loss did not improve from 0.32919\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8407 - loss: 0.3196 - val_accuracy: 0.8365 - val_loss: 0.3297 - learning_rate: 0.0100\n",
      "Epoch 139/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8298 - loss: 0.3186 \n",
      "Epoch 139: val_loss did not improve from 0.32919\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8317 - loss: 0.3207 - val_accuracy: 0.8316 - val_loss: 0.3325 - learning_rate: 0.0100\n",
      "Epoch 140/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8338 - loss: 0.3193 \n",
      "Epoch 140: val_loss improved from 0.32919 to 0.32882, saving model to folds5.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8340 - loss: 0.3191 - val_accuracy: 0.8298 - val_loss: 0.3288 - learning_rate: 0.0100\n",
      "Epoch 141/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8374 - loss: 0.3197 \n",
      "Epoch 141: val_loss improved from 0.32882 to 0.32687, saving model to folds5.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8377 - loss: 0.3177 - val_accuracy: 0.8304 - val_loss: 0.3269 - learning_rate: 0.0100\n",
      "Epoch 142/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8359 - loss: 0.3125 \n",
      "Epoch 142: val_loss improved from 0.32687 to 0.32645, saving model to folds5.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8361 - loss: 0.3130 - val_accuracy: 0.8359 - val_loss: 0.3264 - learning_rate: 0.0100\n",
      "Epoch 143/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8399 - loss: 0.3113 \n",
      "Epoch 143: val_loss did not improve from 0.32645\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8399 - loss: 0.3121 - val_accuracy: 0.8304 - val_loss: 0.3289 - learning_rate: 0.0100\n",
      "Epoch 144/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8426 - loss: 0.3112  \n",
      "Epoch 144: val_loss did not improve from 0.32645\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8383 - loss: 0.3160 - val_accuracy: 0.8182 - val_loss: 0.3505 - learning_rate: 0.0100\n",
      "Epoch 145/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8287 - loss: 0.3229 \n",
      "Epoch 145: val_loss did not improve from 0.32645\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8305 - loss: 0.3207 - val_accuracy: 0.8206 - val_loss: 0.3349 - learning_rate: 0.0100\n",
      "Epoch 146/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8360 - loss: 0.3205  \n",
      "Epoch 146: val_loss did not improve from 0.32645\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8367 - loss: 0.3183 - val_accuracy: 0.8322 - val_loss: 0.3328 - learning_rate: 0.0100\n",
      "Epoch 147/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8307 - loss: 0.3186 \n",
      "Epoch 147: val_loss did not improve from 0.32645\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8317 - loss: 0.3194 - val_accuracy: 0.8304 - val_loss: 0.3321 - learning_rate: 0.0100\n",
      "Epoch 148/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8233 - loss: 0.3357 \n",
      "Epoch 148: val_loss did not improve from 0.32645\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8267 - loss: 0.3320 - val_accuracy: 0.8261 - val_loss: 0.3427 - learning_rate: 0.0100\n",
      "Epoch 149/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8355 - loss: 0.3209 \n",
      "Epoch 149: val_loss did not improve from 0.32645\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8344 - loss: 0.3243 - val_accuracy: 0.8231 - val_loss: 0.3469 - learning_rate: 0.0100\n",
      "Epoch 150/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8250 - loss: 0.3310 \n",
      "Epoch 150: val_loss did not improve from 0.32645\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8283 - loss: 0.3270 - val_accuracy: 0.8267 - val_loss: 0.3350 - learning_rate: 0.0100\n",
      "Epoch 151/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8311 - loss: 0.3244 \n",
      "Epoch 151: val_loss did not improve from 0.32645\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8334 - loss: 0.3224 - val_accuracy: 0.8292 - val_loss: 0.3322 - learning_rate: 0.0100\n",
      "Epoch 152/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8404 - loss: 0.3163 \n",
      "Epoch 152: val_loss improved from 0.32645 to 0.32474, saving model to folds5.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8420 - loss: 0.3146 - val_accuracy: 0.8365 - val_loss: 0.3247 - learning_rate: 0.0100\n",
      "Epoch 153/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8364 - loss: 0.3159\n",
      "Epoch 153: val_loss did not improve from 0.32474\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8362 - loss: 0.3161 - val_accuracy: 0.8414 - val_loss: 0.3340 - learning_rate: 0.0100\n",
      "Epoch 154/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8379 - loss: 0.3154 \n",
      "Epoch 154: val_loss did not improve from 0.32474\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8378 - loss: 0.3148 - val_accuracy: 0.8395 - val_loss: 0.3275 - learning_rate: 0.0100\n",
      "Epoch 155/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8387 - loss: 0.3120 \n",
      "Epoch 155: val_loss improved from 0.32474 to 0.32405, saving model to folds5.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8382 - loss: 0.3135 - val_accuracy: 0.8377 - val_loss: 0.3241 - learning_rate: 0.0100\n",
      "Epoch 156/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8450 - loss: 0.3041 \n",
      "Epoch 156: val_loss improved from 0.32405 to 0.32067, saving model to folds5.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8446 - loss: 0.3057 - val_accuracy: 0.8438 - val_loss: 0.3207 - learning_rate: 0.0100\n",
      "Epoch 157/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8412 - loss: 0.3135 \n",
      "Epoch 157: val_loss did not improve from 0.32067\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8410 - loss: 0.3133 - val_accuracy: 0.8377 - val_loss: 0.3358 - learning_rate: 0.0100\n",
      "Epoch 158/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8277 - loss: 0.3271  \n",
      "Epoch 158: val_loss did not improve from 0.32067\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8314 - loss: 0.3225 - val_accuracy: 0.8267 - val_loss: 0.3326 - learning_rate: 0.0100\n",
      "Epoch 159/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8390 - loss: 0.3098\n",
      "Epoch 159: val_loss improved from 0.32067 to 0.31871, saving model to folds5.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8392 - loss: 0.3097 - val_accuracy: 0.8359 - val_loss: 0.3187 - learning_rate: 0.0100\n",
      "Epoch 160/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8406 - loss: 0.3028\n",
      "Epoch 160: val_loss did not improve from 0.31871\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8407 - loss: 0.3030 - val_accuracy: 0.8414 - val_loss: 0.3196 - learning_rate: 0.0100\n",
      "Epoch 161/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8418 - loss: 0.3102  \n",
      "Epoch 161: val_loss improved from 0.31871 to 0.31784, saving model to folds5.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8412 - loss: 0.3106 - val_accuracy: 0.8487 - val_loss: 0.3178 - learning_rate: 0.0100\n",
      "Epoch 162/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8423 - loss: 0.3134 \n",
      "Epoch 162: val_loss did not improve from 0.31784\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8419 - loss: 0.3121 - val_accuracy: 0.8304 - val_loss: 0.3234 - learning_rate: 0.0100\n",
      "Epoch 163/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8447 - loss: 0.3079 \n",
      "Epoch 163: val_loss did not improve from 0.31784\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8433 - loss: 0.3088 - val_accuracy: 0.8432 - val_loss: 0.3215 - learning_rate: 0.0100\n",
      "Epoch 164/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8373 - loss: 0.3272 \n",
      "Epoch 164: val_loss did not improve from 0.31784\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8323 - loss: 0.3337 - val_accuracy: 0.8304 - val_loss: 0.3556 - learning_rate: 0.0100\n",
      "Epoch 165/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8359 - loss: 0.3267 \n",
      "Epoch 165: val_loss did not improve from 0.31784\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8377 - loss: 0.3218 - val_accuracy: 0.8511 - val_loss: 0.3196 - learning_rate: 0.0100\n",
      "Epoch 166/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8435 - loss: 0.3101\n",
      "Epoch 166: val_loss improved from 0.31784 to 0.31676, saving model to folds5.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8437 - loss: 0.3097 - val_accuracy: 0.8462 - val_loss: 0.3168 - learning_rate: 0.0100\n",
      "Epoch 167/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8414 - loss: 0.3077 \n",
      "Epoch 167: val_loss did not improve from 0.31676\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8429 - loss: 0.3057 - val_accuracy: 0.8310 - val_loss: 0.3235 - learning_rate: 0.0100\n",
      "Epoch 168/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8410 - loss: 0.3040 \n",
      "Epoch 168: val_loss improved from 0.31676 to 0.31494, saving model to folds5.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8435 - loss: 0.3018 - val_accuracy: 0.8353 - val_loss: 0.3149 - learning_rate: 0.0100\n",
      "Epoch 169/300\n",
      "\u001b[1m13/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8440 - loss: 0.3009\n",
      "Epoch 169: val_loss improved from 0.31494 to 0.31241, saving model to folds5.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8434 - loss: 0.3022 - val_accuracy: 0.8475 - val_loss: 0.3124 - learning_rate: 0.0100\n",
      "Epoch 170/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8377 - loss: 0.3078 \n",
      "Epoch 170: val_loss did not improve from 0.31241\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8392 - loss: 0.3058 - val_accuracy: 0.8462 - val_loss: 0.3127 - learning_rate: 0.0100\n",
      "Epoch 171/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8459 - loss: 0.3044 \n",
      "Epoch 171: val_loss did not improve from 0.31241\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8459 - loss: 0.3026 - val_accuracy: 0.8395 - val_loss: 0.3179 - learning_rate: 0.0100\n",
      "Epoch 172/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8505 - loss: 0.2999\n",
      "Epoch 172: val_loss did not improve from 0.31241\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8503 - loss: 0.2999 - val_accuracy: 0.8371 - val_loss: 0.3282 - learning_rate: 0.0100\n",
      "Epoch 173/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8413 - loss: 0.3102 \n",
      "Epoch 173: val_loss did not improve from 0.31241\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8426 - loss: 0.3087 - val_accuracy: 0.8444 - val_loss: 0.3258 - learning_rate: 0.0100\n",
      "Epoch 174/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8463 - loss: 0.3125 \n",
      "Epoch 174: val_loss did not improve from 0.31241\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8451 - loss: 0.3100 - val_accuracy: 0.8322 - val_loss: 0.3192 - learning_rate: 0.0100\n",
      "Epoch 175/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8404 - loss: 0.3081 \n",
      "Epoch 175: val_loss did not improve from 0.31241\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8439 - loss: 0.3048 - val_accuracy: 0.8456 - val_loss: 0.3185 - learning_rate: 0.0100\n",
      "Epoch 176/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8426 - loss: 0.3087 \n",
      "Epoch 176: val_loss did not improve from 0.31241\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8434 - loss: 0.3051 - val_accuracy: 0.8438 - val_loss: 0.3138 - learning_rate: 0.0100\n",
      "Epoch 177/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8497 - loss: 0.2994\n",
      "Epoch 177: val_loss did not improve from 0.31241\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8495 - loss: 0.2995 - val_accuracy: 0.8462 - val_loss: 0.3153 - learning_rate: 0.0100\n",
      "Epoch 178/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8490 - loss: 0.3018 \n",
      "Epoch 178: val_loss did not improve from 0.31241\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8471 - loss: 0.3021 - val_accuracy: 0.8218 - val_loss: 0.3161 - learning_rate: 0.0100\n",
      "Epoch 179/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8341 - loss: 0.2982\n",
      "Epoch 179: val_loss improved from 0.31241 to 0.31103, saving model to folds5.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.8354 - loss: 0.3007 - val_accuracy: 0.8615 - val_loss: 0.3110 - learning_rate: 0.0100\n",
      "Epoch 180/300\n",
      "\u001b[1m13/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8475 - loss: 0.2996\n",
      "Epoch 180: val_loss did not improve from 0.31103\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.8469 - loss: 0.3001 - val_accuracy: 0.8395 - val_loss: 0.3165 - learning_rate: 0.0100\n",
      "Epoch 181/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8461 - loss: 0.2991\n",
      "Epoch 181: val_loss improved from 0.31103 to 0.31080, saving model to folds5.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.8461 - loss: 0.2989 - val_accuracy: 0.8444 - val_loss: 0.3108 - learning_rate: 0.0100\n",
      "Epoch 182/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8456 - loss: 0.2986\n",
      "Epoch 182: val_loss did not improve from 0.31080\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.8463 - loss: 0.2987 - val_accuracy: 0.8542 - val_loss: 0.3130 - learning_rate: 0.0100\n",
      "Epoch 183/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8547 - loss: 0.2923\n",
      "Epoch 183: val_loss did not improve from 0.31080\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.8544 - loss: 0.2925 - val_accuracy: 0.8475 - val_loss: 0.3163 - learning_rate: 0.0100\n",
      "Epoch 184/300\n",
      "\u001b[1m12/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8413 - loss: 0.2999\n",
      "Epoch 184: val_loss did not improve from 0.31080\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.8421 - loss: 0.2993 - val_accuracy: 0.8414 - val_loss: 0.3143 - learning_rate: 0.0100\n",
      "Epoch 185/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8445 - loss: 0.3042\n",
      "Epoch 185: val_loss did not improve from 0.31080\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.8448 - loss: 0.3034 - val_accuracy: 0.8408 - val_loss: 0.3181 - learning_rate: 0.0100\n",
      "Epoch 186/300\n",
      "\u001b[1m12/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8479 - loss: 0.2959\n",
      "Epoch 186: val_loss did not improve from 0.31080\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.8473 - loss: 0.2969 - val_accuracy: 0.8456 - val_loss: 0.3115 - learning_rate: 0.0100\n",
      "Epoch 187/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8433 - loss: 0.2954  \n",
      "Epoch 187: val_loss did not improve from 0.31080\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8444 - loss: 0.2945 - val_accuracy: 0.8383 - val_loss: 0.3208 - learning_rate: 0.0100\n",
      "Epoch 188/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8402 - loss: 0.3064 \n",
      "Epoch 188: val_loss did not improve from 0.31080\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8407 - loss: 0.3042 - val_accuracy: 0.8218 - val_loss: 0.3209 - learning_rate: 0.0100\n",
      "Epoch 189/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8477 - loss: 0.2987 \n",
      "Epoch 189: val_loss did not improve from 0.31080\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8465 - loss: 0.2968 - val_accuracy: 0.8627 - val_loss: 0.3117 - learning_rate: 0.0100\n",
      "Epoch 190/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8472 - loss: 0.2976\n",
      "Epoch 190: val_loss did not improve from 0.31080\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8471 - loss: 0.2970 - val_accuracy: 0.8377 - val_loss: 0.3130 - learning_rate: 0.0100\n",
      "Epoch 191/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8493 - loss: 0.2916 \n",
      "Epoch 191: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
      "\n",
      "Epoch 191: val_loss did not improve from 0.31080\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8483 - loss: 0.2924 - val_accuracy: 0.8340 - val_loss: 0.3145 - learning_rate: 0.0100\n",
      "Epoch 192/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8440 - loss: 0.3008 \n",
      "Epoch 192: val_loss improved from 0.31080 to 0.31024, saving model to folds5.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8472 - loss: 0.2964 - val_accuracy: 0.8499 - val_loss: 0.3102 - learning_rate: 0.0050\n",
      "Epoch 193/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8529 - loss: 0.2846 \n",
      "Epoch 193: val_loss improved from 0.31024 to 0.30637, saving model to folds5.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8516 - loss: 0.2867 - val_accuracy: 0.8493 - val_loss: 0.3064 - learning_rate: 0.0050\n",
      "Epoch 194/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8553 - loss: 0.2878 \n",
      "Epoch 194: val_loss improved from 0.30637 to 0.30316, saving model to folds5.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8549 - loss: 0.2866 - val_accuracy: 0.8487 - val_loss: 0.3032 - learning_rate: 0.0050\n",
      "Epoch 195/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8538 - loss: 0.2799 \n",
      "Epoch 195: val_loss did not improve from 0.30316\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8538 - loss: 0.2831 - val_accuracy: 0.8523 - val_loss: 0.3033 - learning_rate: 0.0050\n",
      "Epoch 196/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8427 - loss: 0.2922 \n",
      "Epoch 196: val_loss did not improve from 0.30316\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8467 - loss: 0.2898 - val_accuracy: 0.8536 - val_loss: 0.3039 - learning_rate: 0.0050\n",
      "Epoch 197/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8597 - loss: 0.2803 \n",
      "Epoch 197: val_loss did not improve from 0.30316\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8572 - loss: 0.2821 - val_accuracy: 0.8481 - val_loss: 0.3036 - learning_rate: 0.0050\n",
      "Epoch 198/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8548 - loss: 0.2854\n",
      "Epoch 198: val_loss did not improve from 0.30316\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8548 - loss: 0.2854 - val_accuracy: 0.8469 - val_loss: 0.3039 - learning_rate: 0.0050\n",
      "Epoch 199/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8559 - loss: 0.2766 \n",
      "Epoch 199: val_loss did not improve from 0.30316\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8552 - loss: 0.2802 - val_accuracy: 0.8426 - val_loss: 0.3044 - learning_rate: 0.0050\n",
      "Epoch 200/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8517 - loss: 0.2817  \n",
      "Epoch 200: val_loss did not improve from 0.30316\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8512 - loss: 0.2839 - val_accuracy: 0.8395 - val_loss: 0.3044 - learning_rate: 0.0050\n",
      "Epoch 201/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8587 - loss: 0.2777\n",
      "Epoch 201: val_loss did not improve from 0.30316\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8583 - loss: 0.2782 - val_accuracy: 0.8450 - val_loss: 0.3050 - learning_rate: 0.0050\n",
      "Epoch 202/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8556 - loss: 0.2861 \n",
      "Epoch 202: val_loss improved from 0.30316 to 0.30185, saving model to folds5.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8547 - loss: 0.2861 - val_accuracy: 0.8560 - val_loss: 0.3019 - learning_rate: 0.0050\n",
      "Epoch 203/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8549 - loss: 0.2851 \n",
      "Epoch 203: val_loss did not improve from 0.30185\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8555 - loss: 0.2852 - val_accuracy: 0.8499 - val_loss: 0.3081 - learning_rate: 0.0050\n",
      "Epoch 204/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8452 - loss: 0.2939 \n",
      "Epoch 204: val_loss did not improve from 0.30185\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8481 - loss: 0.2906 - val_accuracy: 0.8401 - val_loss: 0.3027 - learning_rate: 0.0050\n",
      "Epoch 205/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8456 - loss: 0.2875  \n",
      "Epoch 205: val_loss did not improve from 0.30185\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8483 - loss: 0.2864 - val_accuracy: 0.8481 - val_loss: 0.3056 - learning_rate: 0.0050\n",
      "Epoch 206/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8545 - loss: 0.2862 \n",
      "Epoch 206: val_loss did not improve from 0.30185\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8539 - loss: 0.2860 - val_accuracy: 0.8493 - val_loss: 0.3022 - learning_rate: 0.0050\n",
      "Epoch 207/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8483 - loss: 0.2874 \n",
      "Epoch 207: val_loss improved from 0.30185 to 0.30105, saving model to folds5.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8511 - loss: 0.2859 - val_accuracy: 0.8493 - val_loss: 0.3010 - learning_rate: 0.0050\n",
      "Epoch 208/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8559 - loss: 0.2856\n",
      "Epoch 208: val_loss improved from 0.30105 to 0.30044, saving model to folds5.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8560 - loss: 0.2852 - val_accuracy: 0.8505 - val_loss: 0.3004 - learning_rate: 0.0050\n",
      "Epoch 209/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8573 - loss: 0.2775 \n",
      "Epoch 209: val_loss did not improve from 0.30044\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8565 - loss: 0.2803 - val_accuracy: 0.8377 - val_loss: 0.3033 - learning_rate: 0.0050\n",
      "Epoch 210/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8548 - loss: 0.2824\n",
      "Epoch 210: val_loss did not improve from 0.30044\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8547 - loss: 0.2824 - val_accuracy: 0.8554 - val_loss: 0.3005 - learning_rate: 0.0050\n",
      "Epoch 211/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8528 - loss: 0.2874 \n",
      "Epoch 211: val_loss did not improve from 0.30044\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8545 - loss: 0.2844 - val_accuracy: 0.8566 - val_loss: 0.3005 - learning_rate: 0.0050\n",
      "Epoch 212/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8572 - loss: 0.2798\n",
      "Epoch 212: val_loss did not improve from 0.30044\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8571 - loss: 0.2799 - val_accuracy: 0.8530 - val_loss: 0.3015 - learning_rate: 0.0050\n",
      "Epoch 213/300\n",
      "\u001b[1m12/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8500 - loss: 0.2862\n",
      "Epoch 213: val_loss improved from 0.30044 to 0.29921, saving model to folds5.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8509 - loss: 0.2850 - val_accuracy: 0.8481 - val_loss: 0.2992 - learning_rate: 0.0050\n",
      "Epoch 214/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8549 - loss: 0.2846 \n",
      "Epoch 214: val_loss did not improve from 0.29921\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8558 - loss: 0.2832 - val_accuracy: 0.8597 - val_loss: 0.2995 - learning_rate: 0.0050\n",
      "Epoch 215/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8577 - loss: 0.2799\n",
      "Epoch 215: val_loss did not improve from 0.29921\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8575 - loss: 0.2800 - val_accuracy: 0.8542 - val_loss: 0.2998 - learning_rate: 0.0050\n",
      "Epoch 216/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8507 - loss: 0.2857 \n",
      "Epoch 216: val_loss did not improve from 0.29921\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8521 - loss: 0.2840 - val_accuracy: 0.8456 - val_loss: 0.3059 - learning_rate: 0.0050\n",
      "Epoch 217/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8614 - loss: 0.2769 \n",
      "Epoch 217: val_loss did not improve from 0.29921\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8578 - loss: 0.2790 - val_accuracy: 0.8542 - val_loss: 0.3004 - learning_rate: 0.0050\n",
      "Epoch 218/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8575 - loss: 0.2771  \n",
      "Epoch 218: val_loss did not improve from 0.29921\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8561 - loss: 0.2796 - val_accuracy: 0.8456 - val_loss: 0.3050 - learning_rate: 0.0050\n",
      "Epoch 219/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8515 - loss: 0.2812 \n",
      "Epoch 219: val_loss did not improve from 0.29921\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8537 - loss: 0.2810 - val_accuracy: 0.8475 - val_loss: 0.3015 - learning_rate: 0.0050\n",
      "Epoch 220/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8531 - loss: 0.2881\n",
      "Epoch 220: val_loss did not improve from 0.29921\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8530 - loss: 0.2879 - val_accuracy: 0.8469 - val_loss: 0.3125 - learning_rate: 0.0050\n",
      "Epoch 221/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8511 - loss: 0.2845\n",
      "Epoch 221: val_loss improved from 0.29921 to 0.29829, saving model to folds5.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8512 - loss: 0.2845 - val_accuracy: 0.8523 - val_loss: 0.2983 - learning_rate: 0.0050\n",
      "Epoch 222/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8550 - loss: 0.2779\n",
      "Epoch 222: val_loss did not improve from 0.29829\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8551 - loss: 0.2780 - val_accuracy: 0.8475 - val_loss: 0.2988 - learning_rate: 0.0050\n",
      "Epoch 223/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8541 - loss: 0.2791\n",
      "Epoch 223: val_loss did not improve from 0.29829\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8541 - loss: 0.2792 - val_accuracy: 0.8450 - val_loss: 0.3032 - learning_rate: 0.0050\n",
      "Epoch 224/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8536 - loss: 0.2829 \n",
      "Epoch 224: val_loss did not improve from 0.29829\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8542 - loss: 0.2819 - val_accuracy: 0.8505 - val_loss: 0.2985 - learning_rate: 0.0050\n",
      "Epoch 225/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8518 - loss: 0.2777 \n",
      "Epoch 225: val_loss improved from 0.29829 to 0.29732, saving model to folds5.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8526 - loss: 0.2784 - val_accuracy: 0.8523 - val_loss: 0.2973 - learning_rate: 0.0050\n",
      "Epoch 226/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8627 - loss: 0.2804 \n",
      "Epoch 226: val_loss did not improve from 0.29732\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8594 - loss: 0.2796 - val_accuracy: 0.8469 - val_loss: 0.3016 - learning_rate: 0.0050\n",
      "Epoch 227/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8516 - loss: 0.2833\n",
      "Epoch 227: val_loss improved from 0.29732 to 0.29688, saving model to folds5.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8519 - loss: 0.2830 - val_accuracy: 0.8542 - val_loss: 0.2969 - learning_rate: 0.0050\n",
      "Epoch 228/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8618 - loss: 0.2701 \n",
      "Epoch 228: val_loss improved from 0.29688 to 0.29545, saving model to folds5.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8584 - loss: 0.2740 - val_accuracy: 0.8609 - val_loss: 0.2955 - learning_rate: 0.0050\n",
      "Epoch 229/300\n",
      "\u001b[1m13/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8595 - loss: 0.2760\n",
      "Epoch 229: val_loss did not improve from 0.29545\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.8589 - loss: 0.2763 - val_accuracy: 0.8585 - val_loss: 0.2986 - learning_rate: 0.0050\n",
      "Epoch 230/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8581 - loss: 0.2788\n",
      "Epoch 230: val_loss improved from 0.29545 to 0.29519, saving model to folds5.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.8573 - loss: 0.2787 - val_accuracy: 0.8603 - val_loss: 0.2952 - learning_rate: 0.0050\n",
      "Epoch 231/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8615 - loss: 0.2804\n",
      "Epoch 231: val_loss did not improve from 0.29519\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.8600 - loss: 0.2799 - val_accuracy: 0.8548 - val_loss: 0.2980 - learning_rate: 0.0050\n",
      "Epoch 232/300\n",
      "\u001b[1m13/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8499 - loss: 0.2851 \n",
      "Epoch 232: val_loss did not improve from 0.29519\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.8504 - loss: 0.2843 - val_accuracy: 0.8597 - val_loss: 0.2964 - learning_rate: 0.0050\n",
      "Epoch 233/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8519 - loss: 0.2845\n",
      "Epoch 233: val_loss did not improve from 0.29519\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.8530 - loss: 0.2830 - val_accuracy: 0.8475 - val_loss: 0.3025 - learning_rate: 0.0050\n",
      "Epoch 234/300\n",
      "\u001b[1m12/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8508 - loss: 0.2844\n",
      "Epoch 234: val_loss did not improve from 0.29519\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8512 - loss: 0.2833 - val_accuracy: 0.8517 - val_loss: 0.3010 - learning_rate: 0.0050\n",
      "Epoch 235/300\n",
      "\u001b[1m13/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8605 - loss: 0.2792 \n",
      "Epoch 235: val_loss did not improve from 0.29519\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8597 - loss: 0.2792 - val_accuracy: 0.8572 - val_loss: 0.2962 - learning_rate: 0.0050\n",
      "Epoch 236/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8583 - loss: 0.2745\n",
      "Epoch 236: val_loss did not improve from 0.29519\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.8571 - loss: 0.2758 - val_accuracy: 0.8505 - val_loss: 0.2953 - learning_rate: 0.0050\n",
      "Epoch 237/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8559 - loss: 0.2776\n",
      "Epoch 237: val_loss did not improve from 0.29519\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.8559 - loss: 0.2776 - val_accuracy: 0.8603 - val_loss: 0.2954 - learning_rate: 0.0050\n",
      "Epoch 238/300\n",
      "\u001b[1m12/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8525 - loss: 0.2795\n",
      "Epoch 238: val_loss did not improve from 0.29519\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.8526 - loss: 0.2788 - val_accuracy: 0.8530 - val_loss: 0.3028 - learning_rate: 0.0050\n",
      "Epoch 239/300\n",
      "\u001b[1m12/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8618 - loss: 0.2712\n",
      "Epoch 239: val_loss did not improve from 0.29519\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.8599 - loss: 0.2733 - val_accuracy: 0.8511 - val_loss: 0.2996 - learning_rate: 0.0050\n",
      "Epoch 240/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8566 - loss: 0.2833\n",
      "Epoch 240: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "\n",
      "Epoch 240: val_loss did not improve from 0.29519\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8566 - loss: 0.2827 - val_accuracy: 0.8469 - val_loss: 0.3001 - learning_rate: 0.0050\n",
      "Epoch 241/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8519 - loss: 0.2810 \n",
      "Epoch 241: val_loss did not improve from 0.29519\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8541 - loss: 0.2790 - val_accuracy: 0.8530 - val_loss: 0.2956 - learning_rate: 0.0025\n",
      "Epoch 242/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8610 - loss: 0.2740\n",
      "Epoch 242: val_loss improved from 0.29519 to 0.29423, saving model to folds5.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8608 - loss: 0.2740 - val_accuracy: 0.8585 - val_loss: 0.2942 - learning_rate: 0.0025\n",
      "Epoch 243/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8562 - loss: 0.2712\n",
      "Epoch 243: val_loss improved from 0.29423 to 0.29254, saving model to folds5.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8563 - loss: 0.2716 - val_accuracy: 0.8615 - val_loss: 0.2925 - learning_rate: 0.0025\n",
      "Epoch 244/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8577 - loss: 0.2764 \n",
      "Epoch 244: val_loss did not improve from 0.29254\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8581 - loss: 0.2748 - val_accuracy: 0.8566 - val_loss: 0.2941 - learning_rate: 0.0025\n",
      "Epoch 245/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8526 - loss: 0.2787 \n",
      "Epoch 245: val_loss improved from 0.29254 to 0.29237, saving model to folds5.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8546 - loss: 0.2772 - val_accuracy: 0.8585 - val_loss: 0.2924 - learning_rate: 0.0025\n",
      "Epoch 246/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8558 - loss: 0.2802 \n",
      "Epoch 246: val_loss did not improve from 0.29237\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8575 - loss: 0.2780 - val_accuracy: 0.8493 - val_loss: 0.2933 - learning_rate: 0.0025\n",
      "Epoch 247/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8505 - loss: 0.2794 \n",
      "Epoch 247: val_loss did not improve from 0.29237\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8524 - loss: 0.2777 - val_accuracy: 0.8566 - val_loss: 0.2931 - learning_rate: 0.0025\n",
      "Epoch 248/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8569 - loss: 0.2709\n",
      "Epoch 248: val_loss improved from 0.29237 to 0.29149, saving model to folds5.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8570 - loss: 0.2710 - val_accuracy: 0.8585 - val_loss: 0.2915 - learning_rate: 0.0025\n",
      "Epoch 249/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8634 - loss: 0.2724 \n",
      "Epoch 249: val_loss did not improve from 0.29149\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8607 - loss: 0.2728 - val_accuracy: 0.8548 - val_loss: 0.2941 - learning_rate: 0.0025\n",
      "Epoch 250/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8568 - loss: 0.2837 \n",
      "Epoch 250: val_loss did not improve from 0.29149\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8566 - loss: 0.2806 - val_accuracy: 0.8523 - val_loss: 0.2950 - learning_rate: 0.0025\n",
      "Epoch 251/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8609 - loss: 0.2644 \n",
      "Epoch 251: val_loss did not improve from 0.29149\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8590 - loss: 0.2680 - val_accuracy: 0.8578 - val_loss: 0.2926 - learning_rate: 0.0025\n",
      "Epoch 252/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8561 - loss: 0.2715\n",
      "Epoch 252: val_loss did not improve from 0.29149\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8562 - loss: 0.2716 - val_accuracy: 0.8621 - val_loss: 0.2916 - learning_rate: 0.0025\n",
      "Epoch 253/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8554 - loss: 0.2804\n",
      "Epoch 253: val_loss did not improve from 0.29149\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8558 - loss: 0.2795 - val_accuracy: 0.8536 - val_loss: 0.2933 - learning_rate: 0.0025\n",
      "Epoch 254/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8475 - loss: 0.2772 \n",
      "Epoch 254: val_loss did not improve from 0.29149\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8516 - loss: 0.2758 - val_accuracy: 0.8523 - val_loss: 0.2930 - learning_rate: 0.0025\n",
      "Epoch 255/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8518 - loss: 0.2677 \n",
      "Epoch 255: val_loss did not improve from 0.29149\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8547 - loss: 0.2696 - val_accuracy: 0.8633 - val_loss: 0.2917 - learning_rate: 0.0025\n",
      "Epoch 256/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8572 - loss: 0.2759\n",
      "Epoch 256: val_loss did not improve from 0.29149\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8573 - loss: 0.2757 - val_accuracy: 0.8560 - val_loss: 0.2938 - learning_rate: 0.0025\n",
      "Epoch 257/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8587 - loss: 0.2741\n",
      "Epoch 257: val_loss did not improve from 0.29149\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8587 - loss: 0.2740 - val_accuracy: 0.8536 - val_loss: 0.2929 - learning_rate: 0.0025\n",
      "Epoch 258/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8527 - loss: 0.2708 \n",
      "Epoch 258: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
      "\n",
      "Epoch 258: val_loss did not improve from 0.29149\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8551 - loss: 0.2716 - val_accuracy: 0.8542 - val_loss: 0.2919 - learning_rate: 0.0025\n",
      "Epoch 259/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8567 - loss: 0.2760\n",
      "Epoch 259: val_loss improved from 0.29149 to 0.29138, saving model to folds5.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8569 - loss: 0.2757 - val_accuracy: 0.8597 - val_loss: 0.2914 - learning_rate: 0.0012\n",
      "Epoch 260/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8633 - loss: 0.2745 \n",
      "Epoch 260: val_loss did not improve from 0.29138\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8623 - loss: 0.2727 - val_accuracy: 0.8603 - val_loss: 0.2919 - learning_rate: 0.0012\n",
      "Epoch 261/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8595 - loss: 0.2742 \n",
      "Epoch 261: val_loss did not improve from 0.29138\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8595 - loss: 0.2731 - val_accuracy: 0.8591 - val_loss: 0.2914 - learning_rate: 0.0012\n",
      "Epoch 262/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8585 - loss: 0.2740\n",
      "Epoch 262: val_loss improved from 0.29138 to 0.29082, saving model to folds5.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8584 - loss: 0.2738 - val_accuracy: 0.8621 - val_loss: 0.2908 - learning_rate: 0.0012\n",
      "Epoch 263/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8588 - loss: 0.2707 \n",
      "Epoch 263: val_loss did not improve from 0.29082\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8595 - loss: 0.2709 - val_accuracy: 0.8548 - val_loss: 0.2925 - learning_rate: 0.0012\n",
      "Epoch 264/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8490 - loss: 0.2713 \n",
      "Epoch 264: val_loss improved from 0.29082 to 0.29050, saving model to folds5.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8519 - loss: 0.2719 - val_accuracy: 0.8627 - val_loss: 0.2905 - learning_rate: 0.0012\n",
      "Epoch 265/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8653 - loss: 0.2652 \n",
      "Epoch 265: val_loss did not improve from 0.29050\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8635 - loss: 0.2675 - val_accuracy: 0.8560 - val_loss: 0.2919 - learning_rate: 0.0012\n",
      "Epoch 266/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8483 - loss: 0.2797  \n",
      "Epoch 266: val_loss did not improve from 0.29050\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8531 - loss: 0.2759 - val_accuracy: 0.8603 - val_loss: 0.2921 - learning_rate: 0.0012\n",
      "Epoch 267/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8545 - loss: 0.2747 \n",
      "Epoch 267: val_loss did not improve from 0.29050\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8551 - loss: 0.2734 - val_accuracy: 0.8591 - val_loss: 0.2908 - learning_rate: 0.0012\n",
      "Epoch 268/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8589 - loss: 0.2692\n",
      "Epoch 268: val_loss improved from 0.29050 to 0.29040, saving model to folds5.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8588 - loss: 0.2694 - val_accuracy: 0.8627 - val_loss: 0.2904 - learning_rate: 0.0012\n",
      "Epoch 269/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8615 - loss: 0.2648 \n",
      "Epoch 269: val_loss did not improve from 0.29040\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8595 - loss: 0.2679 - val_accuracy: 0.8572 - val_loss: 0.2911 - learning_rate: 0.0012\n",
      "Epoch 270/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8570 - loss: 0.2775\n",
      "Epoch 270: val_loss did not improve from 0.29040\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8571 - loss: 0.2771 - val_accuracy: 0.8621 - val_loss: 0.2904 - learning_rate: 0.0012\n",
      "Epoch 271/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8620 - loss: 0.2743 \n",
      "Epoch 271: val_loss did not improve from 0.29040\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8614 - loss: 0.2730 - val_accuracy: 0.8615 - val_loss: 0.2912 - learning_rate: 0.0012\n",
      "Epoch 272/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8628 - loss: 0.2628 \n",
      "Epoch 272: val_loss did not improve from 0.29040\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8602 - loss: 0.2677 - val_accuracy: 0.8603 - val_loss: 0.2913 - learning_rate: 0.0012\n",
      "Epoch 273/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8602 - loss: 0.2701 \n",
      "Epoch 273: val_loss did not improve from 0.29040\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8584 - loss: 0.2715 - val_accuracy: 0.8572 - val_loss: 0.2914 - learning_rate: 0.0012\n",
      "Epoch 274/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8613 - loss: 0.2662\n",
      "Epoch 274: val_loss improved from 0.29040 to 0.29037, saving model to folds5.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8612 - loss: 0.2665 - val_accuracy: 0.8578 - val_loss: 0.2904 - learning_rate: 0.0012\n",
      "Epoch 275/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8558 - loss: 0.2722 \n",
      "Epoch 275: val_loss improved from 0.29037 to 0.29023, saving model to folds5.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8562 - loss: 0.2721 - val_accuracy: 0.8597 - val_loss: 0.2902 - learning_rate: 0.0012\n",
      "Epoch 276/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8584 - loss: 0.2681 \n",
      "Epoch 276: val_loss improved from 0.29023 to 0.29003, saving model to folds5.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8584 - loss: 0.2689 - val_accuracy: 0.8560 - val_loss: 0.2900 - learning_rate: 0.0012\n",
      "Epoch 277/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8663 - loss: 0.2573 \n",
      "Epoch 277: val_loss improved from 0.29003 to 0.28978, saving model to folds5.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8631 - loss: 0.2630 - val_accuracy: 0.8621 - val_loss: 0.2898 - learning_rate: 0.0012\n",
      "Epoch 278/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8624 - loss: 0.2648 \n",
      "Epoch 278: val_loss did not improve from 0.28978\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8619 - loss: 0.2667 - val_accuracy: 0.8591 - val_loss: 0.2906 - learning_rate: 0.0012\n",
      "Epoch 279/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8611 - loss: 0.2660\n",
      "Epoch 279: val_loss did not improve from 0.28978\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8610 - loss: 0.2665 - val_accuracy: 0.8591 - val_loss: 0.2902 - learning_rate: 0.0012\n",
      "Epoch 280/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8560 - loss: 0.2695 \n",
      "Epoch 280: val_loss improved from 0.28978 to 0.28973, saving model to folds5.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8572 - loss: 0.2688 - val_accuracy: 0.8609 - val_loss: 0.2897 - learning_rate: 0.0012\n",
      "Epoch 281/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8575 - loss: 0.2711\n",
      "Epoch 281: val_loss did not improve from 0.28973\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8576 - loss: 0.2711 - val_accuracy: 0.8597 - val_loss: 0.2907 - learning_rate: 0.0012\n",
      "Epoch 282/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8641 - loss: 0.2649 \n",
      "Epoch 282: val_loss did not improve from 0.28973\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8631 - loss: 0.2671 - val_accuracy: 0.8585 - val_loss: 0.2898 - learning_rate: 0.0012\n",
      "Epoch 283/300\n",
      "\u001b[1m12/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8622 - loss: 0.2683\n",
      "Epoch 283: val_loss did not improve from 0.28973\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.8615 - loss: 0.2687 - val_accuracy: 0.8633 - val_loss: 0.2898 - learning_rate: 0.0012\n",
      "Epoch 284/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8627 - loss: 0.2656\n",
      "Epoch 284: val_loss improved from 0.28973 to 0.28929, saving model to folds5.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.8625 - loss: 0.2659 - val_accuracy: 0.8591 - val_loss: 0.2893 - learning_rate: 0.0012\n",
      "Epoch 285/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8617 - loss: 0.2715\n",
      "Epoch 285: val_loss did not improve from 0.28929\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8616 - loss: 0.2713 - val_accuracy: 0.8591 - val_loss: 0.2904 - learning_rate: 0.0012\n",
      "Epoch 286/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8612 - loss: 0.2666\n",
      "Epoch 286: val_loss improved from 0.28929 to 0.28908, saving model to folds5.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.8612 - loss: 0.2678 - val_accuracy: 0.8627 - val_loss: 0.2891 - learning_rate: 0.0012\n",
      "Epoch 287/300\n",
      "\u001b[1m13/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8629 - loss: 0.2686 \n",
      "Epoch 287: val_loss improved from 0.28908 to 0.28898, saving model to folds5.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.8624 - loss: 0.2688 - val_accuracy: 0.8670 - val_loss: 0.2890 - learning_rate: 0.0012\n",
      "Epoch 288/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8612 - loss: 0.2663\n",
      "Epoch 288: val_loss did not improve from 0.28898\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.8618 - loss: 0.2671 - val_accuracy: 0.8627 - val_loss: 0.2891 - learning_rate: 0.0012\n",
      "Epoch 289/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8597 - loss: 0.2701\n",
      "Epoch 289: val_loss improved from 0.28898 to 0.28897, saving model to folds5.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.8598 - loss: 0.2701 - val_accuracy: 0.8664 - val_loss: 0.2890 - learning_rate: 0.0012\n",
      "Epoch 290/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8648 - loss: 0.2673\n",
      "Epoch 290: val_loss did not improve from 0.28897\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.8633 - loss: 0.2688 - val_accuracy: 0.8585 - val_loss: 0.2903 - learning_rate: 0.0012\n",
      "Epoch 291/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8564 - loss: 0.2727\n",
      "Epoch 291: val_loss improved from 0.28897 to 0.28887, saving model to folds5.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.8569 - loss: 0.2723 - val_accuracy: 0.8639 - val_loss: 0.2889 - learning_rate: 0.0012\n",
      "Epoch 292/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8682 - loss: 0.2689 \n",
      "Epoch 292: val_loss improved from 0.28887 to 0.28846, saving model to folds5.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8654 - loss: 0.2689 - val_accuracy: 0.8597 - val_loss: 0.2885 - learning_rate: 0.0012\n",
      "Epoch 293/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8565 - loss: 0.2766\n",
      "Epoch 293: val_loss did not improve from 0.28846\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8568 - loss: 0.2762 - val_accuracy: 0.8694 - val_loss: 0.2889 - learning_rate: 0.0012\n",
      "Epoch 294/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8592 - loss: 0.2733 \n",
      "Epoch 294: val_loss did not improve from 0.28846\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8598 - loss: 0.2724 - val_accuracy: 0.8621 - val_loss: 0.2890 - learning_rate: 0.0012\n",
      "Epoch 295/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8585 - loss: 0.2676  \n",
      "Epoch 295: val_loss did not improve from 0.28846\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8591 - loss: 0.2678 - val_accuracy: 0.8639 - val_loss: 0.2888 - learning_rate: 0.0012\n",
      "Epoch 296/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8634 - loss: 0.2670 \n",
      "Epoch 296: val_loss improved from 0.28846 to 0.28844, saving model to folds5.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8622 - loss: 0.2675 - val_accuracy: 0.8652 - val_loss: 0.2884 - learning_rate: 0.0012\n",
      "Epoch 297/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8638 - loss: 0.2665 \n",
      "Epoch 297: val_loss did not improve from 0.28844\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8634 - loss: 0.2679 - val_accuracy: 0.8542 - val_loss: 0.2907 - learning_rate: 0.0012\n",
      "Epoch 298/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8580 - loss: 0.2723  \n",
      "Epoch 298: val_loss did not improve from 0.28844\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8591 - loss: 0.2709 - val_accuracy: 0.8603 - val_loss: 0.2921 - learning_rate: 0.0012\n",
      "Epoch 299/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8567 - loss: 0.2805\n",
      "Epoch 299: val_loss did not improve from 0.28844\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8569 - loss: 0.2798 - val_accuracy: 0.8627 - val_loss: 0.2894 - learning_rate: 0.0012\n",
      "Epoch 300/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8568 - loss: 0.2742\n",
      "Epoch 300: val_loss improved from 0.28844 to 0.28813, saving model to folds5.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8570 - loss: 0.2738 - val_accuracy: 0.8609 - val_loss: 0.2881 - learning_rate: 0.0012\n",
      "Restoring model weights from the end of the best epoch: 300.\n",
      " Computing LSTM feature importance...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:01<00:00,  5.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- > Fold 7 < ---------------\n",
      "Epoch 1/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7089 - loss: 0.5752\n",
      "Epoch 1: val_loss improved from inf to 0.52417, saving model to folds6.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step - accuracy: 0.7106 - loss: 0.5614 - val_accuracy: 0.7187 - val_loss: 0.5242 - learning_rate: 0.0100\n",
      "Epoch 2/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7154 - loss: 0.5106 \n",
      "Epoch 2: val_loss improved from 0.52417 to 0.50497, saving model to folds6.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7157 - loss: 0.5097 - val_accuracy: 0.7206 - val_loss: 0.5050 - learning_rate: 0.0100\n",
      "Epoch 3/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7221 - loss: 0.4976 \n",
      "Epoch 3: val_loss improved from 0.50497 to 0.49388, saving model to folds6.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7230 - loss: 0.4942 - val_accuracy: 0.7175 - val_loss: 0.4939 - learning_rate: 0.0100\n",
      "Epoch 4/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7250 - loss: 0.4873 \n",
      "Epoch 4: val_loss improved from 0.49388 to 0.49099, saving model to folds6.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7250 - loss: 0.4867 - val_accuracy: 0.7328 - val_loss: 0.4910 - learning_rate: 0.0100\n",
      "Epoch 5/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7283 - loss: 0.4782 \n",
      "Epoch 5: val_loss did not improve from 0.49099\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7291 - loss: 0.4781 - val_accuracy: 0.7206 - val_loss: 0.4911 - learning_rate: 0.0100\n",
      "Epoch 6/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7318 - loss: 0.4759 \n",
      "Epoch 6: val_loss improved from 0.49099 to 0.48943, saving model to folds6.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.7339 - loss: 0.4765 - val_accuracy: 0.7334 - val_loss: 0.4894 - learning_rate: 0.0100\n",
      "Epoch 7/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7363 - loss: 0.4812 \n",
      "Epoch 7: val_loss improved from 0.48943 to 0.47835, saving model to folds6.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7364 - loss: 0.4783 - val_accuracy: 0.7340 - val_loss: 0.4784 - learning_rate: 0.0100\n",
      "Epoch 8/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7345 - loss: 0.4726 \n",
      "Epoch 8: val_loss improved from 0.47835 to 0.47799, saving model to folds6.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7320 - loss: 0.4736 - val_accuracy: 0.7248 - val_loss: 0.4780 - learning_rate: 0.0100\n",
      "Epoch 9/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7232 - loss: 0.4807 \n",
      "Epoch 9: val_loss improved from 0.47799 to 0.47286, saving model to folds6.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7276 - loss: 0.4770 - val_accuracy: 0.7297 - val_loss: 0.4729 - learning_rate: 0.0100\n",
      "Epoch 10/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7350 - loss: 0.4681 \n",
      "Epoch 10: val_loss improved from 0.47286 to 0.46833, saving model to folds6.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7342 - loss: 0.4685 - val_accuracy: 0.7535 - val_loss: 0.4683 - learning_rate: 0.0100\n",
      "Epoch 11/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7450 - loss: 0.4635 \n",
      "Epoch 11: val_loss did not improve from 0.46833\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7431 - loss: 0.4640 - val_accuracy: 0.7480 - val_loss: 0.4699 - learning_rate: 0.0100\n",
      "Epoch 12/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7436 - loss: 0.4572  \n",
      "Epoch 12: val_loss improved from 0.46833 to 0.46707, saving model to folds6.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7436 - loss: 0.4589 - val_accuracy: 0.7474 - val_loss: 0.4671 - learning_rate: 0.0100\n",
      "Epoch 13/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7505 - loss: 0.4630 \n",
      "Epoch 13: val_loss did not improve from 0.46707\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7481 - loss: 0.4630 - val_accuracy: 0.7450 - val_loss: 0.4671 - learning_rate: 0.0100\n",
      "Epoch 14/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7465 - loss: 0.4585 \n",
      "Epoch 14: val_loss did not improve from 0.46707\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7466 - loss: 0.4579 - val_accuracy: 0.7352 - val_loss: 0.4749 - learning_rate: 0.0100\n",
      "Epoch 15/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7498 - loss: 0.4571  \n",
      "Epoch 15: val_loss did not improve from 0.46707\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7466 - loss: 0.4590 - val_accuracy: 0.7486 - val_loss: 0.4671 - learning_rate: 0.0100\n",
      "Epoch 16/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7450 - loss: 0.4585 \n",
      "Epoch 16: val_loss did not improve from 0.46707\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7444 - loss: 0.4596 - val_accuracy: 0.7267 - val_loss: 0.4804 - learning_rate: 0.0100\n",
      "Epoch 17/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7429 - loss: 0.4608\n",
      "Epoch 17: val_loss improved from 0.46707 to 0.46515, saving model to folds6.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.7420 - loss: 0.4609 - val_accuracy: 0.7419 - val_loss: 0.4651 - learning_rate: 0.0100\n",
      "Epoch 18/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7410 - loss: 0.4589\n",
      "Epoch 18: val_loss improved from 0.46515 to 0.46377, saving model to folds6.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.7426 - loss: 0.4589 - val_accuracy: 0.7407 - val_loss: 0.4638 - learning_rate: 0.0100\n",
      "Epoch 19/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7515 - loss: 0.4525\n",
      "Epoch 19: val_loss improved from 0.46377 to 0.46021, saving model to folds6.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.7516 - loss: 0.4523 - val_accuracy: 0.7407 - val_loss: 0.4602 - learning_rate: 0.0100\n",
      "Epoch 20/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7515 - loss: 0.4527\n",
      "Epoch 20: val_loss improved from 0.46021 to 0.45678, saving model to folds6.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.7515 - loss: 0.4527 - val_accuracy: 0.7505 - val_loss: 0.4568 - learning_rate: 0.0100\n",
      "Epoch 21/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7567 - loss: 0.4488\n",
      "Epoch 21: val_loss improved from 0.45678 to 0.45204, saving model to folds6.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.7572 - loss: 0.4472 - val_accuracy: 0.7547 - val_loss: 0.4520 - learning_rate: 0.0100\n",
      "Epoch 22/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7566 - loss: 0.4421\n",
      "Epoch 22: val_loss improved from 0.45204 to 0.44578, saving model to folds6.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.7566 - loss: 0.4420 - val_accuracy: 0.7614 - val_loss: 0.4458 - learning_rate: 0.0100\n",
      "Epoch 23/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7621 - loss: 0.4413\n",
      "Epoch 23: val_loss did not improve from 0.44578\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.7620 - loss: 0.4402 - val_accuracy: 0.7559 - val_loss: 0.4481 - learning_rate: 0.0100\n",
      "Epoch 24/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7654 - loss: 0.4409\n",
      "Epoch 24: val_loss improved from 0.44578 to 0.43922, saving model to folds6.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.7647 - loss: 0.4400 - val_accuracy: 0.7700 - val_loss: 0.4392 - learning_rate: 0.0100\n",
      "Epoch 25/300\n",
      "\u001b[1m12/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7644 - loss: 0.4330\n",
      "Epoch 25: val_loss did not improve from 0.43922\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.7633 - loss: 0.4340 - val_accuracy: 0.7352 - val_loss: 0.4510 - learning_rate: 0.0100\n",
      "Epoch 26/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7446 - loss: 0.4451 \n",
      "Epoch 26: val_loss did not improve from 0.43922\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7484 - loss: 0.4436 - val_accuracy: 0.7480 - val_loss: 0.4564 - learning_rate: 0.0100\n",
      "Epoch 27/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7663 - loss: 0.4384\n",
      "Epoch 27: val_loss did not improve from 0.43922\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7661 - loss: 0.4384 - val_accuracy: 0.7614 - val_loss: 0.4408 - learning_rate: 0.0100\n",
      "Epoch 28/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7588 - loss: 0.4449 \n",
      "Epoch 28: val_loss did not improve from 0.43922\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7584 - loss: 0.4434 - val_accuracy: 0.7480 - val_loss: 0.4566 - learning_rate: 0.0100\n",
      "Epoch 29/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7612 - loss: 0.4339\n",
      "Epoch 29: val_loss did not improve from 0.43922\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7613 - loss: 0.4342 - val_accuracy: 0.7444 - val_loss: 0.4481 - learning_rate: 0.0100\n",
      "Epoch 30/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7420 - loss: 0.4479  \n",
      "Epoch 30: val_loss did not improve from 0.43922\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7458 - loss: 0.4459 - val_accuracy: 0.7553 - val_loss: 0.4424 - learning_rate: 0.0100\n",
      "Epoch 31/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7683 - loss: 0.4220 \n",
      "Epoch 31: val_loss did not improve from 0.43922\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7671 - loss: 0.4259 - val_accuracy: 0.7523 - val_loss: 0.4431 - learning_rate: 0.0100\n",
      "Epoch 32/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7744 - loss: 0.4250 \n",
      "Epoch 32: val_loss improved from 0.43922 to 0.43769, saving model to folds6.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7732 - loss: 0.4274 - val_accuracy: 0.7785 - val_loss: 0.4377 - learning_rate: 0.0100\n",
      "Epoch 33/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7715 - loss: 0.4263 \n",
      "Epoch 33: val_loss did not improve from 0.43769\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7709 - loss: 0.4262 - val_accuracy: 0.7669 - val_loss: 0.4384 - learning_rate: 0.0100\n",
      "Epoch 34/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7722 - loss: 0.4236\n",
      "Epoch 34: val_loss improved from 0.43769 to 0.42661, saving model to folds6.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.7720 - loss: 0.4236 - val_accuracy: 0.7761 - val_loss: 0.4266 - learning_rate: 0.0100\n",
      "Epoch 35/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7715 - loss: 0.4223 \n",
      "Epoch 35: val_loss did not improve from 0.42661\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7718 - loss: 0.4226 - val_accuracy: 0.7730 - val_loss: 0.4416 - learning_rate: 0.0100\n",
      "Epoch 36/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7737 - loss: 0.4266\n",
      "Epoch 36: val_loss improved from 0.42661 to 0.42473, saving model to folds6.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7736 - loss: 0.4264 - val_accuracy: 0.7804 - val_loss: 0.4247 - learning_rate: 0.0100\n",
      "Epoch 37/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7751 - loss: 0.4228 \n",
      "Epoch 37: val_loss improved from 0.42473 to 0.42217, saving model to folds6.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7753 - loss: 0.4212 - val_accuracy: 0.7791 - val_loss: 0.4222 - learning_rate: 0.0100\n",
      "Epoch 38/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7781 - loss: 0.4135\n",
      "Epoch 38: val_loss did not improve from 0.42217\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7778 - loss: 0.4139 - val_accuracy: 0.7785 - val_loss: 0.4258 - learning_rate: 0.0100\n",
      "Epoch 39/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7828 - loss: 0.4129 \n",
      "Epoch 39: val_loss did not improve from 0.42217\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7805 - loss: 0.4154 - val_accuracy: 0.7511 - val_loss: 0.4298 - learning_rate: 0.0100\n",
      "Epoch 40/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7718 - loss: 0.4167 \n",
      "Epoch 40: val_loss did not improve from 0.42217\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7751 - loss: 0.4157 - val_accuracy: 0.7797 - val_loss: 0.4269 - learning_rate: 0.0100\n",
      "Epoch 41/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7844 - loss: 0.4123 \n",
      "Epoch 41: val_loss did not improve from 0.42217\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7834 - loss: 0.4143 - val_accuracy: 0.7822 - val_loss: 0.4270 - learning_rate: 0.0100\n",
      "Epoch 42/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7733 - loss: 0.4244 \n",
      "Epoch 42: val_loss improved from 0.42217 to 0.41913, saving model to folds6.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7753 - loss: 0.4187 - val_accuracy: 0.7846 - val_loss: 0.4191 - learning_rate: 0.0100\n",
      "Epoch 43/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7755 - loss: 0.4089 \n",
      "Epoch 43: val_loss improved from 0.41913 to 0.41447, saving model to folds6.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7758 - loss: 0.4103 - val_accuracy: 0.7846 - val_loss: 0.4145 - learning_rate: 0.0100\n",
      "Epoch 44/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7853 - loss: 0.4092 \n",
      "Epoch 44: val_loss did not improve from 0.41447\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7845 - loss: 0.4095 - val_accuracy: 0.7694 - val_loss: 0.4230 - learning_rate: 0.0100\n",
      "Epoch 45/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7729 - loss: 0.4178 \n",
      "Epoch 45: val_loss improved from 0.41447 to 0.41388, saving model to folds6.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7759 - loss: 0.4144 - val_accuracy: 0.7932 - val_loss: 0.4139 - learning_rate: 0.0100\n",
      "Epoch 46/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7893 - loss: 0.3990 \n",
      "Epoch 46: val_loss did not improve from 0.41388\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7854 - loss: 0.4036 - val_accuracy: 0.7785 - val_loss: 0.4206 - learning_rate: 0.0100\n",
      "Epoch 47/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7782 - loss: 0.4126 \n",
      "Epoch 47: val_loss did not improve from 0.41388\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7816 - loss: 0.4103 - val_accuracy: 0.7749 - val_loss: 0.4192 - learning_rate: 0.0100\n",
      "Epoch 48/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7772 - loss: 0.4137\n",
      "Epoch 48: val_loss improved from 0.41388 to 0.40421, saving model to folds6.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7773 - loss: 0.4133 - val_accuracy: 0.7944 - val_loss: 0.4042 - learning_rate: 0.0100\n",
      "Epoch 49/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7852 - loss: 0.3994 \n",
      "Epoch 49: val_loss did not improve from 0.40421\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7837 - loss: 0.3995 - val_accuracy: 0.7816 - val_loss: 0.4105 - learning_rate: 0.0100\n",
      "Epoch 50/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7855 - loss: 0.4030 \n",
      "Epoch 50: val_loss did not improve from 0.40421\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7838 - loss: 0.4033 - val_accuracy: 0.7627 - val_loss: 0.4304 - learning_rate: 0.0100\n",
      "Epoch 51/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7837 - loss: 0.4042 \n",
      "Epoch 51: val_loss did not improve from 0.40421\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7823 - loss: 0.4055 - val_accuracy: 0.7907 - val_loss: 0.4096 - learning_rate: 0.0100\n",
      "Epoch 52/300\n",
      "\u001b[1m13/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7858 - loss: 0.3970\n",
      "Epoch 52: val_loss improved from 0.40421 to 0.40070, saving model to folds6.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.7859 - loss: 0.3980 - val_accuracy: 0.7950 - val_loss: 0.4007 - learning_rate: 0.0100\n",
      "Epoch 53/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7861 - loss: 0.3947 \n",
      "Epoch 53: val_loss did not improve from 0.40070\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7835 - loss: 0.3959 - val_accuracy: 0.7828 - val_loss: 0.4152 - learning_rate: 0.0100\n",
      "Epoch 54/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7821 - loss: 0.4084 \n",
      "Epoch 54: val_loss did not improve from 0.40070\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7841 - loss: 0.4033 - val_accuracy: 0.7907 - val_loss: 0.4024 - learning_rate: 0.0100\n",
      "Epoch 55/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7919 - loss: 0.3929 \n",
      "Epoch 55: val_loss did not improve from 0.40070\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7911 - loss: 0.3929 - val_accuracy: 0.7919 - val_loss: 0.4073 - learning_rate: 0.0100\n",
      "Epoch 56/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7806 - loss: 0.4026 \n",
      "Epoch 56: val_loss did not improve from 0.40070\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7804 - loss: 0.4016 - val_accuracy: 0.7822 - val_loss: 0.4112 - learning_rate: 0.0100\n",
      "Epoch 57/300\n",
      "\u001b[1m13/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7826 - loss: 0.3989 \n",
      "Epoch 57: val_loss did not improve from 0.40070\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7828 - loss: 0.3985 - val_accuracy: 0.7913 - val_loss: 0.4021 - learning_rate: 0.0100\n",
      "Epoch 58/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7899 - loss: 0.3953\n",
      "Epoch 58: val_loss did not improve from 0.40070\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7902 - loss: 0.3951 - val_accuracy: 0.7871 - val_loss: 0.4041 - learning_rate: 0.0100\n",
      "Epoch 59/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7858 - loss: 0.3960 \n",
      "Epoch 59: val_loss improved from 0.40070 to 0.39800, saving model to folds6.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7873 - loss: 0.3949 - val_accuracy: 0.7974 - val_loss: 0.3980 - learning_rate: 0.0100\n",
      "Epoch 60/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7947 - loss: 0.3837 \n",
      "Epoch 60: val_loss did not improve from 0.39800\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7929 - loss: 0.3872 - val_accuracy: 0.7865 - val_loss: 0.3982 - learning_rate: 0.0100\n",
      "Epoch 61/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7885 - loss: 0.3938\n",
      "Epoch 61: val_loss did not improve from 0.39800\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7887 - loss: 0.3938 - val_accuracy: 0.7913 - val_loss: 0.4011 - learning_rate: 0.0100\n",
      "Epoch 62/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7892 - loss: 0.3914 \n",
      "Epoch 62: val_loss improved from 0.39800 to 0.38687, saving model to folds6.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7907 - loss: 0.3900 - val_accuracy: 0.8103 - val_loss: 0.3869 - learning_rate: 0.0100\n",
      "Epoch 63/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7983 - loss: 0.3850 \n",
      "Epoch 63: val_loss did not improve from 0.38687\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7942 - loss: 0.3876 - val_accuracy: 0.7956 - val_loss: 0.4058 - learning_rate: 0.0100\n",
      "Epoch 64/300\n",
      "\u001b[1m13/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7875 - loss: 0.3908\n",
      "Epoch 64: val_loss did not improve from 0.38687\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7879 - loss: 0.3907 - val_accuracy: 0.7987 - val_loss: 0.3997 - learning_rate: 0.0100\n",
      "Epoch 65/300\n",
      "\u001b[1m13/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7902 - loss: 0.3909 \n",
      "Epoch 65: val_loss did not improve from 0.38687\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7899 - loss: 0.3908 - val_accuracy: 0.7944 - val_loss: 0.3901 - learning_rate: 0.0100\n",
      "Epoch 66/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7833 - loss: 0.3946\n",
      "Epoch 66: val_loss did not improve from 0.38687\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7837 - loss: 0.3943 - val_accuracy: 0.8048 - val_loss: 0.3965 - learning_rate: 0.0100\n",
      "Epoch 67/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7930 - loss: 0.3867 \n",
      "Epoch 67: val_loss did not improve from 0.38687\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7921 - loss: 0.3875 - val_accuracy: 0.7871 - val_loss: 0.3955 - learning_rate: 0.0100\n",
      "Epoch 68/300\n",
      "\u001b[1m13/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7863 - loss: 0.3981\n",
      "Epoch 68: val_loss did not improve from 0.38687\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7879 - loss: 0.3961 - val_accuracy: 0.7956 - val_loss: 0.3950 - learning_rate: 0.0100\n",
      "Epoch 69/300\n",
      "\u001b[1m13/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7961 - loss: 0.3899\n",
      "Epoch 69: val_loss did not improve from 0.38687\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.7959 - loss: 0.3890 - val_accuracy: 0.8066 - val_loss: 0.3880 - learning_rate: 0.0100\n",
      "Epoch 70/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7896 - loss: 0.3895\n",
      "Epoch 70: val_loss did not improve from 0.38687\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.7900 - loss: 0.3892 - val_accuracy: 0.7987 - val_loss: 0.3975 - learning_rate: 0.0100\n",
      "Epoch 71/300\n",
      "\u001b[1m13/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7957 - loss: 0.3863\n",
      "Epoch 71: val_loss did not improve from 0.38687\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.7949 - loss: 0.3865 - val_accuracy: 0.8035 - val_loss: 0.3940 - learning_rate: 0.0100\n",
      "Epoch 72/300\n",
      "\u001b[1m13/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7955 - loss: 0.3863\n",
      "Epoch 72: val_loss improved from 0.38687 to 0.38180, saving model to folds6.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.7952 - loss: 0.3865 - val_accuracy: 0.8035 - val_loss: 0.3818 - learning_rate: 0.0100\n",
      "Epoch 73/300\n",
      "\u001b[1m12/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7945 - loss: 0.3864\n",
      "Epoch 73: val_loss did not improve from 0.38180\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.7939 - loss: 0.3870 - val_accuracy: 0.7974 - val_loss: 0.3930 - learning_rate: 0.0100\n",
      "Epoch 74/300\n",
      "\u001b[1m13/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7841 - loss: 0.3938\n",
      "Epoch 74: val_loss did not improve from 0.38180\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.7840 - loss: 0.3939 - val_accuracy: 0.7926 - val_loss: 0.3989 - learning_rate: 0.0100\n",
      "Epoch 75/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7875 - loss: 0.3867\n",
      "Epoch 75: val_loss did not improve from 0.38180\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.7877 - loss: 0.3867 - val_accuracy: 0.7968 - val_loss: 0.3904 - learning_rate: 0.0100\n",
      "Epoch 76/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7940 - loss: 0.3849\n",
      "Epoch 76: val_loss did not improve from 0.38180\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.7940 - loss: 0.3847 - val_accuracy: 0.8060 - val_loss: 0.3857 - learning_rate: 0.0100\n",
      "Epoch 77/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7950 - loss: 0.3871\n",
      "Epoch 77: val_loss did not improve from 0.38180\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.7949 - loss: 0.3869 - val_accuracy: 0.7944 - val_loss: 0.3922 - learning_rate: 0.0100\n",
      "Epoch 78/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7939 - loss: 0.3852  \n",
      "Epoch 78: val_loss did not improve from 0.38180\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7943 - loss: 0.3844 - val_accuracy: 0.7968 - val_loss: 0.3878 - learning_rate: 0.0100\n",
      "Epoch 79/300\n",
      "\u001b[1m12/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7845 - loss: 0.3944 \n",
      "Epoch 79: val_loss did not improve from 0.38180\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7857 - loss: 0.3921 - val_accuracy: 0.7895 - val_loss: 0.3997 - learning_rate: 0.0100\n",
      "Epoch 80/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7984 - loss: 0.3851 \n",
      "Epoch 80: val_loss did not improve from 0.38180\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8000 - loss: 0.3807 - val_accuracy: 0.8017 - val_loss: 0.3828 - learning_rate: 0.0100\n",
      "Epoch 81/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8011 - loss: 0.3768\n",
      "Epoch 81: val_loss improved from 0.38180 to 0.37840, saving model to folds6.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8012 - loss: 0.3766 - val_accuracy: 0.8072 - val_loss: 0.3784 - learning_rate: 0.0100\n",
      "Epoch 82/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8023 - loss: 0.3748\n",
      "Epoch 82: val_loss improved from 0.37840 to 0.37720, saving model to folds6.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8024 - loss: 0.3748 - val_accuracy: 0.8011 - val_loss: 0.3772 - learning_rate: 0.0100\n",
      "Epoch 83/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8070 - loss: 0.3665\n",
      "Epoch 83: val_loss improved from 0.37720 to 0.37412, saving model to folds6.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8054 - loss: 0.3676 - val_accuracy: 0.8133 - val_loss: 0.3741 - learning_rate: 0.0100\n",
      "Epoch 84/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8018 - loss: 0.3816 \n",
      "Epoch 84: val_loss did not improve from 0.37412\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8000 - loss: 0.3777 - val_accuracy: 0.8084 - val_loss: 0.3748 - learning_rate: 0.0100\n",
      "Epoch 85/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8032 - loss: 0.3671\n",
      "Epoch 85: val_loss did not improve from 0.37412\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8030 - loss: 0.3674 - val_accuracy: 0.8035 - val_loss: 0.3831 - learning_rate: 0.0100\n",
      "Epoch 86/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7972 - loss: 0.3760\n",
      "Epoch 86: val_loss did not improve from 0.37412\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7975 - loss: 0.3757 - val_accuracy: 0.7938 - val_loss: 0.3859 - learning_rate: 0.0100\n",
      "Epoch 87/300\n",
      "\u001b[1m13/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7920 - loss: 0.3787\n",
      "Epoch 87: val_loss did not improve from 0.37412\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7918 - loss: 0.3790 - val_accuracy: 0.7816 - val_loss: 0.3923 - learning_rate: 0.0100\n",
      "Epoch 88/300\n",
      "\u001b[1m13/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8000 - loss: 0.3721\n",
      "Epoch 88: val_loss improved from 0.37412 to 0.37213, saving model to folds6.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8003 - loss: 0.3720 - val_accuracy: 0.8115 - val_loss: 0.3721 - learning_rate: 0.0100\n",
      "Epoch 89/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8127 - loss: 0.3635\n",
      "Epoch 89: val_loss improved from 0.37213 to 0.37191, saving model to folds6.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8123 - loss: 0.3637 - val_accuracy: 0.8035 - val_loss: 0.3719 - learning_rate: 0.0100\n",
      "Epoch 90/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7980 - loss: 0.3747\n",
      "Epoch 90: val_loss improved from 0.37191 to 0.36970, saving model to folds6.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.7987 - loss: 0.3738 - val_accuracy: 0.8139 - val_loss: 0.3697 - learning_rate: 0.0100\n",
      "Epoch 91/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8087 - loss: 0.3631\n",
      "Epoch 91: val_loss improved from 0.36970 to 0.36677, saving model to folds6.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8085 - loss: 0.3633 - val_accuracy: 0.8096 - val_loss: 0.3668 - learning_rate: 0.0100\n",
      "Epoch 92/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7999 - loss: 0.3726 \n",
      "Epoch 92: val_loss did not improve from 0.36677\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8019 - loss: 0.3705 - val_accuracy: 0.8017 - val_loss: 0.3805 - learning_rate: 0.0100\n",
      "Epoch 93/300\n",
      "\u001b[1m12/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7984 - loss: 0.3690\n",
      "Epoch 93: val_loss did not improve from 0.36677\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7991 - loss: 0.3688 - val_accuracy: 0.8011 - val_loss: 0.3728 - learning_rate: 0.0100\n",
      "Epoch 94/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8035 - loss: 0.3680 \n",
      "Epoch 94: val_loss did not improve from 0.36677\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8042 - loss: 0.3679 - val_accuracy: 0.8005 - val_loss: 0.3842 - learning_rate: 0.0100\n",
      "Epoch 95/300\n",
      "\u001b[1m12/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8108 - loss: 0.3643\n",
      "Epoch 95: val_loss did not improve from 0.36677\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8099 - loss: 0.3649 - val_accuracy: 0.8017 - val_loss: 0.3739 - learning_rate: 0.0100\n",
      "Epoch 96/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8022 - loss: 0.3646\n",
      "Epoch 96: val_loss did not improve from 0.36677\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8025 - loss: 0.3644 - val_accuracy: 0.8060 - val_loss: 0.3679 - learning_rate: 0.0100\n",
      "Epoch 97/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8127 - loss: 0.3615\n",
      "Epoch 97: val_loss did not improve from 0.36677\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8123 - loss: 0.3617 - val_accuracy: 0.8096 - val_loss: 0.3786 - learning_rate: 0.0100\n",
      "Epoch 98/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8021 - loss: 0.3646\n",
      "Epoch 98: val_loss did not improve from 0.36677\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8023 - loss: 0.3645 - val_accuracy: 0.7950 - val_loss: 0.3808 - learning_rate: 0.0100\n",
      "Epoch 99/300\n",
      "\u001b[1m12/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8019 - loss: 0.3760\n",
      "Epoch 99: val_loss did not improve from 0.36677\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8022 - loss: 0.3750 - val_accuracy: 0.7877 - val_loss: 0.3792 - learning_rate: 0.0100\n",
      "Epoch 100/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8073 - loss: 0.3709\n",
      "Epoch 100: val_loss did not improve from 0.36677\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8075 - loss: 0.3707 - val_accuracy: 0.7968 - val_loss: 0.3780 - learning_rate: 0.0100\n",
      "Epoch 101/300\n",
      "\u001b[1m13/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8054 - loss: 0.3702\n",
      "Epoch 101: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
      "\n",
      "Epoch 101: val_loss did not improve from 0.36677\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8057 - loss: 0.3696 - val_accuracy: 0.7858 - val_loss: 0.3793 - learning_rate: 0.0100\n",
      "Epoch 102/300\n",
      "\u001b[1m12/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8001 - loss: 0.3673\n",
      "Epoch 102: val_loss did not improve from 0.36677\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8021 - loss: 0.3664 - val_accuracy: 0.8127 - val_loss: 0.3758 - learning_rate: 0.0050\n",
      "Epoch 103/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8126 - loss: 0.3616\n",
      "Epoch 103: val_loss improved from 0.36677 to 0.36541, saving model to folds6.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.8127 - loss: 0.3615 - val_accuracy: 0.8103 - val_loss: 0.3654 - learning_rate: 0.0050\n",
      "Epoch 104/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8042 - loss: 0.3607\n",
      "Epoch 104: val_loss improved from 0.36541 to 0.36382, saving model to folds6.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8048 - loss: 0.3606 - val_accuracy: 0.8109 - val_loss: 0.3638 - learning_rate: 0.0050\n",
      "Epoch 105/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8148 - loss: 0.3567\n",
      "Epoch 105: val_loss did not improve from 0.36382\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8148 - loss: 0.3567 - val_accuracy: 0.7974 - val_loss: 0.3764 - learning_rate: 0.0050\n",
      "Epoch 106/300\n",
      "\u001b[1m13/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8107 - loss: 0.3608\n",
      "Epoch 106: val_loss did not improve from 0.36382\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8118 - loss: 0.3601 - val_accuracy: 0.8090 - val_loss: 0.3671 - learning_rate: 0.0050\n",
      "Epoch 107/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8100 - loss: 0.3573\n",
      "Epoch 107: val_loss did not improve from 0.36382\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8103 - loss: 0.3571 - val_accuracy: 0.8170 - val_loss: 0.3702 - learning_rate: 0.0050\n",
      "Epoch 108/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8164 - loss: 0.3575\n",
      "Epoch 108: val_loss did not improve from 0.36382\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8162 - loss: 0.3572 - val_accuracy: 0.8109 - val_loss: 0.3701 - learning_rate: 0.0050\n",
      "Epoch 109/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8147 - loss: 0.3555\n",
      "Epoch 109: val_loss improved from 0.36382 to 0.36361, saving model to folds6.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8147 - loss: 0.3558 - val_accuracy: 0.8127 - val_loss: 0.3636 - learning_rate: 0.0050\n",
      "Epoch 110/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8152 - loss: 0.3569\n",
      "Epoch 110: val_loss did not improve from 0.36361\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8155 - loss: 0.3569 - val_accuracy: 0.8121 - val_loss: 0.3640 - learning_rate: 0.0050\n",
      "Epoch 111/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8121 - loss: 0.3572\n",
      "Epoch 111: val_loss improved from 0.36361 to 0.36134, saving model to folds6.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.8122 - loss: 0.3572 - val_accuracy: 0.8170 - val_loss: 0.3613 - learning_rate: 0.0050\n",
      "Epoch 112/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8202 - loss: 0.3526\n",
      "Epoch 112: val_loss did not improve from 0.36134\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8200 - loss: 0.3526 - val_accuracy: 0.8115 - val_loss: 0.3649 - learning_rate: 0.0050\n",
      "Epoch 113/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8160 - loss: 0.3574\n",
      "Epoch 113: val_loss did not improve from 0.36134\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8161 - loss: 0.3571 - val_accuracy: 0.8188 - val_loss: 0.3660 - learning_rate: 0.0050\n",
      "Epoch 114/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8153 - loss: 0.3556\n",
      "Epoch 114: val_loss improved from 0.36134 to 0.35635, saving model to folds6.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8149 - loss: 0.3555 - val_accuracy: 0.8237 - val_loss: 0.3563 - learning_rate: 0.0050\n",
      "Epoch 115/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8176 - loss: 0.3532\n",
      "Epoch 115: val_loss did not improve from 0.35635\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8175 - loss: 0.3530 - val_accuracy: 0.8054 - val_loss: 0.3613 - learning_rate: 0.0050\n",
      "Epoch 116/300\n",
      "\u001b[1m12/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8135 - loss: 0.3496\n",
      "Epoch 116: val_loss did not improve from 0.35635\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8135 - loss: 0.3503 - val_accuracy: 0.8048 - val_loss: 0.3677 - learning_rate: 0.0050\n",
      "Epoch 117/300\n",
      "\u001b[1m12/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8135 - loss: 0.3549\n",
      "Epoch 117: val_loss did not improve from 0.35635\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.8142 - loss: 0.3541 - val_accuracy: 0.8011 - val_loss: 0.3780 - learning_rate: 0.0050\n",
      "Epoch 118/300\n",
      "\u001b[1m12/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8167 - loss: 0.3531\n",
      "Epoch 118: val_loss did not improve from 0.35635\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.8165 - loss: 0.3533 - val_accuracy: 0.8041 - val_loss: 0.3740 - learning_rate: 0.0050\n",
      "Epoch 119/300\n",
      "\u001b[1m13/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8141 - loss: 0.3538\n",
      "Epoch 119: val_loss did not improve from 0.35635\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.8140 - loss: 0.3535 - val_accuracy: 0.8115 - val_loss: 0.3579 - learning_rate: 0.0050\n",
      "Epoch 120/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8123 - loss: 0.3511\n",
      "Epoch 120: val_loss did not improve from 0.35635\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.8123 - loss: 0.3509 - val_accuracy: 0.8182 - val_loss: 0.3603 - learning_rate: 0.0050\n",
      "Epoch 121/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8136 - loss: 0.3548\n",
      "Epoch 121: val_loss did not improve from 0.35635\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.8138 - loss: 0.3540 - val_accuracy: 0.8109 - val_loss: 0.3579 - learning_rate: 0.0050\n",
      "Epoch 122/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8245 - loss: 0.3437\n",
      "Epoch 122: val_loss did not improve from 0.35635\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.8219 - loss: 0.3461 - val_accuracy: 0.8096 - val_loss: 0.3650 - learning_rate: 0.0050\n",
      "Epoch 123/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8130 - loss: 0.3540\n",
      "Epoch 123: val_loss did not improve from 0.35635\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.8137 - loss: 0.3529 - val_accuracy: 0.8072 - val_loss: 0.3621 - learning_rate: 0.0050\n",
      "Epoch 124/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8263 - loss: 0.3415\n",
      "Epoch 124: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "\n",
      "Epoch 124: val_loss did not improve from 0.35635\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8238 - loss: 0.3436 - val_accuracy: 0.8103 - val_loss: 0.3567 - learning_rate: 0.0050\n",
      "Epoch 125/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8181 - loss: 0.3459\n",
      "Epoch 125: val_loss did not improve from 0.35635\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8183 - loss: 0.3459 - val_accuracy: 0.8164 - val_loss: 0.3573 - learning_rate: 0.0025\n",
      "Epoch 126/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8225 - loss: 0.3429 \n",
      "Epoch 126: val_loss improved from 0.35635 to 0.35336, saving model to folds6.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8223 - loss: 0.3438 - val_accuracy: 0.8151 - val_loss: 0.3534 - learning_rate: 0.0025\n",
      "Epoch 127/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8179 - loss: 0.3455 \n",
      "Epoch 127: val_loss improved from 0.35336 to 0.35048, saving model to folds6.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8197 - loss: 0.3445 - val_accuracy: 0.8225 - val_loss: 0.3505 - learning_rate: 0.0025\n",
      "Epoch 128/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8244 - loss: 0.3370  \n",
      "Epoch 128: val_loss did not improve from 0.35048\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8228 - loss: 0.3394 - val_accuracy: 0.8188 - val_loss: 0.3534 - learning_rate: 0.0025\n",
      "Epoch 129/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8213 - loss: 0.3416\n",
      "Epoch 129: val_loss did not improve from 0.35048\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8212 - loss: 0.3418 - val_accuracy: 0.8243 - val_loss: 0.3537 - learning_rate: 0.0025\n",
      "Epoch 130/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8230 - loss: 0.3473\n",
      "Epoch 130: val_loss did not improve from 0.35048\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8231 - loss: 0.3468 - val_accuracy: 0.8176 - val_loss: 0.3538 - learning_rate: 0.0025\n",
      "Epoch 131/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8193 - loss: 0.3472 \n",
      "Epoch 131: val_loss did not improve from 0.35048\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8208 - loss: 0.3460 - val_accuracy: 0.8157 - val_loss: 0.3507 - learning_rate: 0.0025\n",
      "Epoch 132/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8219 - loss: 0.3413 \n",
      "Epoch 132: val_loss did not improve from 0.35048\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8216 - loss: 0.3426 - val_accuracy: 0.8157 - val_loss: 0.3584 - learning_rate: 0.0025\n",
      "Epoch 133/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8216 - loss: 0.3489 \n",
      "Epoch 133: val_loss did not improve from 0.35048\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8216 - loss: 0.3468 - val_accuracy: 0.8188 - val_loss: 0.3529 - learning_rate: 0.0025\n",
      "Epoch 134/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8154 - loss: 0.3489\n",
      "Epoch 134: val_loss did not improve from 0.35048\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8158 - loss: 0.3485 - val_accuracy: 0.8225 - val_loss: 0.3585 - learning_rate: 0.0025\n",
      "Epoch 135/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8216 - loss: 0.3446\n",
      "Epoch 135: val_loss improved from 0.35048 to 0.34961, saving model to folds6.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8218 - loss: 0.3445 - val_accuracy: 0.8304 - val_loss: 0.3496 - learning_rate: 0.0025\n",
      "Epoch 136/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8286 - loss: 0.3403 \n",
      "Epoch 136: val_loss did not improve from 0.34961\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8273 - loss: 0.3412 - val_accuracy: 0.8176 - val_loss: 0.3545 - learning_rate: 0.0025\n",
      "Epoch 137/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8317 - loss: 0.3404 \n",
      "Epoch 137: val_loss improved from 0.34961 to 0.34888, saving model to folds6.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8291 - loss: 0.3411 - val_accuracy: 0.8225 - val_loss: 0.3489 - learning_rate: 0.0025\n",
      "Epoch 138/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8206 - loss: 0.3451 \n",
      "Epoch 138: val_loss did not improve from 0.34888\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8226 - loss: 0.3426 - val_accuracy: 0.8170 - val_loss: 0.3518 - learning_rate: 0.0025\n",
      "Epoch 139/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8245 - loss: 0.3402  \n",
      "Epoch 139: val_loss did not improve from 0.34888\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8227 - loss: 0.3409 - val_accuracy: 0.8176 - val_loss: 0.3505 - learning_rate: 0.0025\n",
      "Epoch 140/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8223 - loss: 0.3446\n",
      "Epoch 140: val_loss did not improve from 0.34888\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8224 - loss: 0.3444 - val_accuracy: 0.8151 - val_loss: 0.3523 - learning_rate: 0.0025\n",
      "Epoch 141/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8268 - loss: 0.3381\n",
      "Epoch 141: val_loss improved from 0.34888 to 0.34606, saving model to folds6.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8267 - loss: 0.3382 - val_accuracy: 0.8279 - val_loss: 0.3461 - learning_rate: 0.0025\n",
      "Epoch 142/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8281 - loss: 0.3337 \n",
      "Epoch 142: val_loss did not improve from 0.34606\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8242 - loss: 0.3372 - val_accuracy: 0.8237 - val_loss: 0.3531 - learning_rate: 0.0025\n",
      "Epoch 143/300\n",
      "\u001b[1m13/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8236 - loss: 0.3448\n",
      "Epoch 143: val_loss did not improve from 0.34606\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8237 - loss: 0.3441 - val_accuracy: 0.8176 - val_loss: 0.3481 - learning_rate: 0.0025\n",
      "Epoch 144/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8207 - loss: 0.3379 \n",
      "Epoch 144: val_loss did not improve from 0.34606\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8221 - loss: 0.3385 - val_accuracy: 0.8133 - val_loss: 0.3525 - learning_rate: 0.0025\n",
      "Epoch 145/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8270 - loss: 0.3361\n",
      "Epoch 145: val_loss did not improve from 0.34606\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8269 - loss: 0.3362 - val_accuracy: 0.8103 - val_loss: 0.3469 - learning_rate: 0.0025\n",
      "Epoch 146/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8196 - loss: 0.3344 \n",
      "Epoch 146: val_loss improved from 0.34606 to 0.34371, saving model to folds6.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8208 - loss: 0.3361 - val_accuracy: 0.8231 - val_loss: 0.3437 - learning_rate: 0.0025\n",
      "Epoch 147/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8205 - loss: 0.3427  \n",
      "Epoch 147: val_loss did not improve from 0.34371\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8221 - loss: 0.3411 - val_accuracy: 0.8225 - val_loss: 0.3454 - learning_rate: 0.0025\n",
      "Epoch 148/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8308 - loss: 0.3341 \n",
      "Epoch 148: val_loss did not improve from 0.34371\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8282 - loss: 0.3360 - val_accuracy: 0.8225 - val_loss: 0.3542 - learning_rate: 0.0025\n",
      "Epoch 149/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8221 - loss: 0.3467 \n",
      "Epoch 149: val_loss improved from 0.34371 to 0.34341, saving model to folds6.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8234 - loss: 0.3430 - val_accuracy: 0.8267 - val_loss: 0.3434 - learning_rate: 0.0025\n",
      "Epoch 150/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8238 - loss: 0.3427 \n",
      "Epoch 150: val_loss did not improve from 0.34341\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8242 - loss: 0.3398 - val_accuracy: 0.8206 - val_loss: 0.3462 - learning_rate: 0.0025\n",
      "Epoch 151/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8219 - loss: 0.3409 \n",
      "Epoch 151: val_loss improved from 0.34341 to 0.34146, saving model to folds6.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8239 - loss: 0.3389 - val_accuracy: 0.8261 - val_loss: 0.3415 - learning_rate: 0.0025\n",
      "Epoch 152/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8217 - loss: 0.3409\n",
      "Epoch 152: val_loss did not improve from 0.34146\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8218 - loss: 0.3407 - val_accuracy: 0.8231 - val_loss: 0.3523 - learning_rate: 0.0025\n",
      "Epoch 153/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8269 - loss: 0.3414\n",
      "Epoch 153: val_loss did not improve from 0.34146\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8268 - loss: 0.3412 - val_accuracy: 0.8255 - val_loss: 0.3451 - learning_rate: 0.0025\n",
      "Epoch 154/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8326 - loss: 0.3352 \n",
      "Epoch 154: val_loss did not improve from 0.34146\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8296 - loss: 0.3361 - val_accuracy: 0.8206 - val_loss: 0.3417 - learning_rate: 0.0025\n",
      "Epoch 155/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8229 - loss: 0.3390 \n",
      "Epoch 155: val_loss did not improve from 0.34146\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8254 - loss: 0.3379 - val_accuracy: 0.8200 - val_loss: 0.3424 - learning_rate: 0.0025\n",
      "Epoch 156/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8213 - loss: 0.3364 \n",
      "Epoch 156: val_loss did not improve from 0.34146\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8208 - loss: 0.3383 - val_accuracy: 0.8151 - val_loss: 0.3477 - learning_rate: 0.0025\n",
      "Epoch 157/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8254 - loss: 0.3348\n",
      "Epoch 157: val_loss improved from 0.34146 to 0.34093, saving model to folds6.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8253 - loss: 0.3350 - val_accuracy: 0.8231 - val_loss: 0.3409 - learning_rate: 0.0025\n",
      "Epoch 158/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8219 - loss: 0.3303 \n",
      "Epoch 158: val_loss did not improve from 0.34093\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8226 - loss: 0.3327 - val_accuracy: 0.8255 - val_loss: 0.3438 - learning_rate: 0.0025\n",
      "Epoch 159/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8259 - loss: 0.3389\n",
      "Epoch 159: val_loss did not improve from 0.34093\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8257 - loss: 0.3388 - val_accuracy: 0.8267 - val_loss: 0.3413 - learning_rate: 0.0025\n",
      "Epoch 160/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8353 - loss: 0.3304 \n",
      "Epoch 160: val_loss did not improve from 0.34093\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8309 - loss: 0.3322 - val_accuracy: 0.8249 - val_loss: 0.3416 - learning_rate: 0.0025\n",
      "Epoch 161/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8259 - loss: 0.3324\n",
      "Epoch 161: val_loss did not improve from 0.34093\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8259 - loss: 0.3326 - val_accuracy: 0.8182 - val_loss: 0.3457 - learning_rate: 0.0025\n",
      "Epoch 162/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8244 - loss: 0.3392 \n",
      "Epoch 162: val_loss did not improve from 0.34093\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8224 - loss: 0.3377 - val_accuracy: 0.8231 - val_loss: 0.3427 - learning_rate: 0.0025\n",
      "Epoch 163/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8238 - loss: 0.3351\n",
      "Epoch 163: val_loss did not improve from 0.34093\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8239 - loss: 0.3351 - val_accuracy: 0.8243 - val_loss: 0.3419 - learning_rate: 0.0025\n",
      "Epoch 164/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8236 - loss: 0.3334 \n",
      "Epoch 164: val_loss did not improve from 0.34093\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8235 - loss: 0.3342 - val_accuracy: 0.8237 - val_loss: 0.3415 - learning_rate: 0.0025\n",
      "Epoch 165/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8245 - loss: 0.3348\n",
      "Epoch 165: val_loss improved from 0.34093 to 0.34082, saving model to folds6.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8245 - loss: 0.3348 - val_accuracy: 0.8255 - val_loss: 0.3408 - learning_rate: 0.0025\n",
      "Epoch 166/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8309 - loss: 0.3291 \n",
      "Epoch 166: val_loss improved from 0.34082 to 0.33900, saving model to folds6.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8289 - loss: 0.3312 - val_accuracy: 0.8267 - val_loss: 0.3390 - learning_rate: 0.0025\n",
      "Epoch 167/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8232 - loss: 0.3361\n",
      "Epoch 167: val_loss did not improve from 0.33900\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8234 - loss: 0.3360 - val_accuracy: 0.8212 - val_loss: 0.3426 - learning_rate: 0.0025\n",
      "Epoch 168/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8311 - loss: 0.3311\n",
      "Epoch 168: val_loss did not improve from 0.33900\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.8309 - loss: 0.3313 - val_accuracy: 0.8206 - val_loss: 0.3416 - learning_rate: 0.0025\n",
      "Epoch 169/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8203 - loss: 0.3373\n",
      "Epoch 169: val_loss did not improve from 0.33900\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.8220 - loss: 0.3367 - val_accuracy: 0.8225 - val_loss: 0.3477 - learning_rate: 0.0025\n",
      "Epoch 170/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8280 - loss: 0.3323\n",
      "Epoch 170: val_loss did not improve from 0.33900\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.8271 - loss: 0.3339 - val_accuracy: 0.8206 - val_loss: 0.3477 - learning_rate: 0.0025\n",
      "Epoch 171/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8273 - loss: 0.3306\n",
      "Epoch 171: val_loss improved from 0.33900 to 0.33837, saving model to folds6.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.8263 - loss: 0.3313 - val_accuracy: 0.8316 - val_loss: 0.3384 - learning_rate: 0.0025\n",
      "Epoch 172/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8292 - loss: 0.3280\n",
      "Epoch 172: val_loss did not improve from 0.33837\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.8290 - loss: 0.3283 - val_accuracy: 0.8267 - val_loss: 0.3405 - learning_rate: 0.0025\n",
      "Epoch 173/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8270 - loss: 0.3331\n",
      "Epoch 173: val_loss did not improve from 0.33837\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.8269 - loss: 0.3331 - val_accuracy: 0.8347 - val_loss: 0.3390 - learning_rate: 0.0025\n",
      "Epoch 174/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8307 - loss: 0.3295\n",
      "Epoch 174: val_loss did not improve from 0.33837\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.8305 - loss: 0.3297 - val_accuracy: 0.8176 - val_loss: 0.3467 - learning_rate: 0.0025\n",
      "Epoch 175/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8368 - loss: 0.3220\n",
      "Epoch 175: val_loss did not improve from 0.33837\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.8339 - loss: 0.3258 - val_accuracy: 0.8304 - val_loss: 0.3402 - learning_rate: 0.0025\n",
      "Epoch 176/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8312 - loss: 0.3275\n",
      "Epoch 176: val_loss did not improve from 0.33837\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.8306 - loss: 0.3291 - val_accuracy: 0.8249 - val_loss: 0.3411 - learning_rate: 0.0025\n",
      "Epoch 177/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8236 - loss: 0.3307\n",
      "Epoch 177: val_loss did not improve from 0.33837\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8233 - loss: 0.3311 - val_accuracy: 0.8212 - val_loss: 0.3432 - learning_rate: 0.0025\n",
      "Epoch 178/300\n",
      "\u001b[1m13/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8222 - loss: 0.3326\n",
      "Epoch 178: val_loss did not improve from 0.33837\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8221 - loss: 0.3328 - val_accuracy: 0.8218 - val_loss: 0.3435 - learning_rate: 0.0025\n",
      "Epoch 179/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8369 - loss: 0.3253\n",
      "Epoch 179: val_loss did not improve from 0.33837\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.8345 - loss: 0.3285 - val_accuracy: 0.8279 - val_loss: 0.3410 - learning_rate: 0.0025\n",
      "Epoch 180/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8280 - loss: 0.3329\n",
      "Epoch 180: val_loss did not improve from 0.33837\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.8273 - loss: 0.3329 - val_accuracy: 0.8310 - val_loss: 0.3393 - learning_rate: 0.0025\n",
      "Epoch 181/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8231 - loss: 0.3334\n",
      "Epoch 181: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
      "\n",
      "Epoch 181: val_loss did not improve from 0.33837\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.8233 - loss: 0.3332 - val_accuracy: 0.8237 - val_loss: 0.3418 - learning_rate: 0.0025\n",
      "Epoch 182/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8293 - loss: 0.3296 \n",
      "Epoch 182: val_loss did not improve from 0.33837\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8279 - loss: 0.3295 - val_accuracy: 0.8200 - val_loss: 0.3406 - learning_rate: 0.0012\n",
      "Epoch 183/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8319 - loss: 0.3271\n",
      "Epoch 183: val_loss did not improve from 0.33837\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8313 - loss: 0.3276 - val_accuracy: 0.8170 - val_loss: 0.3427 - learning_rate: 0.0012\n",
      "Epoch 184/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8218 - loss: 0.3397 \n",
      "Epoch 184: val_loss improved from 0.33837 to 0.33611, saving model to folds6.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8253 - loss: 0.3360 - val_accuracy: 0.8225 - val_loss: 0.3361 - learning_rate: 0.0012\n",
      "Epoch 185/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8290 - loss: 0.3262 \n",
      "Epoch 185: val_loss did not improve from 0.33611\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8279 - loss: 0.3279 - val_accuracy: 0.8298 - val_loss: 0.3372 - learning_rate: 0.0012\n",
      "Epoch 186/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8331 - loss: 0.3251 \n",
      "Epoch 186: val_loss improved from 0.33611 to 0.33602, saving model to folds6.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8312 - loss: 0.3275 - val_accuracy: 0.8237 - val_loss: 0.3360 - learning_rate: 0.0012\n",
      "Epoch 187/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8236 - loss: 0.3277 \n",
      "Epoch 187: val_loss did not improve from 0.33602\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8260 - loss: 0.3295 - val_accuracy: 0.8304 - val_loss: 0.3370 - learning_rate: 0.0012\n",
      "Epoch 188/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8350 - loss: 0.3273\n",
      "Epoch 188: val_loss did not improve from 0.33602\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8349 - loss: 0.3275 - val_accuracy: 0.8218 - val_loss: 0.3385 - learning_rate: 0.0012\n",
      "Epoch 189/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8322 - loss: 0.3218 \n",
      "Epoch 189: val_loss did not improve from 0.33602\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8302 - loss: 0.3253 - val_accuracy: 0.8237 - val_loss: 0.3384 - learning_rate: 0.0012\n",
      "Epoch 190/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8335 - loss: 0.3290\n",
      "Epoch 190: val_loss did not improve from 0.33602\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8333 - loss: 0.3290 - val_accuracy: 0.8292 - val_loss: 0.3375 - learning_rate: 0.0012\n",
      "Epoch 191/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8268 - loss: 0.3291\n",
      "Epoch 191: val_loss did not improve from 0.33602\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8271 - loss: 0.3291 - val_accuracy: 0.8328 - val_loss: 0.3366 - learning_rate: 0.0012\n",
      "Epoch 192/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8250 - loss: 0.3312  \n",
      "Epoch 192: val_loss did not improve from 0.33602\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8245 - loss: 0.3311 - val_accuracy: 0.8267 - val_loss: 0.3396 - learning_rate: 0.0012\n",
      "Epoch 193/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8297 - loss: 0.3300\n",
      "Epoch 193: val_loss did not improve from 0.33602\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8295 - loss: 0.3300 - val_accuracy: 0.8322 - val_loss: 0.3363 - learning_rate: 0.0012\n",
      "Epoch 194/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8316 - loss: 0.3299 \n",
      "Epoch 194: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "\n",
      "Epoch 194: val_loss did not improve from 0.33602\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8313 - loss: 0.3299 - val_accuracy: 0.8243 - val_loss: 0.3378 - learning_rate: 0.0012\n",
      "Epoch 195/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8229 - loss: 0.3325  \n",
      "Epoch 195: val_loss improved from 0.33602 to 0.33565, saving model to folds6.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8246 - loss: 0.3313 - val_accuracy: 0.8292 - val_loss: 0.3357 - learning_rate: 6.2500e-04\n",
      "Epoch 196/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8342 - loss: 0.3234  \n",
      "Epoch 196: val_loss improved from 0.33565 to 0.33515, saving model to folds6.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8331 - loss: 0.3249 - val_accuracy: 0.8304 - val_loss: 0.3352 - learning_rate: 6.2500e-04\n",
      "Epoch 197/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8254 - loss: 0.3271 \n",
      "Epoch 197: val_loss did not improve from 0.33515\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8256 - loss: 0.3275 - val_accuracy: 0.8261 - val_loss: 0.3373 - learning_rate: 6.2500e-04\n",
      "Epoch 198/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8277 - loss: 0.3260 \n",
      "Epoch 198: val_loss did not improve from 0.33515\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8282 - loss: 0.3271 - val_accuracy: 0.8322 - val_loss: 0.3355 - learning_rate: 6.2500e-04\n",
      "Epoch 199/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8345 - loss: 0.3217 \n",
      "Epoch 199: val_loss improved from 0.33515 to 0.33456, saving model to folds6.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8324 - loss: 0.3241 - val_accuracy: 0.8310 - val_loss: 0.3346 - learning_rate: 6.2500e-04\n",
      "Epoch 200/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8209 - loss: 0.3407 \n",
      "Epoch 200: val_loss did not improve from 0.33456\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8246 - loss: 0.3352 - val_accuracy: 0.8328 - val_loss: 0.3350 - learning_rate: 6.2500e-04\n",
      "Epoch 201/300\n",
      "\u001b[1m13/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8260 - loss: 0.3331\n",
      "Epoch 201: val_loss did not improve from 0.33456\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8266 - loss: 0.3322 - val_accuracy: 0.8316 - val_loss: 0.3346 - learning_rate: 6.2500e-04\n",
      "Epoch 202/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8284 - loss: 0.3255\n",
      "Epoch 202: val_loss did not improve from 0.33456\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8285 - loss: 0.3258 - val_accuracy: 0.8328 - val_loss: 0.3347 - learning_rate: 6.2500e-04\n",
      "Epoch 203/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8287 - loss: 0.3286\n",
      "Epoch 203: val_loss did not improve from 0.33456\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8289 - loss: 0.3286 - val_accuracy: 0.8249 - val_loss: 0.3355 - learning_rate: 6.2500e-04\n",
      "Epoch 204/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8267 - loss: 0.3268  \n",
      "Epoch 204: val_loss did not improve from 0.33456\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8264 - loss: 0.3269 - val_accuracy: 0.8304 - val_loss: 0.3350 - learning_rate: 6.2500e-04\n",
      "Epoch 205/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8340 - loss: 0.3268\n",
      "Epoch 205: val_loss did not improve from 0.33456\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8341 - loss: 0.3269 - val_accuracy: 0.8237 - val_loss: 0.3360 - learning_rate: 6.2500e-04\n",
      "Epoch 206/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8218 - loss: 0.3280  \n",
      "Epoch 206: val_loss did not improve from 0.33456\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8238 - loss: 0.3280 - val_accuracy: 0.8304 - val_loss: 0.3371 - learning_rate: 6.2500e-04\n",
      "Epoch 207/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8307 - loss: 0.3296\n",
      "Epoch 207: val_loss did not improve from 0.33456\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8307 - loss: 0.3295 - val_accuracy: 0.8261 - val_loss: 0.3348 - learning_rate: 6.2500e-04\n",
      "Epoch 208/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8305 - loss: 0.3281  \n",
      "Epoch 208: val_loss did not improve from 0.33456\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8301 - loss: 0.3278 - val_accuracy: 0.8322 - val_loss: 0.3353 - learning_rate: 6.2500e-04\n",
      "Epoch 209/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8313 - loss: 0.3274\n",
      "Epoch 209: ReduceLROnPlateau reducing learning rate to 0.0003124999930150807.\n",
      "\n",
      "Epoch 209: val_loss did not improve from 0.33456\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8311 - loss: 0.3274 - val_accuracy: 0.8286 - val_loss: 0.3349 - learning_rate: 6.2500e-04\n",
      "Epoch 210/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8254 - loss: 0.3260 \n",
      "Epoch 210: val_loss did not improve from 0.33456\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8276 - loss: 0.3260 - val_accuracy: 0.8279 - val_loss: 0.3358 - learning_rate: 3.1250e-04\n",
      "Epoch 211/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8310 - loss: 0.3251 \n",
      "Epoch 211: val_loss did not improve from 0.33456\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8302 - loss: 0.3268 - val_accuracy: 0.8304 - val_loss: 0.3351 - learning_rate: 3.1250e-04\n",
      "Epoch 212/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8337 - loss: 0.3323 \n",
      "Epoch 212: val_loss did not improve from 0.33456\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8326 - loss: 0.3292 - val_accuracy: 0.8310 - val_loss: 0.3348 - learning_rate: 3.1250e-04\n",
      "Epoch 213/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8276 - loss: 0.3276 \n",
      "Epoch 213: val_loss did not improve from 0.33456\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8288 - loss: 0.3278 - val_accuracy: 0.8322 - val_loss: 0.3354 - learning_rate: 3.1250e-04\n",
      "Epoch 214/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8329 - loss: 0.3287\n",
      "Epoch 214: val_loss improved from 0.33456 to 0.33446, saving model to folds6.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8328 - loss: 0.3286 - val_accuracy: 0.8298 - val_loss: 0.3345 - learning_rate: 3.1250e-04\n",
      "Epoch 215/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8303 - loss: 0.3234 \n",
      "Epoch 215: val_loss did not improve from 0.33446\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8308 - loss: 0.3250 - val_accuracy: 0.8298 - val_loss: 0.3357 - learning_rate: 3.1250e-04\n",
      "Epoch 216/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8343 - loss: 0.3240\n",
      "Epoch 216: val_loss did not improve from 0.33446\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8341 - loss: 0.3242 - val_accuracy: 0.8310 - val_loss: 0.3351 - learning_rate: 3.1250e-04\n",
      "Epoch 217/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8338 - loss: 0.3280\n",
      "Epoch 217: val_loss did not improve from 0.33446\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8336 - loss: 0.3280 - val_accuracy: 0.8292 - val_loss: 0.3350 - learning_rate: 3.1250e-04\n",
      "Epoch 218/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8286 - loss: 0.3305 \n",
      "Epoch 218: val_loss did not improve from 0.33446\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8301 - loss: 0.3287 - val_accuracy: 0.8292 - val_loss: 0.3354 - learning_rate: 3.1250e-04\n",
      "Epoch 219/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8303 - loss: 0.3274\n",
      "Epoch 219: val_loss did not improve from 0.33446\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8304 - loss: 0.3274 - val_accuracy: 0.8286 - val_loss: 0.3350 - learning_rate: 3.1250e-04\n",
      "Epoch 220/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8288 - loss: 0.3315\n",
      "Epoch 220: val_loss did not improve from 0.33446\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8289 - loss: 0.3313 - val_accuracy: 0.8304 - val_loss: 0.3349 - learning_rate: 3.1250e-04\n",
      "Epoch 221/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8336 - loss: 0.3240\n",
      "Epoch 221: val_loss did not improve from 0.33446\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8335 - loss: 0.3242 - val_accuracy: 0.8292 - val_loss: 0.3349 - learning_rate: 3.1250e-04\n",
      "Epoch 222/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8292 - loss: 0.3280 \n",
      "Epoch 222: val_loss did not improve from 0.33446\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8292 - loss: 0.3278 - val_accuracy: 0.8310 - val_loss: 0.3355 - learning_rate: 3.1250e-04\n",
      "Epoch 223/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8287 - loss: 0.3363 \n",
      "Epoch 223: val_loss improved from 0.33446 to 0.33412, saving model to folds6.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8304 - loss: 0.3315 - val_accuracy: 0.8359 - val_loss: 0.3341 - learning_rate: 3.1250e-04\n",
      "Epoch 224/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8258 - loss: 0.3319\n",
      "Epoch 224: val_loss did not improve from 0.33412\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8262 - loss: 0.3316 - val_accuracy: 0.8273 - val_loss: 0.3360 - learning_rate: 3.1250e-04\n",
      "Epoch 225/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8314 - loss: 0.3363\n",
      "Epoch 225: val_loss did not improve from 0.33412\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.8321 - loss: 0.3335 - val_accuracy: 0.8328 - val_loss: 0.3349 - learning_rate: 3.1250e-04\n",
      "Epoch 226/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8306 - loss: 0.3288\n",
      "Epoch 226: val_loss did not improve from 0.33412\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.8307 - loss: 0.3278 - val_accuracy: 0.8286 - val_loss: 0.3343 - learning_rate: 3.1250e-04\n",
      "Epoch 227/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8335 - loss: 0.3237\n",
      "Epoch 227: val_loss did not improve from 0.33412\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.8328 - loss: 0.3248 - val_accuracy: 0.8310 - val_loss: 0.3366 - learning_rate: 3.1250e-04\n",
      "Epoch 228/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8344 - loss: 0.3241\n",
      "Epoch 228: val_loss improved from 0.33412 to 0.33378, saving model to folds6.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.8341 - loss: 0.3243 - val_accuracy: 0.8304 - val_loss: 0.3338 - learning_rate: 3.1250e-04\n",
      "Epoch 229/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8243 - loss: 0.3294\n",
      "Epoch 229: val_loss did not improve from 0.33378\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.8266 - loss: 0.3282 - val_accuracy: 0.8316 - val_loss: 0.3364 - learning_rate: 3.1250e-04\n",
      "Epoch 230/300\n",
      "\u001b[1m12/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8325 - loss: 0.3304\n",
      "Epoch 230: val_loss did not improve from 0.33378\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.8318 - loss: 0.3296 - val_accuracy: 0.8322 - val_loss: 0.3340 - learning_rate: 3.1250e-04\n",
      "Epoch 231/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8268 - loss: 0.3271\n",
      "Epoch 231: val_loss did not improve from 0.33378\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.8272 - loss: 0.3272 - val_accuracy: 0.8286 - val_loss: 0.3355 - learning_rate: 3.1250e-04\n",
      "Epoch 232/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8296 - loss: 0.3296\n",
      "Epoch 232: val_loss did not improve from 0.33378\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.8306 - loss: 0.3288 - val_accuracy: 0.8292 - val_loss: 0.3355 - learning_rate: 3.1250e-04\n",
      "Epoch 233/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8296 - loss: 0.3276\n",
      "Epoch 233: val_loss did not improve from 0.33378\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.8297 - loss: 0.3279 - val_accuracy: 0.8310 - val_loss: 0.3350 - learning_rate: 3.1250e-04\n",
      "Epoch 234/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8351 - loss: 0.3251\n",
      "Epoch 234: val_loss did not improve from 0.33378\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.8331 - loss: 0.3259 - val_accuracy: 0.8279 - val_loss: 0.3352 - learning_rate: 3.1250e-04\n",
      "Epoch 235/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8305 - loss: 0.3274\n",
      "Epoch 235: val_loss did not improve from 0.33378\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.8303 - loss: 0.3271 - val_accuracy: 0.8359 - val_loss: 0.3341 - learning_rate: 3.1250e-04\n",
      "Epoch 236/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8303 - loss: 0.3263\n",
      "Epoch 236: val_loss did not improve from 0.33378\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8307 - loss: 0.3264 - val_accuracy: 0.8322 - val_loss: 0.3355 - learning_rate: 3.1250e-04\n",
      "Epoch 237/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8310 - loss: 0.3279\n",
      "Epoch 237: val_loss improved from 0.33378 to 0.33372, saving model to folds6.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8311 - loss: 0.3279 - val_accuracy: 0.8353 - val_loss: 0.3337 - learning_rate: 3.1250e-04\n",
      "Epoch 238/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8209 - loss: 0.3345 \n",
      "Epoch 238: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "\n",
      "Epoch 238: val_loss did not improve from 0.33372\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8243 - loss: 0.3319 - val_accuracy: 0.8292 - val_loss: 0.3349 - learning_rate: 3.1250e-04\n",
      "Epoch 239/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8332 - loss: 0.3239\n",
      "Epoch 239: val_loss did not improve from 0.33372\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8331 - loss: 0.3241 - val_accuracy: 0.8316 - val_loss: 0.3342 - learning_rate: 1.5625e-04\n",
      "Epoch 240/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8266 - loss: 0.3305\n",
      "Epoch 240: val_loss did not improve from 0.33372\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8270 - loss: 0.3300 - val_accuracy: 0.8298 - val_loss: 0.3346 - learning_rate: 1.5625e-04\n",
      "Epoch 241/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8372 - loss: 0.3197 \n",
      "Epoch 241: val_loss did not improve from 0.33372\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8357 - loss: 0.3222 - val_accuracy: 0.8334 - val_loss: 0.3346 - learning_rate: 1.5625e-04\n",
      "Epoch 242/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8303 - loss: 0.3294\n",
      "Epoch 242: val_loss did not improve from 0.33372\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8305 - loss: 0.3292 - val_accuracy: 0.8310 - val_loss: 0.3350 - learning_rate: 1.5625e-04\n",
      "Epoch 243/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8330 - loss: 0.3249\n",
      "Epoch 243: val_loss did not improve from 0.33372\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8330 - loss: 0.3250 - val_accuracy: 0.8292 - val_loss: 0.3343 - learning_rate: 1.5625e-04\n",
      "Epoch 244/300\n",
      "\u001b[1m13/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8362 - loss: 0.3267 \n",
      "Epoch 244: val_loss did not improve from 0.33372\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8353 - loss: 0.3266 - val_accuracy: 0.8292 - val_loss: 0.3348 - learning_rate: 1.5625e-04\n",
      "Epoch 245/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8312 - loss: 0.3276\n",
      "Epoch 245: val_loss did not improve from 0.33372\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8314 - loss: 0.3275 - val_accuracy: 0.8322 - val_loss: 0.3344 - learning_rate: 1.5625e-04\n",
      "Epoch 246/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8312 - loss: 0.3289  \n",
      "Epoch 246: val_loss did not improve from 0.33372\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8306 - loss: 0.3286 - val_accuracy: 0.8292 - val_loss: 0.3347 - learning_rate: 1.5625e-04\n",
      "Epoch 247/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8329 - loss: 0.3211 \n",
      "Epoch 247: val_loss did not improve from 0.33372\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8327 - loss: 0.3232 - val_accuracy: 0.8316 - val_loss: 0.3346 - learning_rate: 1.5625e-04\n",
      "Epoch 248/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8338 - loss: 0.3233\n",
      "Epoch 248: ReduceLROnPlateau reducing learning rate to 7.812499825377017e-05.\n",
      "\n",
      "Epoch 248: val_loss did not improve from 0.33372\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8337 - loss: 0.3235 - val_accuracy: 0.8322 - val_loss: 0.3343 - learning_rate: 1.5625e-04\n",
      "Epoch 249/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8336 - loss: 0.3276\n",
      "Epoch 249: val_loss did not improve from 0.33372\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8335 - loss: 0.3276 - val_accuracy: 0.8304 - val_loss: 0.3344 - learning_rate: 7.8125e-05\n",
      "Epoch 250/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8306 - loss: 0.3291 \n",
      "Epoch 250: val_loss did not improve from 0.33372\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8313 - loss: 0.3277 - val_accuracy: 0.8298 - val_loss: 0.3348 - learning_rate: 7.8125e-05\n",
      "Epoch 251/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8315 - loss: 0.3249\n",
      "Epoch 251: val_loss did not improve from 0.33372\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8315 - loss: 0.3250 - val_accuracy: 0.8310 - val_loss: 0.3345 - learning_rate: 7.8125e-05\n",
      "Epoch 252/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8343 - loss: 0.3217\n",
      "Epoch 252: val_loss did not improve from 0.33372\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8341 - loss: 0.3220 - val_accuracy: 0.8310 - val_loss: 0.3343 - learning_rate: 7.8125e-05\n",
      "Epoch 253/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8367 - loss: 0.3209\n",
      "Epoch 253: val_loss did not improve from 0.33372\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8362 - loss: 0.3216 - val_accuracy: 0.8316 - val_loss: 0.3346 - learning_rate: 7.8125e-05\n",
      "Epoch 254/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8277 - loss: 0.3296\n",
      "Epoch 254: val_loss did not improve from 0.33372\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8282 - loss: 0.3292 - val_accuracy: 0.8304 - val_loss: 0.3344 - learning_rate: 7.8125e-05\n",
      "Epoch 255/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8378 - loss: 0.3189 \n",
      "Epoch 255: val_loss did not improve from 0.33372\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8357 - loss: 0.3221 - val_accuracy: 0.8310 - val_loss: 0.3346 - learning_rate: 7.8125e-05\n",
      "Epoch 256/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8348 - loss: 0.3253\n",
      "Epoch 256: val_loss did not improve from 0.33372\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8348 - loss: 0.3253 - val_accuracy: 0.8328 - val_loss: 0.3348 - learning_rate: 7.8125e-05\n",
      "Epoch 257/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8300 - loss: 0.3282\n",
      "Epoch 257: val_loss did not improve from 0.33372\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8302 - loss: 0.3281 - val_accuracy: 0.8340 - val_loss: 0.3342 - learning_rate: 7.8125e-05\n",
      "Epoch 258/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8315 - loss: 0.3281\n",
      "Epoch 258: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "\n",
      "Epoch 258: val_loss did not improve from 0.33372\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8316 - loss: 0.3280 - val_accuracy: 0.8316 - val_loss: 0.3342 - learning_rate: 7.8125e-05\n",
      "Epoch 259/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8369 - loss: 0.3227\n",
      "Epoch 259: val_loss did not improve from 0.33372\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8363 - loss: 0.3232 - val_accuracy: 0.8304 - val_loss: 0.3343 - learning_rate: 3.9062e-05\n",
      "Epoch 260/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8381 - loss: 0.3188 \n",
      "Epoch 260: val_loss did not improve from 0.33372\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8359 - loss: 0.3217 - val_accuracy: 0.8310 - val_loss: 0.3345 - learning_rate: 3.9062e-05\n",
      "Epoch 261/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8323 - loss: 0.3259  \n",
      "Epoch 261: val_loss did not improve from 0.33372\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8327 - loss: 0.3256 - val_accuracy: 0.8328 - val_loss: 0.3345 - learning_rate: 3.9062e-05\n",
      "Epoch 262/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8412 - loss: 0.3215\n",
      "Epoch 262: val_loss did not improve from 0.33372\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8407 - loss: 0.3218 - val_accuracy: 0.8328 - val_loss: 0.3345 - learning_rate: 3.9062e-05\n",
      "Epoch 263/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8334 - loss: 0.3262\n",
      "Epoch 263: val_loss did not improve from 0.33372\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8333 - loss: 0.3262 - val_accuracy: 0.8340 - val_loss: 0.3342 - learning_rate: 3.9062e-05\n",
      "Epoch 264/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8356 - loss: 0.3242\n",
      "Epoch 264: val_loss did not improve from 0.33372\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8354 - loss: 0.3243 - val_accuracy: 0.8310 - val_loss: 0.3344 - learning_rate: 3.9062e-05\n",
      "Epoch 265/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8276 - loss: 0.3264  \n",
      "Epoch 265: val_loss did not improve from 0.33372\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8296 - loss: 0.3262 - val_accuracy: 0.8322 - val_loss: 0.3343 - learning_rate: 3.9062e-05\n",
      "Epoch 266/300\n",
      "\u001b[1m12/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8336 - loss: 0.3268\n",
      "Epoch 266: val_loss did not improve from 0.33372\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8335 - loss: 0.3265 - val_accuracy: 0.8310 - val_loss: 0.3347 - learning_rate: 3.9062e-05\n",
      "Epoch 267/300\n",
      "\u001b[1m13/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8338 - loss: 0.3243\n",
      "Epoch 267: val_loss did not improve from 0.33372\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8338 - loss: 0.3246 - val_accuracy: 0.8322 - val_loss: 0.3344 - learning_rate: 3.9062e-05\n",
      "Epoch 268/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8326 - loss: 0.3238 \n",
      "Epoch 268: ReduceLROnPlateau reducing learning rate to 1.9531249563442543e-05.\n",
      "\n",
      "Epoch 268: val_loss did not improve from 0.33372\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8327 - loss: 0.3251 - val_accuracy: 0.8310 - val_loss: 0.3344 - learning_rate: 3.9062e-05\n",
      "Epoch 269/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8391 - loss: 0.3171 \n",
      "Epoch 269: val_loss did not improve from 0.33372\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8363 - loss: 0.3208 - val_accuracy: 0.8316 - val_loss: 0.3343 - learning_rate: 1.9531e-05\n",
      "Epoch 270/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8347 - loss: 0.3227\n",
      "Epoch 270: val_loss did not improve from 0.33372\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8346 - loss: 0.3229 - val_accuracy: 0.8310 - val_loss: 0.3344 - learning_rate: 1.9531e-05\n",
      "Epoch 271/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8364 - loss: 0.3242 \n",
      "Epoch 271: val_loss did not improve from 0.33372\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8357 - loss: 0.3245 - val_accuracy: 0.8310 - val_loss: 0.3344 - learning_rate: 1.9531e-05\n",
      "Epoch 272/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8309 - loss: 0.3271 \n",
      "Epoch 272: val_loss did not improve from 0.33372\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8314 - loss: 0.3279 - val_accuracy: 0.8316 - val_loss: 0.3345 - learning_rate: 1.9531e-05\n",
      "Epoch 273/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8311 - loss: 0.3297 \n",
      "Epoch 273: val_loss did not improve from 0.33372\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8320 - loss: 0.3283 - val_accuracy: 0.8310 - val_loss: 0.3344 - learning_rate: 1.9531e-05\n",
      "Epoch 274/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8311 - loss: 0.3297\n",
      "Epoch 274: val_loss did not improve from 0.33372\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8312 - loss: 0.3295 - val_accuracy: 0.8310 - val_loss: 0.3343 - learning_rate: 1.9531e-05\n",
      "Epoch 275/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8333 - loss: 0.3269\n",
      "Epoch 275: val_loss did not improve from 0.33372\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8333 - loss: 0.3269 - val_accuracy: 0.8322 - val_loss: 0.3345 - learning_rate: 1.9531e-05\n",
      "Epoch 276/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8284 - loss: 0.3319\n",
      "Epoch 276: val_loss did not improve from 0.33372\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8287 - loss: 0.3315 - val_accuracy: 0.8322 - val_loss: 0.3345 - learning_rate: 1.9531e-05\n",
      "Epoch 277/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8326 - loss: 0.3285  \n",
      "Epoch 277: val_loss did not improve from 0.33372\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8329 - loss: 0.3274 - val_accuracy: 0.8304 - val_loss: 0.3344 - learning_rate: 1.9531e-05\n",
      "Epoch 278/300\n",
      "\u001b[1m13/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8325 - loss: 0.3282\n",
      "Epoch 278: ReduceLROnPlateau reducing learning rate to 9.765624781721272e-06.\n",
      "\n",
      "Epoch 278: val_loss did not improve from 0.33372\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.8327 - loss: 0.3278 - val_accuracy: 0.8322 - val_loss: 0.3344 - learning_rate: 1.9531e-05\n",
      "Epoch 279/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8367 - loss: 0.3203\n",
      "Epoch 279: val_loss did not improve from 0.33372\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.8363 - loss: 0.3211 - val_accuracy: 0.8328 - val_loss: 0.3344 - learning_rate: 9.7656e-06\n",
      "Epoch 280/300\n",
      "\u001b[1m12/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8327 - loss: 0.3258\n",
      "Epoch 280: val_loss did not improve from 0.33372\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.8328 - loss: 0.3260 - val_accuracy: 0.8316 - val_loss: 0.3344 - learning_rate: 9.7656e-06\n",
      "Epoch 281/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8303 - loss: 0.3291\n",
      "Epoch 281: val_loss did not improve from 0.33372\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.8304 - loss: 0.3289 - val_accuracy: 0.8316 - val_loss: 0.3344 - learning_rate: 9.7656e-06\n",
      "Epoch 282/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8301 - loss: 0.3280\n",
      "Epoch 282: val_loss did not improve from 0.33372\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.8308 - loss: 0.3279 - val_accuracy: 0.8310 - val_loss: 0.3344 - learning_rate: 9.7656e-06\n",
      "Epoch 283/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8312 - loss: 0.3293\n",
      "Epoch 283: val_loss did not improve from 0.33372\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.8313 - loss: 0.3291 - val_accuracy: 0.8310 - val_loss: 0.3344 - learning_rate: 9.7656e-06\n",
      "Epoch 284/300\n",
      "\u001b[1m13/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8302 - loss: 0.3304\n",
      "Epoch 284: val_loss did not improve from 0.33372\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.8308 - loss: 0.3296 - val_accuracy: 0.8316 - val_loss: 0.3344 - learning_rate: 9.7656e-06\n",
      "Epoch 285/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8335 - loss: 0.3288 \n",
      "Epoch 285: val_loss did not improve from 0.33372\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8331 - loss: 0.3275 - val_accuracy: 0.8316 - val_loss: 0.3344 - learning_rate: 9.7656e-06\n",
      "Epoch 286/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8339 - loss: 0.3239\n",
      "Epoch 286: val_loss did not improve from 0.33372\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8338 - loss: 0.3240 - val_accuracy: 0.8316 - val_loss: 0.3344 - learning_rate: 9.7656e-06\n",
      "Epoch 287/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8339 - loss: 0.3259\n",
      "Epoch 287: val_loss did not improve from 0.33372\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8339 - loss: 0.3259 - val_accuracy: 0.8310 - val_loss: 0.3344 - learning_rate: 9.7656e-06\n",
      "Epoch 288/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8342 - loss: 0.3238\n",
      "Epoch 288: ReduceLROnPlateau reducing learning rate to 4.882812390860636e-06.\n",
      "\n",
      "Epoch 288: val_loss did not improve from 0.33372\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8341 - loss: 0.3239 - val_accuracy: 0.8310 - val_loss: 0.3344 - learning_rate: 9.7656e-06\n",
      "Epoch 289/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8281 - loss: 0.3310\n",
      "Epoch 289: val_loss did not improve from 0.33372\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8284 - loss: 0.3307 - val_accuracy: 0.8310 - val_loss: 0.3344 - learning_rate: 4.8828e-06\n",
      "Epoch 290/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8306 - loss: 0.3272  \n",
      "Epoch 290: val_loss did not improve from 0.33372\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8321 - loss: 0.3261 - val_accuracy: 0.8316 - val_loss: 0.3343 - learning_rate: 4.8828e-06\n",
      "Epoch 291/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8279 - loss: 0.3302\n",
      "Epoch 291: val_loss did not improve from 0.33372\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8282 - loss: 0.3300 - val_accuracy: 0.8310 - val_loss: 0.3344 - learning_rate: 4.8828e-06\n",
      "Epoch 292/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8287 - loss: 0.3288\n",
      "Epoch 292: val_loss did not improve from 0.33372\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8290 - loss: 0.3286 - val_accuracy: 0.8310 - val_loss: 0.3344 - learning_rate: 4.8828e-06\n",
      "Epoch 293/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8314 - loss: 0.3300\n",
      "Epoch 293: val_loss did not improve from 0.33372\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8316 - loss: 0.3296 - val_accuracy: 0.8310 - val_loss: 0.3344 - learning_rate: 4.8828e-06\n",
      "Epoch 294/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8403 - loss: 0.3188  \n",
      "Epoch 294: val_loss did not improve from 0.33372\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8377 - loss: 0.3216 - val_accuracy: 0.8316 - val_loss: 0.3344 - learning_rate: 4.8828e-06\n",
      "Epoch 295/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8296 - loss: 0.3317  \n",
      "Epoch 295: val_loss did not improve from 0.33372\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8313 - loss: 0.3295 - val_accuracy: 0.8310 - val_loss: 0.3344 - learning_rate: 4.8828e-06\n",
      "Epoch 296/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8367 - loss: 0.3241\n",
      "Epoch 296: val_loss did not improve from 0.33372\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8365 - loss: 0.3243 - val_accuracy: 0.8316 - val_loss: 0.3344 - learning_rate: 4.8828e-06\n",
      "Epoch 297/300\n",
      "\u001b[1m12/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8306 - loss: 0.3269\n",
      "Epoch 297: val_loss did not improve from 0.33372\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8314 - loss: 0.3266 - val_accuracy: 0.8316 - val_loss: 0.3344 - learning_rate: 4.8828e-06\n",
      "Epoch 297: early stopping\n",
      "Restoring model weights from the end of the best epoch: 237.\n",
      " Computing LSTM feature importance...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:01<00:00,  5.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- > Fold 8 < ---------------\n",
      "Epoch 1/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6870 - loss: 0.5865\n",
      "Epoch 1: val_loss improved from inf to 0.51034, saving model to folds7.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step - accuracy: 0.6960 - loss: 0.5708 - val_accuracy: 0.7248 - val_loss: 0.5103 - learning_rate: 0.0100\n",
      "Epoch 2/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7139 - loss: 0.5210 \n",
      "Epoch 2: val_loss improved from 0.51034 to 0.48560, saving model to folds7.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7152 - loss: 0.5174 - val_accuracy: 0.7303 - val_loss: 0.4856 - learning_rate: 0.0100\n",
      "Epoch 3/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7247 - loss: 0.4968 \n",
      "Epoch 3: val_loss did not improve from 0.48560\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7244 - loss: 0.4949 - val_accuracy: 0.7315 - val_loss: 0.4907 - learning_rate: 0.0100\n",
      "Epoch 4/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7281 - loss: 0.4899\n",
      "Epoch 4: val_loss improved from 0.48560 to 0.47201, saving model to folds7.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7279 - loss: 0.4900 - val_accuracy: 0.7401 - val_loss: 0.4720 - learning_rate: 0.0100\n",
      "Epoch 5/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7247 - loss: 0.4845\n",
      "Epoch 5: val_loss improved from 0.47201 to 0.46687, saving model to folds7.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7251 - loss: 0.4843 - val_accuracy: 0.7608 - val_loss: 0.4669 - learning_rate: 0.0100\n",
      "Epoch 6/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7324 - loss: 0.4788 \n",
      "Epoch 6: val_loss improved from 0.46687 to 0.46230, saving model to folds7.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7324 - loss: 0.4788 - val_accuracy: 0.7517 - val_loss: 0.4623 - learning_rate: 0.0100\n",
      "Epoch 7/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7424 - loss: 0.4748\n",
      "Epoch 7: val_loss did not improve from 0.46230\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7424 - loss: 0.4746 - val_accuracy: 0.7401 - val_loss: 0.4635 - learning_rate: 0.0100\n",
      "Epoch 8/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7385 - loss: 0.4708\n",
      "Epoch 8: val_loss did not improve from 0.46230\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7384 - loss: 0.4707 - val_accuracy: 0.7547 - val_loss: 0.4624 - learning_rate: 0.0100\n",
      "Epoch 9/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7397 - loss: 0.4686\n",
      "Epoch 9: val_loss improved from 0.46230 to 0.45027, saving model to folds7.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.7398 - loss: 0.4685 - val_accuracy: 0.7602 - val_loss: 0.4503 - learning_rate: 0.0100\n",
      "Epoch 10/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7363 - loss: 0.4644 \n",
      "Epoch 10: val_loss did not improve from 0.45027\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7396 - loss: 0.4623 - val_accuracy: 0.7541 - val_loss: 0.4571 - learning_rate: 0.0100\n",
      "Epoch 11/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7448 - loss: 0.4581\n",
      "Epoch 11: val_loss did not improve from 0.45027\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.7448 - loss: 0.4582 - val_accuracy: 0.7431 - val_loss: 0.4575 - learning_rate: 0.0100\n",
      "Epoch 12/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7416 - loss: 0.4584\n",
      "Epoch 12: val_loss improved from 0.45027 to 0.44203, saving model to folds7.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.7420 - loss: 0.4582 - val_accuracy: 0.7755 - val_loss: 0.4420 - learning_rate: 0.0100\n",
      "Epoch 13/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7522 - loss: 0.4461\n",
      "Epoch 13: val_loss improved from 0.44203 to 0.44037, saving model to folds7.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.7515 - loss: 0.4475 - val_accuracy: 0.7706 - val_loss: 0.4404 - learning_rate: 0.0100\n",
      "Epoch 14/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7478 - loss: 0.4529\n",
      "Epoch 14: val_loss did not improve from 0.44037\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.7479 - loss: 0.4527 - val_accuracy: 0.7773 - val_loss: 0.4431 - learning_rate: 0.0100\n",
      "Epoch 15/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7516 - loss: 0.4534\n",
      "Epoch 15: val_loss did not improve from 0.44037\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.7513 - loss: 0.4533 - val_accuracy: 0.7340 - val_loss: 0.4559 - learning_rate: 0.0100\n",
      "Epoch 16/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7437 - loss: 0.4602\n",
      "Epoch 16: val_loss improved from 0.44037 to 0.43761, saving model to folds7.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.7445 - loss: 0.4595 - val_accuracy: 0.7797 - val_loss: 0.4376 - learning_rate: 0.0100\n",
      "Epoch 17/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7538 - loss: 0.4544\n",
      "Epoch 17: val_loss did not improve from 0.43761\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.7548 - loss: 0.4527 - val_accuracy: 0.7517 - val_loss: 0.4561 - learning_rate: 0.0100\n",
      "Epoch 18/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7623 - loss: 0.4507\n",
      "Epoch 18: val_loss improved from 0.43761 to 0.43538, saving model to folds7.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.7607 - loss: 0.4500 - val_accuracy: 0.7688 - val_loss: 0.4354 - learning_rate: 0.0100\n",
      "Epoch 19/300\n",
      "\u001b[1m12/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7575 - loss: 0.4487\n",
      "Epoch 19: val_loss did not improve from 0.43538\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.7569 - loss: 0.4481 - val_accuracy: 0.7566 - val_loss: 0.4411 - learning_rate: 0.0100\n",
      "Epoch 20/300\n",
      "\u001b[1m12/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7512 - loss: 0.4497\n",
      "Epoch 20: val_loss improved from 0.43538 to 0.43025, saving model to folds7.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.7529 - loss: 0.4489 - val_accuracy: 0.7797 - val_loss: 0.4302 - learning_rate: 0.0100\n",
      "Epoch 21/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7623 - loss: 0.4421 \n",
      "Epoch 21: val_loss improved from 0.43025 to 0.42829, saving model to folds7.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7620 - loss: 0.4414 - val_accuracy: 0.7883 - val_loss: 0.4283 - learning_rate: 0.0100\n",
      "Epoch 22/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7682 - loss: 0.4383 \n",
      "Epoch 22: val_loss improved from 0.42829 to 0.42725, saving model to folds7.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7687 - loss: 0.4361 - val_accuracy: 0.7840 - val_loss: 0.4272 - learning_rate: 0.0100\n",
      "Epoch 23/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7607 - loss: 0.4451 \n",
      "Epoch 23: val_loss did not improve from 0.42725\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7611 - loss: 0.4436 - val_accuracy: 0.7779 - val_loss: 0.4353 - learning_rate: 0.0100\n",
      "Epoch 24/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7667 - loss: 0.4405 \n",
      "Epoch 24: val_loss did not improve from 0.42725\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7651 - loss: 0.4436 - val_accuracy: 0.7736 - val_loss: 0.4390 - learning_rate: 0.0100\n",
      "Epoch 25/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7620 - loss: 0.4473\n",
      "Epoch 25: val_loss did not improve from 0.42725\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7621 - loss: 0.4470 - val_accuracy: 0.7474 - val_loss: 0.4454 - learning_rate: 0.0100\n",
      "Epoch 26/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7587 - loss: 0.4519 \n",
      "Epoch 26: val_loss did not improve from 0.42725\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7593 - loss: 0.4506 - val_accuracy: 0.7736 - val_loss: 0.4308 - learning_rate: 0.0100\n",
      "Epoch 27/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7604 - loss: 0.4434 \n",
      "Epoch 27: val_loss improved from 0.42725 to 0.42345, saving model to folds7.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7641 - loss: 0.4388 - val_accuracy: 0.7736 - val_loss: 0.4234 - learning_rate: 0.0100\n",
      "Epoch 28/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7523 - loss: 0.4406 \n",
      "Epoch 28: val_loss did not improve from 0.42345\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7547 - loss: 0.4392 - val_accuracy: 0.7858 - val_loss: 0.4310 - learning_rate: 0.0100\n",
      "Epoch 29/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7639 - loss: 0.4410 \n",
      "Epoch 29: val_loss improved from 0.42345 to 0.42184, saving model to folds7.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7650 - loss: 0.4376 - val_accuracy: 0.7877 - val_loss: 0.4218 - learning_rate: 0.0100\n",
      "Epoch 30/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7747 - loss: 0.4262 \n",
      "Epoch 30: val_loss did not improve from 0.42184\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7741 - loss: 0.4258 - val_accuracy: 0.7858 - val_loss: 0.4235 - learning_rate: 0.0100\n",
      "Epoch 31/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7735 - loss: 0.4256 \n",
      "Epoch 31: val_loss improved from 0.42184 to 0.42039, saving model to folds7.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7729 - loss: 0.4258 - val_accuracy: 0.7852 - val_loss: 0.4204 - learning_rate: 0.0100\n",
      "Epoch 32/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7754 - loss: 0.4198 \n",
      "Epoch 32: val_loss did not improve from 0.42039\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7745 - loss: 0.4221 - val_accuracy: 0.7907 - val_loss: 0.4262 - learning_rate: 0.0100\n",
      "Epoch 33/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7755 - loss: 0.4291\n",
      "Epoch 33: val_loss did not improve from 0.42039\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7753 - loss: 0.4293 - val_accuracy: 0.7682 - val_loss: 0.4419 - learning_rate: 0.0100\n",
      "Epoch 34/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7726 - loss: 0.4325\n",
      "Epoch 34: val_loss did not improve from 0.42039\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7722 - loss: 0.4326 - val_accuracy: 0.7682 - val_loss: 0.4286 - learning_rate: 0.0100\n",
      "Epoch 35/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7687 - loss: 0.4238  \n",
      "Epoch 35: val_loss improved from 0.42039 to 0.41868, saving model to folds7.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7696 - loss: 0.4243 - val_accuracy: 0.7663 - val_loss: 0.4187 - learning_rate: 0.0100\n",
      "Epoch 36/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7835 - loss: 0.4121 \n",
      "Epoch 36: val_loss improved from 0.41868 to 0.41490, saving model to folds7.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7786 - loss: 0.4183 - val_accuracy: 0.7919 - val_loss: 0.4149 - learning_rate: 0.0100\n",
      "Epoch 37/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7775 - loss: 0.4197\n",
      "Epoch 37: val_loss did not improve from 0.41490\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7774 - loss: 0.4196 - val_accuracy: 0.7889 - val_loss: 0.4170 - learning_rate: 0.0100\n",
      "Epoch 38/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7752 - loss: 0.4226  \n",
      "Epoch 38: val_loss improved from 0.41490 to 0.41236, saving model to folds7.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7747 - loss: 0.4210 - val_accuracy: 0.7913 - val_loss: 0.4124 - learning_rate: 0.0100\n",
      "Epoch 39/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7854 - loss: 0.4112 \n",
      "Epoch 39: val_loss did not improve from 0.41236\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7833 - loss: 0.4125 - val_accuracy: 0.7779 - val_loss: 0.4213 - learning_rate: 0.0100\n",
      "Epoch 40/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7892 - loss: 0.4143  \n",
      "Epoch 40: val_loss did not improve from 0.41236\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7887 - loss: 0.4130 - val_accuracy: 0.7718 - val_loss: 0.4161 - learning_rate: 0.0100\n",
      "Epoch 41/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7768 - loss: 0.4178 \n",
      "Epoch 41: val_loss did not improve from 0.41236\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7768 - loss: 0.4174 - val_accuracy: 0.7932 - val_loss: 0.4130 - learning_rate: 0.0100\n",
      "Epoch 42/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7935 - loss: 0.4057 \n",
      "Epoch 42: val_loss did not improve from 0.41236\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7897 - loss: 0.4087 - val_accuracy: 0.7797 - val_loss: 0.4190 - learning_rate: 0.0100\n",
      "Epoch 43/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7794 - loss: 0.4132\n",
      "Epoch 43: val_loss did not improve from 0.41236\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7787 - loss: 0.4140 - val_accuracy: 0.7987 - val_loss: 0.4157 - learning_rate: 0.0100\n",
      "Epoch 44/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7854 - loss: 0.4133  \n",
      "Epoch 44: val_loss improved from 0.41236 to 0.40863, saving model to folds7.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.7829 - loss: 0.4151 - val_accuracy: 0.7852 - val_loss: 0.4086 - learning_rate: 0.0100\n",
      "Epoch 45/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7873 - loss: 0.4112 \n",
      "Epoch 45: val_loss improved from 0.40863 to 0.39924, saving model to folds7.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7876 - loss: 0.4115 - val_accuracy: 0.7895 - val_loss: 0.3992 - learning_rate: 0.0100\n",
      "Epoch 46/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7871 - loss: 0.4036 \n",
      "Epoch 46: val_loss did not improve from 0.39924\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7874 - loss: 0.4053 - val_accuracy: 0.7791 - val_loss: 0.4064 - learning_rate: 0.0100\n",
      "Epoch 47/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7887 - loss: 0.4096 \n",
      "Epoch 47: val_loss did not improve from 0.39924\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7893 - loss: 0.4084 - val_accuracy: 0.7822 - val_loss: 0.4041 - learning_rate: 0.0100\n",
      "Epoch 48/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7904 - loss: 0.4044 \n",
      "Epoch 48: val_loss improved from 0.39924 to 0.39515, saving model to folds7.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7928 - loss: 0.4024 - val_accuracy: 0.8121 - val_loss: 0.3951 - learning_rate: 0.0100\n",
      "Epoch 49/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7930 - loss: 0.3990 \n",
      "Epoch 49: val_loss did not improve from 0.39515\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7922 - loss: 0.3991 - val_accuracy: 0.8041 - val_loss: 0.3965 - learning_rate: 0.0100\n",
      "Epoch 50/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7969 - loss: 0.3946\n",
      "Epoch 50: val_loss improved from 0.39515 to 0.39136, saving model to folds7.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7967 - loss: 0.3949 - val_accuracy: 0.7980 - val_loss: 0.3914 - learning_rate: 0.0100\n",
      "Epoch 51/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7937 - loss: 0.3905 \n",
      "Epoch 51: val_loss did not improve from 0.39136\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7924 - loss: 0.3942 - val_accuracy: 0.7987 - val_loss: 0.3978 - learning_rate: 0.0100\n",
      "Epoch 52/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7944 - loss: 0.3962 \n",
      "Epoch 52: val_loss improved from 0.39136 to 0.39036, saving model to folds7.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7951 - loss: 0.3977 - val_accuracy: 0.8011 - val_loss: 0.3904 - learning_rate: 0.0100\n",
      "Epoch 53/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7982 - loss: 0.3981 \n",
      "Epoch 53: val_loss did not improve from 0.39036\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7962 - loss: 0.3984 - val_accuracy: 0.7761 - val_loss: 0.4027 - learning_rate: 0.0100\n",
      "Epoch 54/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7935 - loss: 0.3969\n",
      "Epoch 54: val_loss did not improve from 0.39036\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7940 - loss: 0.3968 - val_accuracy: 0.7877 - val_loss: 0.3985 - learning_rate: 0.0100\n",
      "Epoch 55/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7893 - loss: 0.3985\n",
      "Epoch 55: val_loss did not improve from 0.39036\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7898 - loss: 0.3983 - val_accuracy: 0.7938 - val_loss: 0.3982 - learning_rate: 0.0100\n",
      "Epoch 56/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7961 - loss: 0.3903 \n",
      "Epoch 56: val_loss did not improve from 0.39036\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7931 - loss: 0.3941 - val_accuracy: 0.8054 - val_loss: 0.3920 - learning_rate: 0.0100\n",
      "Epoch 57/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7909 - loss: 0.4064  \n",
      "Epoch 57: val_loss improved from 0.39036 to 0.38873, saving model to folds7.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7928 - loss: 0.4015 - val_accuracy: 0.8096 - val_loss: 0.3887 - learning_rate: 0.0100\n",
      "Epoch 58/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7893 - loss: 0.4017 \n",
      "Epoch 58: val_loss did not improve from 0.38873\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7930 - loss: 0.3983 - val_accuracy: 0.7694 - val_loss: 0.4169 - learning_rate: 0.0100\n",
      "Epoch 59/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7910 - loss: 0.4031  \n",
      "Epoch 59: val_loss did not improve from 0.38873\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7924 - loss: 0.3995 - val_accuracy: 0.8103 - val_loss: 0.3945 - learning_rate: 0.0100\n",
      "Epoch 60/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8090 - loss: 0.3861\n",
      "Epoch 60: val_loss improved from 0.38873 to 0.38458, saving model to folds7.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8085 - loss: 0.3862 - val_accuracy: 0.8072 - val_loss: 0.3846 - learning_rate: 0.0100\n",
      "Epoch 61/300\n",
      "\u001b[1m12/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8086 - loss: 0.3903\n",
      "Epoch 61: val_loss did not improve from 0.38458\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8080 - loss: 0.3892 - val_accuracy: 0.7932 - val_loss: 0.3946 - learning_rate: 0.0100\n",
      "Epoch 62/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8026 - loss: 0.3858  \n",
      "Epoch 62: val_loss did not improve from 0.38458\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8015 - loss: 0.3864 - val_accuracy: 0.7962 - val_loss: 0.3997 - learning_rate: 0.0100\n",
      "Epoch 63/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7755 - loss: 0.4139 \n",
      "Epoch 63: val_loss did not improve from 0.38458\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.7797 - loss: 0.4086 - val_accuracy: 0.8090 - val_loss: 0.3934 - learning_rate: 0.0100\n",
      "Epoch 64/300\n",
      "\u001b[1m12/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8006 - loss: 0.3885\n",
      "Epoch 64: val_loss did not improve from 0.38458\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.7996 - loss: 0.3884 - val_accuracy: 0.8115 - val_loss: 0.3890 - learning_rate: 0.0100\n",
      "Epoch 65/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7952 - loss: 0.3919 \n",
      "Epoch 65: val_loss did not improve from 0.38458\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.7954 - loss: 0.3913 - val_accuracy: 0.8218 - val_loss: 0.3862 - learning_rate: 0.0100\n",
      "Epoch 66/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8016 - loss: 0.3847\n",
      "Epoch 66: val_loss improved from 0.38458 to 0.37561, saving model to folds7.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.8028 - loss: 0.3840 - val_accuracy: 0.8127 - val_loss: 0.3756 - learning_rate: 0.0100\n",
      "Epoch 67/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8082 - loss: 0.3736 \n",
      "Epoch 67: val_loss improved from 0.37561 to 0.37555, saving model to folds7.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.8089 - loss: 0.3744 - val_accuracy: 0.8011 - val_loss: 0.3755 - learning_rate: 0.0100\n",
      "Epoch 68/300\n",
      "\u001b[1m12/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8048 - loss: 0.3792\n",
      "Epoch 68: val_loss did not improve from 0.37555\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.8048 - loss: 0.3793 - val_accuracy: 0.8151 - val_loss: 0.3811 - learning_rate: 0.0100\n",
      "Epoch 69/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8057 - loss: 0.3775\n",
      "Epoch 69: val_loss improved from 0.37555 to 0.36834, saving model to folds7.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.8073 - loss: 0.3762 - val_accuracy: 0.8084 - val_loss: 0.3683 - learning_rate: 0.0100\n",
      "Epoch 70/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8116 - loss: 0.3687\n",
      "Epoch 70: val_loss did not improve from 0.36834\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.8118 - loss: 0.3690 - val_accuracy: 0.8078 - val_loss: 0.3739 - learning_rate: 0.0100\n",
      "Epoch 71/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8146 - loss: 0.3677\n",
      "Epoch 71: val_loss did not improve from 0.36834\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.8140 - loss: 0.3691 - val_accuracy: 0.8084 - val_loss: 0.3753 - learning_rate: 0.0100\n",
      "Epoch 72/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8123 - loss: 0.3724  \n",
      "Epoch 72: val_loss did not improve from 0.36834\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8110 - loss: 0.3728 - val_accuracy: 0.8133 - val_loss: 0.3734 - learning_rate: 0.0100\n",
      "Epoch 73/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8114 - loss: 0.3708\n",
      "Epoch 73: val_loss did not improve from 0.36834\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8114 - loss: 0.3708 - val_accuracy: 0.7889 - val_loss: 0.3727 - learning_rate: 0.0100\n",
      "Epoch 74/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8053 - loss: 0.3719\n",
      "Epoch 74: val_loss did not improve from 0.36834\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8056 - loss: 0.3717 - val_accuracy: 0.8103 - val_loss: 0.3693 - learning_rate: 0.0100\n",
      "Epoch 75/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8051 - loss: 0.3710  \n",
      "Epoch 75: val_loss improved from 0.36834 to 0.36722, saving model to folds7.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8075 - loss: 0.3699 - val_accuracy: 0.8005 - val_loss: 0.3672 - learning_rate: 0.0100\n",
      "Epoch 76/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8169 - loss: 0.3669\n",
      "Epoch 76: val_loss did not improve from 0.36722\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8167 - loss: 0.3668 - val_accuracy: 0.7926 - val_loss: 0.3738 - learning_rate: 0.0100\n",
      "Epoch 77/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8105 - loss: 0.3677 \n",
      "Epoch 77: val_loss did not improve from 0.36722\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8114 - loss: 0.3690 - val_accuracy: 0.8151 - val_loss: 0.3752 - learning_rate: 0.0100\n",
      "Epoch 78/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8120 - loss: 0.3734 \n",
      "Epoch 78: val_loss improved from 0.36722 to 0.36027, saving model to folds7.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8119 - loss: 0.3720 - val_accuracy: 0.8310 - val_loss: 0.3603 - learning_rate: 0.0100\n",
      "Epoch 79/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8178 - loss: 0.3641 \n",
      "Epoch 79: val_loss did not improve from 0.36027\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8149 - loss: 0.3643 - val_accuracy: 0.8231 - val_loss: 0.3693 - learning_rate: 0.0100\n",
      "Epoch 80/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8185 - loss: 0.3644\n",
      "Epoch 80: val_loss improved from 0.36027 to 0.36001, saving model to folds7.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8184 - loss: 0.3643 - val_accuracy: 0.8176 - val_loss: 0.3600 - learning_rate: 0.0100\n",
      "Epoch 81/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8143 - loss: 0.3628 \n",
      "Epoch 81: val_loss did not improve from 0.36001\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8160 - loss: 0.3622 - val_accuracy: 0.8261 - val_loss: 0.3608 - learning_rate: 0.0100\n",
      "Epoch 82/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8122 - loss: 0.3603 \n",
      "Epoch 82: val_loss did not improve from 0.36001\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8139 - loss: 0.3620 - val_accuracy: 0.8096 - val_loss: 0.3684 - learning_rate: 0.0100\n",
      "Epoch 83/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8048 - loss: 0.3713\n",
      "Epoch 83: val_loss did not improve from 0.36001\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8048 - loss: 0.3712 - val_accuracy: 0.8157 - val_loss: 0.3735 - learning_rate: 0.0100\n",
      "Epoch 84/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8176 - loss: 0.3630 \n",
      "Epoch 84: val_loss did not improve from 0.36001\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8139 - loss: 0.3644 - val_accuracy: 0.8054 - val_loss: 0.3614 - learning_rate: 0.0100\n",
      "Epoch 85/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8118 - loss: 0.3621 \n",
      "Epoch 85: val_loss did not improve from 0.36001\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8124 - loss: 0.3620 - val_accuracy: 0.7962 - val_loss: 0.3631 - learning_rate: 0.0100\n",
      "Epoch 86/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8125 - loss: 0.3614\n",
      "Epoch 86: val_loss improved from 0.36001 to 0.35691, saving model to folds7.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.8125 - loss: 0.3614 - val_accuracy: 0.8279 - val_loss: 0.3569 - learning_rate: 0.0100\n",
      "Epoch 87/300\n",
      "\u001b[1m 7/15\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8240 - loss: 0.3515 \n",
      "Epoch 87: val_loss did not improve from 0.35691\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8195 - loss: 0.3562 - val_accuracy: 0.7980 - val_loss: 0.3630 - learning_rate: 0.0100\n",
      "Epoch 88/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8051 - loss: 0.3645 \n",
      "Epoch 88: val_loss did not improve from 0.35691\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8089 - loss: 0.3627 - val_accuracy: 0.8066 - val_loss: 0.3616 - learning_rate: 0.0100\n",
      "Epoch 89/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8142 - loss: 0.3581\n",
      "Epoch 89: val_loss did not improve from 0.35691\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8140 - loss: 0.3581 - val_accuracy: 0.8133 - val_loss: 0.3635 - learning_rate: 0.0100\n",
      "Epoch 90/300\n",
      "\u001b[1m12/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8138 - loss: 0.3590 \n",
      "Epoch 90: val_loss improved from 0.35691 to 0.35412, saving model to folds7.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8145 - loss: 0.3579 - val_accuracy: 0.8188 - val_loss: 0.3541 - learning_rate: 0.0100\n",
      "Epoch 91/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8179 - loss: 0.3502\n",
      "Epoch 91: val_loss did not improve from 0.35412\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8178 - loss: 0.3504 - val_accuracy: 0.7907 - val_loss: 0.3626 - learning_rate: 0.0100\n",
      "Epoch 92/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8054 - loss: 0.3630\n",
      "Epoch 92: val_loss did not improve from 0.35412\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8060 - loss: 0.3628 - val_accuracy: 0.8048 - val_loss: 0.3600 - learning_rate: 0.0100\n",
      "Epoch 93/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8041 - loss: 0.3722 \n",
      "Epoch 93: val_loss did not improve from 0.35412\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8081 - loss: 0.3666 - val_accuracy: 0.8072 - val_loss: 0.3592 - learning_rate: 0.0100\n",
      "Epoch 94/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8194 - loss: 0.3561 \n",
      "Epoch 94: val_loss improved from 0.35412 to 0.35367, saving model to folds7.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8191 - loss: 0.3548 - val_accuracy: 0.8292 - val_loss: 0.3537 - learning_rate: 0.0100\n",
      "Epoch 95/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8222 - loss: 0.3488\n",
      "Epoch 95: val_loss did not improve from 0.35367\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8221 - loss: 0.3491 - val_accuracy: 0.8164 - val_loss: 0.3547 - learning_rate: 0.0100\n",
      "Epoch 96/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8120 - loss: 0.3552 \n",
      "Epoch 96: val_loss did not improve from 0.35367\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8117 - loss: 0.3549 - val_accuracy: 0.8157 - val_loss: 0.3552 - learning_rate: 0.0100\n",
      "Epoch 97/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8105 - loss: 0.3587\n",
      "Epoch 97: val_loss did not improve from 0.35367\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8109 - loss: 0.3586 - val_accuracy: 0.8261 - val_loss: 0.3549 - learning_rate: 0.0100\n",
      "Epoch 98/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8193 - loss: 0.3489 \n",
      "Epoch 98: val_loss improved from 0.35367 to 0.35319, saving model to folds7.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8178 - loss: 0.3499 - val_accuracy: 0.8164 - val_loss: 0.3532 - learning_rate: 0.0100\n",
      "Epoch 99/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8169 - loss: 0.3538\n",
      "Epoch 99: val_loss improved from 0.35319 to 0.35103, saving model to folds7.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8168 - loss: 0.3537 - val_accuracy: 0.8286 - val_loss: 0.3510 - learning_rate: 0.0100\n",
      "Epoch 100/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8126 - loss: 0.3499 \n",
      "Epoch 100: val_loss did not improve from 0.35103\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8128 - loss: 0.3509 - val_accuracy: 0.8237 - val_loss: 0.3576 - learning_rate: 0.0100\n",
      "Epoch 101/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8240 - loss: 0.3532 \n",
      "Epoch 101: val_loss did not improve from 0.35103\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8237 - loss: 0.3511 - val_accuracy: 0.8084 - val_loss: 0.3579 - learning_rate: 0.0100\n",
      "Epoch 102/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8213 - loss: 0.3472 \n",
      "Epoch 102: val_loss did not improve from 0.35103\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8224 - loss: 0.3480 - val_accuracy: 0.8194 - val_loss: 0.3603 - learning_rate: 0.0100\n",
      "Epoch 103/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8200 - loss: 0.3533\n",
      "Epoch 103: val_loss did not improve from 0.35103\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8199 - loss: 0.3532 - val_accuracy: 0.8139 - val_loss: 0.3548 - learning_rate: 0.0100\n",
      "Epoch 104/300\n",
      "\u001b[1m13/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8216 - loss: 0.3466 \n",
      "Epoch 104: val_loss did not improve from 0.35103\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8219 - loss: 0.3466 - val_accuracy: 0.8151 - val_loss: 0.3625 - learning_rate: 0.0100\n",
      "Epoch 105/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8181 - loss: 0.3542\n",
      "Epoch 105: val_loss did not improve from 0.35103\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8180 - loss: 0.3540 - val_accuracy: 0.8121 - val_loss: 0.3584 - learning_rate: 0.0100\n",
      "Epoch 106/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8183 - loss: 0.3498\n",
      "Epoch 106: val_loss did not improve from 0.35103\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8182 - loss: 0.3501 - val_accuracy: 0.7950 - val_loss: 0.3722 - learning_rate: 0.0100\n",
      "Epoch 107/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8198 - loss: 0.3492  \n",
      "Epoch 107: val_loss did not improve from 0.35103\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8167 - loss: 0.3507 - val_accuracy: 0.8164 - val_loss: 0.3537 - learning_rate: 0.0100\n",
      "Epoch 108/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8245 - loss: 0.3434\n",
      "Epoch 108: val_loss improved from 0.35103 to 0.34576, saving model to folds7.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8239 - loss: 0.3439 - val_accuracy: 0.8151 - val_loss: 0.3458 - learning_rate: 0.0100\n",
      "Epoch 109/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8102 - loss: 0.3478 \n",
      "Epoch 109: val_loss did not improve from 0.34576\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8128 - loss: 0.3474 - val_accuracy: 0.8121 - val_loss: 0.3577 - learning_rate: 0.0100\n",
      "Epoch 110/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8197 - loss: 0.3477  \n",
      "Epoch 110: val_loss did not improve from 0.34576\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8206 - loss: 0.3464 - val_accuracy: 0.8188 - val_loss: 0.3487 - learning_rate: 0.0100\n",
      "Epoch 111/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8243 - loss: 0.3450\n",
      "Epoch 111: val_loss improved from 0.34576 to 0.34417, saving model to folds7.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8244 - loss: 0.3449 - val_accuracy: 0.8145 - val_loss: 0.3442 - learning_rate: 0.0100\n",
      "Epoch 112/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8220 - loss: 0.3481 \n",
      "Epoch 112: val_loss did not improve from 0.34417\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8233 - loss: 0.3458 - val_accuracy: 0.8139 - val_loss: 0.3482 - learning_rate: 0.0100\n",
      "Epoch 113/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8174 - loss: 0.3452\n",
      "Epoch 113: val_loss did not improve from 0.34417\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8177 - loss: 0.3450 - val_accuracy: 0.8200 - val_loss: 0.3508 - learning_rate: 0.0100\n",
      "Epoch 114/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8220 - loss: 0.3440\n",
      "Epoch 114: val_loss did not improve from 0.34417\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8219 - loss: 0.3442 - val_accuracy: 0.8212 - val_loss: 0.3496 - learning_rate: 0.0100\n",
      "Epoch 115/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8169 - loss: 0.3498\n",
      "Epoch 115: val_loss did not improve from 0.34417\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8175 - loss: 0.3490 - val_accuracy: 0.8206 - val_loss: 0.3541 - learning_rate: 0.0100\n",
      "Epoch 116/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8200 - loss: 0.3470\n",
      "Epoch 116: val_loss did not improve from 0.34417\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.8208 - loss: 0.3471 - val_accuracy: 0.8072 - val_loss: 0.3512 - learning_rate: 0.0100\n",
      "Epoch 117/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8214 - loss: 0.3465\n",
      "Epoch 117: val_loss did not improve from 0.34417\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.8209 - loss: 0.3468 - val_accuracy: 0.8066 - val_loss: 0.3523 - learning_rate: 0.0100\n",
      "Epoch 118/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8218 - loss: 0.3417\n",
      "Epoch 118: val_loss did not improve from 0.34417\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.8219 - loss: 0.3417 - val_accuracy: 0.8170 - val_loss: 0.3448 - learning_rate: 0.0100\n",
      "Epoch 119/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8271 - loss: 0.3408\n",
      "Epoch 119: val_loss did not improve from 0.34417\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.8262 - loss: 0.3412 - val_accuracy: 0.8176 - val_loss: 0.3463 - learning_rate: 0.0100\n",
      "Epoch 120/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8187 - loss: 0.3457\n",
      "Epoch 120: val_loss did not improve from 0.34417\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.8187 - loss: 0.3458 - val_accuracy: 0.8225 - val_loss: 0.3519 - learning_rate: 0.0100\n",
      "Epoch 121/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8212 - loss: 0.3350\n",
      "Epoch 121: val_loss improved from 0.34417 to 0.34139, saving model to folds7.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.8205 - loss: 0.3375 - val_accuracy: 0.8225 - val_loss: 0.3414 - learning_rate: 0.0100\n",
      "Epoch 122/300\n",
      "\u001b[1m12/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8209 - loss: 0.3409\n",
      "Epoch 122: val_loss improved from 0.34139 to 0.34089, saving model to folds7.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.8206 - loss: 0.3410 - val_accuracy: 0.8261 - val_loss: 0.3409 - learning_rate: 0.0100\n",
      "Epoch 123/300\n",
      "\u001b[1m13/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8286 - loss: 0.3336 \n",
      "Epoch 123: val_loss did not improve from 0.34089\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.8287 - loss: 0.3337 - val_accuracy: 0.8084 - val_loss: 0.3476 - learning_rate: 0.0100\n",
      "Epoch 124/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8265 - loss: 0.3351\n",
      "Epoch 124: val_loss improved from 0.34089 to 0.33680, saving model to folds7.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.8274 - loss: 0.3347 - val_accuracy: 0.8322 - val_loss: 0.3368 - learning_rate: 0.0100\n",
      "Epoch 125/300\n",
      "\u001b[1m12/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8275 - loss: 0.3386\n",
      "Epoch 125: val_loss did not improve from 0.33680\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.8275 - loss: 0.3380 - val_accuracy: 0.8237 - val_loss: 0.3460 - learning_rate: 0.0100\n",
      "Epoch 126/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8368 - loss: 0.3297  \n",
      "Epoch 126: val_loss did not improve from 0.33680\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8343 - loss: 0.3324 - val_accuracy: 0.8292 - val_loss: 0.3466 - learning_rate: 0.0100\n",
      "Epoch 127/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8142 - loss: 0.3507\n",
      "Epoch 127: val_loss did not improve from 0.33680\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8145 - loss: 0.3504 - val_accuracy: 0.8164 - val_loss: 0.3638 - learning_rate: 0.0100\n",
      "Epoch 128/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8205 - loss: 0.3481\n",
      "Epoch 128: val_loss did not improve from 0.33680\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8207 - loss: 0.3477 - val_accuracy: 0.8170 - val_loss: 0.3444 - learning_rate: 0.0100\n",
      "Epoch 129/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8158 - loss: 0.3414 \n",
      "Epoch 129: val_loss did not improve from 0.33680\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8192 - loss: 0.3392 - val_accuracy: 0.8090 - val_loss: 0.3472 - learning_rate: 0.0100\n",
      "Epoch 130/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8233 - loss: 0.3385\n",
      "Epoch 130: val_loss did not improve from 0.33680\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8234 - loss: 0.3385 - val_accuracy: 0.8249 - val_loss: 0.3383 - learning_rate: 0.0100\n",
      "Epoch 131/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8260 - loss: 0.3358\n",
      "Epoch 131: val_loss did not improve from 0.33680\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8260 - loss: 0.3359 - val_accuracy: 0.8353 - val_loss: 0.3397 - learning_rate: 0.0100\n",
      "Epoch 132/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8154 - loss: 0.3425\n",
      "Epoch 132: val_loss did not improve from 0.33680\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8156 - loss: 0.3424 - val_accuracy: 0.8127 - val_loss: 0.3532 - learning_rate: 0.0100\n",
      "Epoch 133/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8127 - loss: 0.3469 \n",
      "Epoch 133: val_loss did not improve from 0.33680\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8134 - loss: 0.3470 - val_accuracy: 0.8206 - val_loss: 0.3458 - learning_rate: 0.0100\n",
      "Epoch 134/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8324 - loss: 0.3295\n",
      "Epoch 134: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
      "\n",
      "Epoch 134: val_loss did not improve from 0.33680\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8322 - loss: 0.3297 - val_accuracy: 0.8377 - val_loss: 0.3373 - learning_rate: 0.0100\n",
      "Epoch 135/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8310 - loss: 0.3314 \n",
      "Epoch 135: val_loss improved from 0.33680 to 0.33025, saving model to folds7.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8309 - loss: 0.3290 - val_accuracy: 0.8286 - val_loss: 0.3303 - learning_rate: 0.0050\n",
      "Epoch 136/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8340 - loss: 0.3234\n",
      "Epoch 136: val_loss improved from 0.33025 to 0.32934, saving model to folds7.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8339 - loss: 0.3234 - val_accuracy: 0.8371 - val_loss: 0.3293 - learning_rate: 0.0050\n",
      "Epoch 137/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8381 - loss: 0.3231 \n",
      "Epoch 137: val_loss improved from 0.32934 to 0.32712, saving model to folds7.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8363 - loss: 0.3238 - val_accuracy: 0.8316 - val_loss: 0.3271 - learning_rate: 0.0050\n",
      "Epoch 138/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8327 - loss: 0.3226 \n",
      "Epoch 138: val_loss improved from 0.32712 to 0.32706, saving model to folds7.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8328 - loss: 0.3233 - val_accuracy: 0.8353 - val_loss: 0.3271 - learning_rate: 0.0050\n",
      "Epoch 139/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8344 - loss: 0.3271 \n",
      "Epoch 139: val_loss improved from 0.32706 to 0.32642, saving model to folds7.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8355 - loss: 0.3245 - val_accuracy: 0.8371 - val_loss: 0.3264 - learning_rate: 0.0050\n",
      "Epoch 140/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8386 - loss: 0.3196 \n",
      "Epoch 140: val_loss improved from 0.32642 to 0.32627, saving model to folds7.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8381 - loss: 0.3201 - val_accuracy: 0.8322 - val_loss: 0.3263 - learning_rate: 0.0050\n",
      "Epoch 141/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8412 - loss: 0.3166 \n",
      "Epoch 141: val_loss improved from 0.32627 to 0.32592, saving model to folds7.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8397 - loss: 0.3184 - val_accuracy: 0.8359 - val_loss: 0.3259 - learning_rate: 0.0050\n",
      "Epoch 142/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8370 - loss: 0.3236 \n",
      "Epoch 142: val_loss did not improve from 0.32592\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8361 - loss: 0.3228 - val_accuracy: 0.8365 - val_loss: 0.3266 - learning_rate: 0.0050\n",
      "Epoch 143/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8410 - loss: 0.3208\n",
      "Epoch 143: val_loss improved from 0.32592 to 0.32540, saving model to folds7.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8408 - loss: 0.3209 - val_accuracy: 0.8340 - val_loss: 0.3254 - learning_rate: 0.0050\n",
      "Epoch 144/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8384 - loss: 0.3228\n",
      "Epoch 144: val_loss did not improve from 0.32540\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8384 - loss: 0.3226 - val_accuracy: 0.8334 - val_loss: 0.3258 - learning_rate: 0.0050\n",
      "Epoch 145/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8333 - loss: 0.3211\n",
      "Epoch 145: val_loss did not improve from 0.32540\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8334 - loss: 0.3211 - val_accuracy: 0.8328 - val_loss: 0.3255 - learning_rate: 0.0050\n",
      "Epoch 146/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8420 - loss: 0.3179\n",
      "Epoch 146: val_loss did not improve from 0.32540\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8418 - loss: 0.3180 - val_accuracy: 0.8194 - val_loss: 0.3323 - learning_rate: 0.0050\n",
      "Epoch 147/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8390 - loss: 0.3219\n",
      "Epoch 147: val_loss did not improve from 0.32540\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8387 - loss: 0.3217 - val_accuracy: 0.8298 - val_loss: 0.3317 - learning_rate: 0.0050\n",
      "Epoch 148/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8399 - loss: 0.3186 \n",
      "Epoch 148: val_loss improved from 0.32540 to 0.32400, saving model to folds7.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8404 - loss: 0.3183 - val_accuracy: 0.8353 - val_loss: 0.3240 - learning_rate: 0.0050\n",
      "Epoch 149/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8331 - loss: 0.3280 \n",
      "Epoch 149: val_loss did not improve from 0.32400\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8336 - loss: 0.3248 - val_accuracy: 0.8347 - val_loss: 0.3309 - learning_rate: 0.0050\n",
      "Epoch 150/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8435 - loss: 0.3192\n",
      "Epoch 150: val_loss did not improve from 0.32400\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8433 - loss: 0.3193 - val_accuracy: 0.8340 - val_loss: 0.3245 - learning_rate: 0.0050\n",
      "Epoch 151/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8389 - loss: 0.3145 \n",
      "Epoch 151: val_loss improved from 0.32400 to 0.32197, saving model to folds7.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8376 - loss: 0.3171 - val_accuracy: 0.8383 - val_loss: 0.3220 - learning_rate: 0.0050\n",
      "Epoch 152/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8348 - loss: 0.3194 \n",
      "Epoch 152: val_loss did not improve from 0.32197\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8358 - loss: 0.3188 - val_accuracy: 0.8115 - val_loss: 0.3385 - learning_rate: 0.0050\n",
      "Epoch 153/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8375 - loss: 0.3197\n",
      "Epoch 153: val_loss did not improve from 0.32197\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8375 - loss: 0.3197 - val_accuracy: 0.8347 - val_loss: 0.3231 - learning_rate: 0.0050\n",
      "Epoch 154/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8384 - loss: 0.3121  \n",
      "Epoch 154: val_loss did not improve from 0.32197\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8380 - loss: 0.3148 - val_accuracy: 0.8322 - val_loss: 0.3306 - learning_rate: 0.0050\n",
      "Epoch 155/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8401 - loss: 0.3183\n",
      "Epoch 155: val_loss did not improve from 0.32197\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8400 - loss: 0.3184 - val_accuracy: 0.8359 - val_loss: 0.3226 - learning_rate: 0.0050\n",
      "Epoch 156/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8373 - loss: 0.3192 \n",
      "Epoch 156: val_loss did not improve from 0.32197\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8381 - loss: 0.3187 - val_accuracy: 0.8365 - val_loss: 0.3254 - learning_rate: 0.0050\n",
      "Epoch 157/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8418 - loss: 0.3154  \n",
      "Epoch 157: val_loss did not improve from 0.32197\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8394 - loss: 0.3174 - val_accuracy: 0.8365 - val_loss: 0.3243 - learning_rate: 0.0050\n",
      "Epoch 158/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8369 - loss: 0.3238  \n",
      "Epoch 158: val_loss did not improve from 0.32197\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8369 - loss: 0.3228 - val_accuracy: 0.8334 - val_loss: 0.3300 - learning_rate: 0.0050\n",
      "Epoch 159/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8405 - loss: 0.3184 \n",
      "Epoch 159: val_loss did not improve from 0.32197\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8380 - loss: 0.3202 - val_accuracy: 0.8206 - val_loss: 0.3282 - learning_rate: 0.0050\n",
      "Epoch 160/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8369 - loss: 0.3199\n",
      "Epoch 160: val_loss did not improve from 0.32197\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8369 - loss: 0.3198 - val_accuracy: 0.8334 - val_loss: 0.3254 - learning_rate: 0.0050\n",
      "Epoch 161/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8410 - loss: 0.3188  \n",
      "Epoch 161: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "\n",
      "Epoch 161: val_loss did not improve from 0.32197\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8381 - loss: 0.3201 - val_accuracy: 0.8249 - val_loss: 0.3311 - learning_rate: 0.0050\n",
      "Epoch 162/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8375 - loss: 0.3190\n",
      "Epoch 162: val_loss did not improve from 0.32197\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8377 - loss: 0.3188 - val_accuracy: 0.8328 - val_loss: 0.3228 - learning_rate: 0.0025\n",
      "Epoch 163/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8413 - loss: 0.3195 \n",
      "Epoch 163: val_loss improved from 0.32197 to 0.32145, saving model to folds7.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8408 - loss: 0.3180 - val_accuracy: 0.8395 - val_loss: 0.3214 - learning_rate: 0.0025\n",
      "Epoch 164/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8418 - loss: 0.3135 \n",
      "Epoch 164: val_loss did not improve from 0.32145\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8410 - loss: 0.3141 - val_accuracy: 0.8359 - val_loss: 0.3230 - learning_rate: 0.0025\n",
      "Epoch 165/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8373 - loss: 0.3170\n",
      "Epoch 165: val_loss improved from 0.32145 to 0.32125, saving model to folds7.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8375 - loss: 0.3168 - val_accuracy: 0.8395 - val_loss: 0.3212 - learning_rate: 0.0025\n",
      "Epoch 166/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8450 - loss: 0.3082 \n",
      "Epoch 166: val_loss did not improve from 0.32125\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8434 - loss: 0.3104 - val_accuracy: 0.8395 - val_loss: 0.3214 - learning_rate: 0.0025\n",
      "Epoch 167/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8427 - loss: 0.3118\n",
      "Epoch 167: val_loss improved from 0.32125 to 0.32007, saving model to folds7.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8426 - loss: 0.3119 - val_accuracy: 0.8444 - val_loss: 0.3201 - learning_rate: 0.0025\n",
      "Epoch 168/300\n",
      "\u001b[1m13/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8399 - loss: 0.3151\n",
      "Epoch 168: val_loss did not improve from 0.32007\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.8400 - loss: 0.3150 - val_accuracy: 0.8347 - val_loss: 0.3243 - learning_rate: 0.0025\n",
      "Epoch 169/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8441 - loss: 0.3106\n",
      "Epoch 169: val_loss improved from 0.32007 to 0.31992, saving model to folds7.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.8432 - loss: 0.3117 - val_accuracy: 0.8377 - val_loss: 0.3199 - learning_rate: 0.0025\n",
      "Epoch 170/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8412 - loss: 0.3132\n",
      "Epoch 170: val_loss did not improve from 0.31992\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.8418 - loss: 0.3131 - val_accuracy: 0.8316 - val_loss: 0.3250 - learning_rate: 0.0025\n",
      "Epoch 171/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8384 - loss: 0.3186\n",
      "Epoch 171: val_loss improved from 0.31992 to 0.31941, saving model to folds7.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.8385 - loss: 0.3183 - val_accuracy: 0.8389 - val_loss: 0.3194 - learning_rate: 0.0025\n",
      "Epoch 172/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8380 - loss: 0.3199\n",
      "Epoch 172: val_loss did not improve from 0.31941\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.8387 - loss: 0.3193 - val_accuracy: 0.8340 - val_loss: 0.3199 - learning_rate: 0.0025\n",
      "Epoch 173/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8390 - loss: 0.3109\n",
      "Epoch 173: val_loss did not improve from 0.31941\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.8390 - loss: 0.3113 - val_accuracy: 0.8322 - val_loss: 0.3220 - learning_rate: 0.0025\n",
      "Epoch 174/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8431 - loss: 0.3113\n",
      "Epoch 174: val_loss did not improve from 0.31941\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.8432 - loss: 0.3117 - val_accuracy: 0.8347 - val_loss: 0.3210 - learning_rate: 0.0025\n",
      "Epoch 175/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8373 - loss: 0.3171  \n",
      "Epoch 175: val_loss did not improve from 0.31941\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8387 - loss: 0.3151 - val_accuracy: 0.8353 - val_loss: 0.3198 - learning_rate: 0.0025\n",
      "Epoch 176/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8377 - loss: 0.3189 \n",
      "Epoch 176: val_loss did not improve from 0.31941\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8388 - loss: 0.3165 - val_accuracy: 0.8347 - val_loss: 0.3239 - learning_rate: 0.0025\n",
      "Epoch 177/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8451 - loss: 0.3078 \n",
      "Epoch 177: val_loss improved from 0.31941 to 0.31821, saving model to folds7.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8440 - loss: 0.3091 - val_accuracy: 0.8389 - val_loss: 0.3182 - learning_rate: 0.0025\n",
      "Epoch 178/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8407 - loss: 0.3095\n",
      "Epoch 178: val_loss did not improve from 0.31821\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8408 - loss: 0.3097 - val_accuracy: 0.8469 - val_loss: 0.3188 - learning_rate: 0.0025\n",
      "Epoch 179/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8433 - loss: 0.3149 \n",
      "Epoch 179: val_loss improved from 0.31821 to 0.31778, saving model to folds7.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8434 - loss: 0.3129 - val_accuracy: 0.8395 - val_loss: 0.3178 - learning_rate: 0.0025\n",
      "Epoch 180/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8474 - loss: 0.3091 \n",
      "Epoch 180: val_loss did not improve from 0.31778\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8451 - loss: 0.3105 - val_accuracy: 0.8414 - val_loss: 0.3195 - learning_rate: 0.0025\n",
      "Epoch 181/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8411 - loss: 0.3103\n",
      "Epoch 181: val_loss did not improve from 0.31778\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8411 - loss: 0.3104 - val_accuracy: 0.8377 - val_loss: 0.3187 - learning_rate: 0.0025\n",
      "Epoch 182/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8369 - loss: 0.3142\n",
      "Epoch 182: val_loss did not improve from 0.31778\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8371 - loss: 0.3140 - val_accuracy: 0.8322 - val_loss: 0.3234 - learning_rate: 0.0025\n",
      "Epoch 183/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8433 - loss: 0.3087\n",
      "Epoch 183: val_loss improved from 0.31778 to 0.31695, saving model to folds7.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8432 - loss: 0.3089 - val_accuracy: 0.8395 - val_loss: 0.3169 - learning_rate: 0.0025\n",
      "Epoch 184/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8512 - loss: 0.3015 \n",
      "Epoch 184: val_loss did not improve from 0.31695\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8471 - loss: 0.3050 - val_accuracy: 0.8365 - val_loss: 0.3187 - learning_rate: 0.0025\n",
      "Epoch 185/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8419 - loss: 0.3134\n",
      "Epoch 185: val_loss did not improve from 0.31695\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8418 - loss: 0.3132 - val_accuracy: 0.8401 - val_loss: 0.3188 - learning_rate: 0.0025\n",
      "Epoch 186/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8416 - loss: 0.3110 \n",
      "Epoch 186: val_loss did not improve from 0.31695\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8434 - loss: 0.3111 - val_accuracy: 0.8383 - val_loss: 0.3171 - learning_rate: 0.0025\n",
      "Epoch 187/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8452 - loss: 0.3066\n",
      "Epoch 187: val_loss did not improve from 0.31695\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8451 - loss: 0.3069 - val_accuracy: 0.8475 - val_loss: 0.3214 - learning_rate: 0.0025\n",
      "Epoch 188/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8406 - loss: 0.3125\n",
      "Epoch 188: val_loss did not improve from 0.31695\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8405 - loss: 0.3125 - val_accuracy: 0.8383 - val_loss: 0.3218 - learning_rate: 0.0025\n",
      "Epoch 189/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8393 - loss: 0.3138 \n",
      "Epoch 189: val_loss did not improve from 0.31695\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8398 - loss: 0.3134 - val_accuracy: 0.8481 - val_loss: 0.3170 - learning_rate: 0.0025\n",
      "Epoch 190/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8446 - loss: 0.3115\n",
      "Epoch 190: val_loss did not improve from 0.31695\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8443 - loss: 0.3115 - val_accuracy: 0.8261 - val_loss: 0.3211 - learning_rate: 0.0025\n",
      "Epoch 191/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8384 - loss: 0.3144\n",
      "Epoch 191: val_loss did not improve from 0.31695\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8385 - loss: 0.3143 - val_accuracy: 0.8249 - val_loss: 0.3242 - learning_rate: 0.0025\n",
      "Epoch 192/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8368 - loss: 0.3153\n",
      "Epoch 192: val_loss did not improve from 0.31695\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8371 - loss: 0.3151 - val_accuracy: 0.8475 - val_loss: 0.3170 - learning_rate: 0.0025\n",
      "Epoch 193/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8458 - loss: 0.3072\n",
      "Epoch 193: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
      "\n",
      "Epoch 193: val_loss did not improve from 0.31695\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8456 - loss: 0.3075 - val_accuracy: 0.8414 - val_loss: 0.3242 - learning_rate: 0.0025\n",
      "Epoch 194/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8453 - loss: 0.3093\n",
      "Epoch 194: val_loss did not improve from 0.31695\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8451 - loss: 0.3094 - val_accuracy: 0.8383 - val_loss: 0.3182 - learning_rate: 0.0012\n",
      "Epoch 195/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8444 - loss: 0.3079 \n",
      "Epoch 195: val_loss did not improve from 0.31695\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8438 - loss: 0.3089 - val_accuracy: 0.8316 - val_loss: 0.3197 - learning_rate: 0.0012\n",
      "Epoch 196/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8470 - loss: 0.3073\n",
      "Epoch 196: val_loss improved from 0.31695 to 0.31652, saving model to folds7.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8469 - loss: 0.3074 - val_accuracy: 0.8401 - val_loss: 0.3165 - learning_rate: 0.0012\n",
      "Epoch 197/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8421 - loss: 0.3136 \n",
      "Epoch 197: val_loss improved from 0.31652 to 0.31635, saving model to folds7.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8428 - loss: 0.3120 - val_accuracy: 0.8426 - val_loss: 0.3163 - learning_rate: 0.0012\n",
      "Epoch 198/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8485 - loss: 0.3075 \n",
      "Epoch 198: val_loss did not improve from 0.31635\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8477 - loss: 0.3070 - val_accuracy: 0.8340 - val_loss: 0.3172 - learning_rate: 0.0012\n",
      "Epoch 199/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8500 - loss: 0.2990  \n",
      "Epoch 199: val_loss did not improve from 0.31635\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8473 - loss: 0.3031 - val_accuracy: 0.8377 - val_loss: 0.3166 - learning_rate: 0.0012\n",
      "Epoch 200/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8388 - loss: 0.3122 \n",
      "Epoch 200: val_loss improved from 0.31635 to 0.31619, saving model to folds7.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8413 - loss: 0.3103 - val_accuracy: 0.8353 - val_loss: 0.3162 - learning_rate: 0.0012\n",
      "Epoch 201/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8389 - loss: 0.3111\n",
      "Epoch 201: val_loss improved from 0.31619 to 0.31601, saving model to folds7.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8391 - loss: 0.3109 - val_accuracy: 0.8395 - val_loss: 0.3160 - learning_rate: 0.0012\n",
      "Epoch 202/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8475 - loss: 0.3053 \n",
      "Epoch 202: val_loss improved from 0.31601 to 0.31523, saving model to folds7.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8463 - loss: 0.3067 - val_accuracy: 0.8426 - val_loss: 0.3152 - learning_rate: 0.0012\n",
      "Epoch 203/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8371 - loss: 0.3137 \n",
      "Epoch 203: val_loss did not improve from 0.31523\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8393 - loss: 0.3117 - val_accuracy: 0.8359 - val_loss: 0.3161 - learning_rate: 0.0012\n",
      "Epoch 204/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8471 - loss: 0.3065 \n",
      "Epoch 204: val_loss improved from 0.31523 to 0.31511, saving model to folds7.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.8464 - loss: 0.3068 - val_accuracy: 0.8438 - val_loss: 0.3151 - learning_rate: 0.0012\n",
      "Epoch 205/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8492 - loss: 0.3080 \n",
      "Epoch 205: val_loss improved from 0.31511 to 0.31510, saving model to folds7.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8465 - loss: 0.3084 - val_accuracy: 0.8414 - val_loss: 0.3151 - learning_rate: 0.0012\n",
      "Epoch 206/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8432 - loss: 0.3110 \n",
      "Epoch 206: val_loss improved from 0.31510 to 0.31493, saving model to folds7.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8438 - loss: 0.3094 - val_accuracy: 0.8456 - val_loss: 0.3149 - learning_rate: 0.0012\n",
      "Epoch 207/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8466 - loss: 0.3088\n",
      "Epoch 207: val_loss improved from 0.31493 to 0.31472, saving model to folds7.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8464 - loss: 0.3088 - val_accuracy: 0.8383 - val_loss: 0.3147 - learning_rate: 0.0012\n",
      "Epoch 208/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8446 - loss: 0.3047\n",
      "Epoch 208: val_loss did not improve from 0.31472\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8446 - loss: 0.3049 - val_accuracy: 0.8377 - val_loss: 0.3160 - learning_rate: 0.0012\n",
      "Epoch 209/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8362 - loss: 0.3153  \n",
      "Epoch 209: val_loss improved from 0.31472 to 0.31416, saving model to folds7.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8392 - loss: 0.3124 - val_accuracy: 0.8408 - val_loss: 0.3142 - learning_rate: 0.0012\n",
      "Epoch 210/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8394 - loss: 0.3083\n",
      "Epoch 210: val_loss improved from 0.31416 to 0.31396, saving model to folds7.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8396 - loss: 0.3082 - val_accuracy: 0.8408 - val_loss: 0.3140 - learning_rate: 0.0012\n",
      "Epoch 211/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8517 - loss: 0.3000 \n",
      "Epoch 211: val_loss did not improve from 0.31396\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8492 - loss: 0.3027 - val_accuracy: 0.8347 - val_loss: 0.3188 - learning_rate: 0.0012\n",
      "Epoch 212/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8428 - loss: 0.3100\n",
      "Epoch 212: val_loss did not improve from 0.31396\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8429 - loss: 0.3099 - val_accuracy: 0.8340 - val_loss: 0.3170 - learning_rate: 0.0012\n",
      "Epoch 213/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8467 - loss: 0.3033\n",
      "Epoch 213: val_loss did not improve from 0.31396\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8466 - loss: 0.3036 - val_accuracy: 0.8328 - val_loss: 0.3167 - learning_rate: 0.0012\n",
      "Epoch 214/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8437 - loss: 0.3059\n",
      "Epoch 214: val_loss did not improve from 0.31396\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8437 - loss: 0.3059 - val_accuracy: 0.8377 - val_loss: 0.3144 - learning_rate: 0.0012\n",
      "Epoch 215/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8444 - loss: 0.3084\n",
      "Epoch 215: val_loss did not improve from 0.31396\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8443 - loss: 0.3083 - val_accuracy: 0.8408 - val_loss: 0.3142 - learning_rate: 0.0012\n",
      "Epoch 216/300\n",
      "\u001b[1m12/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8441 - loss: 0.3048\n",
      "Epoch 216: val_loss did not improve from 0.31396\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.8440 - loss: 0.3052 - val_accuracy: 0.8389 - val_loss: 0.3154 - learning_rate: 0.0012\n",
      "Epoch 217/300\n",
      "\u001b[1m12/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8423 - loss: 0.3065\n",
      "Epoch 217: val_loss did not improve from 0.31396\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.8426 - loss: 0.3065 - val_accuracy: 0.8432 - val_loss: 0.3144 - learning_rate: 0.0012\n",
      "Epoch 218/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8516 - loss: 0.3013\n",
      "Epoch 218: val_loss did not improve from 0.31396\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.8495 - loss: 0.3027 - val_accuracy: 0.8347 - val_loss: 0.3146 - learning_rate: 0.0012\n",
      "Epoch 219/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8436 - loss: 0.3035\n",
      "Epoch 219: val_loss improved from 0.31396 to 0.31360, saving model to folds7.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.8433 - loss: 0.3045 - val_accuracy: 0.8395 - val_loss: 0.3136 - learning_rate: 0.0012\n",
      "Epoch 220/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8455 - loss: 0.3056\n",
      "Epoch 220: val_loss did not improve from 0.31360\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.8456 - loss: 0.3058 - val_accuracy: 0.8328 - val_loss: 0.3155 - learning_rate: 0.0012\n",
      "Epoch 221/300\n",
      "\u001b[1m12/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8443 - loss: 0.3061\n",
      "Epoch 221: val_loss improved from 0.31360 to 0.31279, saving model to folds7.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.8442 - loss: 0.3063 - val_accuracy: 0.8401 - val_loss: 0.3128 - learning_rate: 0.0012\n",
      "Epoch 222/300\n",
      "\u001b[1m12/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8457 - loss: 0.3040\n",
      "Epoch 222: val_loss did not improve from 0.31279\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.8458 - loss: 0.3045 - val_accuracy: 0.8389 - val_loss: 0.3135 - learning_rate: 0.0012\n",
      "Epoch 223/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8456 - loss: 0.3055\n",
      "Epoch 223: val_loss did not improve from 0.31279\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.8456 - loss: 0.3055 - val_accuracy: 0.8371 - val_loss: 0.3136 - learning_rate: 0.0012\n",
      "Epoch 224/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8433 - loss: 0.3065\n",
      "Epoch 224: val_loss did not improve from 0.31279\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.8435 - loss: 0.3064 - val_accuracy: 0.8389 - val_loss: 0.3142 - learning_rate: 0.0012\n",
      "Epoch 225/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8432 - loss: 0.3046\n",
      "Epoch 225: val_loss improved from 0.31279 to 0.31246, saving model to folds7.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.8433 - loss: 0.3046 - val_accuracy: 0.8401 - val_loss: 0.3125 - learning_rate: 0.0012\n",
      "Epoch 226/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8477 - loss: 0.3028 \n",
      "Epoch 226: val_loss did not improve from 0.31246\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8462 - loss: 0.3039 - val_accuracy: 0.8383 - val_loss: 0.3125 - learning_rate: 0.0012\n",
      "Epoch 227/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8394 - loss: 0.3099 \n",
      "Epoch 227: val_loss improved from 0.31246 to 0.31226, saving model to folds7.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8419 - loss: 0.3078 - val_accuracy: 0.8450 - val_loss: 0.3123 - learning_rate: 0.0012\n",
      "Epoch 228/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8513 - loss: 0.2984\n",
      "Epoch 228: val_loss did not improve from 0.31226\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8508 - loss: 0.2989 - val_accuracy: 0.8383 - val_loss: 0.3125 - learning_rate: 0.0012\n",
      "Epoch 229/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8364 - loss: 0.3129 \n",
      "Epoch 229: val_loss did not improve from 0.31226\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8400 - loss: 0.3093 - val_accuracy: 0.8389 - val_loss: 0.3146 - learning_rate: 0.0012\n",
      "Epoch 230/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8545 - loss: 0.2961 \n",
      "Epoch 230: val_loss did not improve from 0.31226\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8517 - loss: 0.2998 - val_accuracy: 0.8389 - val_loss: 0.3141 - learning_rate: 0.0012\n",
      "Epoch 231/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8451 - loss: 0.3054\n",
      "Epoch 231: val_loss did not improve from 0.31226\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8450 - loss: 0.3054 - val_accuracy: 0.8340 - val_loss: 0.3142 - learning_rate: 0.0012\n",
      "Epoch 232/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8380 - loss: 0.3086\n",
      "Epoch 232: val_loss did not improve from 0.31226\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8387 - loss: 0.3082 - val_accuracy: 0.8383 - val_loss: 0.3127 - learning_rate: 0.0012\n",
      "Epoch 233/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8421 - loss: 0.3043 \n",
      "Epoch 233: val_loss did not improve from 0.31226\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8424 - loss: 0.3050 - val_accuracy: 0.8408 - val_loss: 0.3124 - learning_rate: 0.0012\n",
      "Epoch 234/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8472 - loss: 0.3013\n",
      "Epoch 234: val_loss improved from 0.31226 to 0.31141, saving model to folds7.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8471 - loss: 0.3015 - val_accuracy: 0.8383 - val_loss: 0.3114 - learning_rate: 0.0012\n",
      "Epoch 235/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8447 - loss: 0.3036\n",
      "Epoch 235: val_loss did not improve from 0.31141\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8446 - loss: 0.3036 - val_accuracy: 0.8340 - val_loss: 0.3140 - learning_rate: 0.0012\n",
      "Epoch 236/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8459 - loss: 0.3042\n",
      "Epoch 236: val_loss did not improve from 0.31141\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8458 - loss: 0.3043 - val_accuracy: 0.8353 - val_loss: 0.3129 - learning_rate: 0.0012\n",
      "Epoch 237/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8442 - loss: 0.3017 \n",
      "Epoch 237: val_loss did not improve from 0.31141\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8446 - loss: 0.3027 - val_accuracy: 0.8365 - val_loss: 0.3138 - learning_rate: 0.0012\n",
      "Epoch 238/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8506 - loss: 0.3023 \n",
      "Epoch 238: val_loss did not improve from 0.31141\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8482 - loss: 0.3035 - val_accuracy: 0.8401 - val_loss: 0.3130 - learning_rate: 0.0012\n",
      "Epoch 239/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8467 - loss: 0.3051\n",
      "Epoch 239: val_loss improved from 0.31141 to 0.31126, saving model to folds7.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8466 - loss: 0.3051 - val_accuracy: 0.8420 - val_loss: 0.3113 - learning_rate: 0.0012\n",
      "Epoch 240/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8525 - loss: 0.3000 \n",
      "Epoch 240: val_loss improved from 0.31126 to 0.31096, saving model to folds7.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8501 - loss: 0.3018 - val_accuracy: 0.8371 - val_loss: 0.3110 - learning_rate: 0.0012\n",
      "Epoch 241/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8414 - loss: 0.3095\n",
      "Epoch 241: val_loss did not improve from 0.31096\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8416 - loss: 0.3088 - val_accuracy: 0.8389 - val_loss: 0.3123 - learning_rate: 0.0012\n",
      "Epoch 242/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8466 - loss: 0.3005\n",
      "Epoch 242: val_loss did not improve from 0.31096\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8466 - loss: 0.3007 - val_accuracy: 0.8365 - val_loss: 0.3125 - learning_rate: 0.0012\n",
      "Epoch 243/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8385 - loss: 0.3027 \n",
      "Epoch 243: val_loss did not improve from 0.31096\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8404 - loss: 0.3035 - val_accuracy: 0.8414 - val_loss: 0.3116 - learning_rate: 0.0012\n",
      "Epoch 244/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8491 - loss: 0.2998\n",
      "Epoch 244: val_loss did not improve from 0.31096\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8489 - loss: 0.3000 - val_accuracy: 0.8383 - val_loss: 0.3132 - learning_rate: 0.0012\n",
      "Epoch 245/300\n",
      "\u001b[1m13/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8402 - loss: 0.3097 \n",
      "Epoch 245: val_loss did not improve from 0.31096\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8408 - loss: 0.3089 - val_accuracy: 0.8395 - val_loss: 0.3122 - learning_rate: 0.0012\n",
      "Epoch 246/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8404 - loss: 0.3070 \n",
      "Epoch 246: val_loss did not improve from 0.31096\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8418 - loss: 0.3062 - val_accuracy: 0.8401 - val_loss: 0.3113 - learning_rate: 0.0012\n",
      "Epoch 247/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8418 - loss: 0.3032 \n",
      "Epoch 247: val_loss did not improve from 0.31096\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8432 - loss: 0.3027 - val_accuracy: 0.8420 - val_loss: 0.3111 - learning_rate: 0.0012\n",
      "Epoch 248/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8475 - loss: 0.2974 \n",
      "Epoch 248: val_loss did not improve from 0.31096\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8473 - loss: 0.2994 - val_accuracy: 0.8359 - val_loss: 0.3120 - learning_rate: 0.0012\n",
      "Epoch 249/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8428 - loss: 0.3033\n",
      "Epoch 249: val_loss did not improve from 0.31096\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8427 - loss: 0.3034 - val_accuracy: 0.8432 - val_loss: 0.3116 - learning_rate: 0.0012\n",
      "Epoch 250/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8500 - loss: 0.3014\n",
      "Epoch 250: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "\n",
      "Epoch 250: val_loss improved from 0.31096 to 0.31088, saving model to folds7.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8497 - loss: 0.3015 - val_accuracy: 0.8371 - val_loss: 0.3109 - learning_rate: 0.0012\n",
      "Epoch 251/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8482 - loss: 0.2988 \n",
      "Epoch 251: val_loss did not improve from 0.31088\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8471 - loss: 0.3001 - val_accuracy: 0.8420 - val_loss: 0.3114 - learning_rate: 6.2500e-04\n",
      "Epoch 252/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8473 - loss: 0.3023\n",
      "Epoch 252: val_loss did not improve from 0.31088\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8473 - loss: 0.3024 - val_accuracy: 0.8371 - val_loss: 0.3114 - learning_rate: 6.2500e-04\n",
      "Epoch 253/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8429 - loss: 0.3067\n",
      "Epoch 253: val_loss did not improve from 0.31088\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8430 - loss: 0.3064 - val_accuracy: 0.8365 - val_loss: 0.3110 - learning_rate: 6.2500e-04\n",
      "Epoch 254/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8472 - loss: 0.2983\n",
      "Epoch 254: val_loss improved from 0.31088 to 0.31081, saving model to folds7.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8471 - loss: 0.2986 - val_accuracy: 0.8408 - val_loss: 0.3108 - learning_rate: 6.2500e-04\n",
      "Epoch 255/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8476 - loss: 0.3053 \n",
      "Epoch 255: val_loss did not improve from 0.31081\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8474 - loss: 0.3040 - val_accuracy: 0.8389 - val_loss: 0.3109 - learning_rate: 6.2500e-04\n",
      "Epoch 256/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8439 - loss: 0.3050 \n",
      "Epoch 256: val_loss improved from 0.31081 to 0.31076, saving model to folds7.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8442 - loss: 0.3044 - val_accuracy: 0.8389 - val_loss: 0.3108 - learning_rate: 6.2500e-04\n",
      "Epoch 257/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8504 - loss: 0.2985\n",
      "Epoch 257: val_loss improved from 0.31076 to 0.31059, saving model to folds7.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8502 - loss: 0.2988 - val_accuracy: 0.8371 - val_loss: 0.3106 - learning_rate: 6.2500e-04\n",
      "Epoch 258/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8461 - loss: 0.2986\n",
      "Epoch 258: val_loss did not improve from 0.31059\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8460 - loss: 0.2989 - val_accuracy: 0.8395 - val_loss: 0.3107 - learning_rate: 6.2500e-04\n",
      "Epoch 259/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8499 - loss: 0.2986\n",
      "Epoch 259: val_loss improved from 0.31059 to 0.31013, saving model to folds7.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8497 - loss: 0.2988 - val_accuracy: 0.8395 - val_loss: 0.3101 - learning_rate: 6.2500e-04\n",
      "Epoch 260/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8511 - loss: 0.3014 \n",
      "Epoch 260: val_loss did not improve from 0.31013\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8486 - loss: 0.3017 - val_accuracy: 0.8365 - val_loss: 0.3110 - learning_rate: 6.2500e-04\n",
      "Epoch 261/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8417 - loss: 0.3094\n",
      "Epoch 261: val_loss improved from 0.31013 to 0.30995, saving model to folds7.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8424 - loss: 0.3086 - val_accuracy: 0.8401 - val_loss: 0.3100 - learning_rate: 6.2500e-04\n",
      "Epoch 262/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8432 - loss: 0.3044\n",
      "Epoch 262: val_loss did not improve from 0.30995\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8436 - loss: 0.3041 - val_accuracy: 0.8353 - val_loss: 0.3120 - learning_rate: 6.2500e-04\n",
      "Epoch 263/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8484 - loss: 0.2989\n",
      "Epoch 263: val_loss did not improve from 0.30995\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8483 - loss: 0.2991 - val_accuracy: 0.8414 - val_loss: 0.3105 - learning_rate: 6.2500e-04\n",
      "Epoch 264/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8525 - loss: 0.2998 \n",
      "Epoch 264: val_loss did not improve from 0.30995\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8489 - loss: 0.3019 - val_accuracy: 0.8401 - val_loss: 0.3103 - learning_rate: 6.2500e-04\n",
      "Epoch 265/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8550 - loss: 0.2913 \n",
      "Epoch 265: val_loss did not improve from 0.30995\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8508 - loss: 0.2967 - val_accuracy: 0.8383 - val_loss: 0.3113 - learning_rate: 6.2500e-04\n",
      "Epoch 266/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8486 - loss: 0.2998\n",
      "Epoch 266: val_loss did not improve from 0.30995\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.8484 - loss: 0.3001 - val_accuracy: 0.8377 - val_loss: 0.3108 - learning_rate: 6.2500e-04\n",
      "Epoch 267/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8447 - loss: 0.3036\n",
      "Epoch 267: val_loss did not improve from 0.30995\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.8447 - loss: 0.3031 - val_accuracy: 0.8383 - val_loss: 0.3109 - learning_rate: 6.2500e-04\n",
      "Epoch 268/300\n",
      "\u001b[1m12/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8424 - loss: 0.3069\n",
      "Epoch 268: val_loss improved from 0.30995 to 0.30967, saving model to folds7.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.8431 - loss: 0.3058 - val_accuracy: 0.8389 - val_loss: 0.3097 - learning_rate: 6.2500e-04\n",
      "Epoch 269/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8485 - loss: 0.2967\n",
      "Epoch 269: val_loss did not improve from 0.30967\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.8476 - loss: 0.2982 - val_accuracy: 0.8377 - val_loss: 0.3101 - learning_rate: 6.2500e-04\n",
      "Epoch 270/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8444 - loss: 0.2993\n",
      "Epoch 270: val_loss did not improve from 0.30967\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.8444 - loss: 0.2995 - val_accuracy: 0.8408 - val_loss: 0.3101 - learning_rate: 6.2500e-04\n",
      "Epoch 271/300\n",
      "\u001b[1m12/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8413 - loss: 0.3070 \n",
      "Epoch 271: val_loss did not improve from 0.30967\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.8425 - loss: 0.3057 - val_accuracy: 0.8389 - val_loss: 0.3103 - learning_rate: 6.2500e-04\n",
      "Epoch 272/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8503 - loss: 0.3001\n",
      "Epoch 272: val_loss did not improve from 0.30967\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.8494 - loss: 0.3007 - val_accuracy: 0.8395 - val_loss: 0.3102 - learning_rate: 6.2500e-04\n",
      "Epoch 273/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8481 - loss: 0.2987\n",
      "Epoch 273: val_loss did not improve from 0.30967\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.8473 - loss: 0.2995 - val_accuracy: 0.8353 - val_loss: 0.3110 - learning_rate: 6.2500e-04\n",
      "Epoch 274/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8442 - loss: 0.3068  \n",
      "Epoch 274: val_loss improved from 0.30967 to 0.30949, saving model to folds7.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.8449 - loss: 0.3048 - val_accuracy: 0.8401 - val_loss: 0.3095 - learning_rate: 6.2500e-04\n",
      "Epoch 275/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8455 - loss: 0.3029 \n",
      "Epoch 275: val_loss improved from 0.30949 to 0.30939, saving model to folds7.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8456 - loss: 0.3027 - val_accuracy: 0.8395 - val_loss: 0.3094 - learning_rate: 6.2500e-04\n",
      "Epoch 276/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8500 - loss: 0.2959 \n",
      "Epoch 276: val_loss did not improve from 0.30939\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8466 - loss: 0.2999 - val_accuracy: 0.8395 - val_loss: 0.3095 - learning_rate: 6.2500e-04\n",
      "Epoch 277/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8420 - loss: 0.3088  \n",
      "Epoch 277: val_loss did not improve from 0.30939\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8429 - loss: 0.3063 - val_accuracy: 0.8371 - val_loss: 0.3114 - learning_rate: 6.2500e-04\n",
      "Epoch 278/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8415 - loss: 0.3045 \n",
      "Epoch 278: val_loss did not improve from 0.30939\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8443 - loss: 0.3027 - val_accuracy: 0.8408 - val_loss: 0.3100 - learning_rate: 6.2500e-04\n",
      "Epoch 279/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8530 - loss: 0.2984\n",
      "Epoch 279: val_loss did not improve from 0.30939\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8523 - loss: 0.2988 - val_accuracy: 0.8389 - val_loss: 0.3097 - learning_rate: 6.2500e-04\n",
      "Epoch 280/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8446 - loss: 0.3042\n",
      "Epoch 280: val_loss did not improve from 0.30939\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8447 - loss: 0.3041 - val_accuracy: 0.8377 - val_loss: 0.3102 - learning_rate: 6.2500e-04\n",
      "Epoch 281/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8437 - loss: 0.3060 \n",
      "Epoch 281: val_loss did not improve from 0.30939\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8441 - loss: 0.3054 - val_accuracy: 0.8408 - val_loss: 0.3108 - learning_rate: 6.2500e-04\n",
      "Epoch 282/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8398 - loss: 0.3086\n",
      "Epoch 282: val_loss did not improve from 0.30939\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8402 - loss: 0.3081 - val_accuracy: 0.8377 - val_loss: 0.3102 - learning_rate: 6.2500e-04\n",
      "Epoch 283/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8482 - loss: 0.3013  \n",
      "Epoch 283: val_loss did not improve from 0.30939\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8478 - loss: 0.3015 - val_accuracy: 0.8401 - val_loss: 0.3102 - learning_rate: 6.2500e-04\n",
      "Epoch 284/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8440 - loss: 0.3068 \n",
      "Epoch 284: ReduceLROnPlateau reducing learning rate to 0.0003124999930150807.\n",
      "\n",
      "Epoch 284: val_loss did not improve from 0.30939\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8445 - loss: 0.3051 - val_accuracy: 0.8401 - val_loss: 0.3097 - learning_rate: 6.2500e-04\n",
      "Epoch 285/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8475 - loss: 0.3013\n",
      "Epoch 285: val_loss improved from 0.30939 to 0.30910, saving model to folds7.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8475 - loss: 0.3013 - val_accuracy: 0.8395 - val_loss: 0.3091 - learning_rate: 3.1250e-04\n",
      "Epoch 286/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8455 - loss: 0.3019\n",
      "Epoch 286: val_loss did not improve from 0.30910\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8457 - loss: 0.3019 - val_accuracy: 0.8383 - val_loss: 0.3098 - learning_rate: 3.1250e-04\n",
      "Epoch 287/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8507 - loss: 0.2962\n",
      "Epoch 287: val_loss did not improve from 0.30910\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8505 - loss: 0.2965 - val_accuracy: 0.8383 - val_loss: 0.3095 - learning_rate: 3.1250e-04\n",
      "Epoch 288/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8545 - loss: 0.2952 \n",
      "Epoch 288: val_loss did not improve from 0.30910\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8509 - loss: 0.2976 - val_accuracy: 0.8395 - val_loss: 0.3093 - learning_rate: 3.1250e-04\n",
      "Epoch 289/300\n",
      "\u001b[1m13/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8476 - loss: 0.3005\n",
      "Epoch 289: val_loss did not improve from 0.30910\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8474 - loss: 0.3006 - val_accuracy: 0.8389 - val_loss: 0.3094 - learning_rate: 3.1250e-04\n",
      "Epoch 290/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8501 - loss: 0.2998 \n",
      "Epoch 290: val_loss did not improve from 0.30910\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8485 - loss: 0.3003 - val_accuracy: 0.8377 - val_loss: 0.3095 - learning_rate: 3.1250e-04\n",
      "Epoch 291/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8481 - loss: 0.2998 \n",
      "Epoch 291: val_loss did not improve from 0.30910\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8477 - loss: 0.3003 - val_accuracy: 0.8408 - val_loss: 0.3095 - learning_rate: 3.1250e-04\n",
      "Epoch 292/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8496 - loss: 0.2991\n",
      "Epoch 292: val_loss did not improve from 0.30910\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8493 - loss: 0.2992 - val_accuracy: 0.8383 - val_loss: 0.3095 - learning_rate: 3.1250e-04\n",
      "Epoch 293/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8455 - loss: 0.2985\n",
      "Epoch 293: val_loss did not improve from 0.30910\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8455 - loss: 0.2986 - val_accuracy: 0.8359 - val_loss: 0.3107 - learning_rate: 3.1250e-04\n",
      "Epoch 294/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8394 - loss: 0.3101 \n",
      "Epoch 294: val_loss improved from 0.30910 to 0.30907, saving model to folds7.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8422 - loss: 0.3060 - val_accuracy: 0.8414 - val_loss: 0.3091 - learning_rate: 3.1250e-04\n",
      "Epoch 295/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8506 - loss: 0.2946 \n",
      "Epoch 295: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "\n",
      "Epoch 295: val_loss did not improve from 0.30907\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8486 - loss: 0.2983 - val_accuracy: 0.8395 - val_loss: 0.3092 - learning_rate: 3.1250e-04\n",
      "Epoch 296/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8453 - loss: 0.2997\n",
      "Epoch 296: val_loss did not improve from 0.30907\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8455 - loss: 0.2999 - val_accuracy: 0.8389 - val_loss: 0.3095 - learning_rate: 1.5625e-04\n",
      "Epoch 297/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8406 - loss: 0.3023 \n",
      "Epoch 297: val_loss did not improve from 0.30907\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8432 - loss: 0.3020 - val_accuracy: 0.8395 - val_loss: 0.3093 - learning_rate: 1.5625e-04\n",
      "Epoch 298/300\n",
      "\u001b[1m13/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8476 - loss: 0.3004 \n",
      "Epoch 298: val_loss did not improve from 0.30907\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8475 - loss: 0.3004 - val_accuracy: 0.8371 - val_loss: 0.3099 - learning_rate: 1.5625e-04\n",
      "Epoch 299/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8451 - loss: 0.3041\n",
      "Epoch 299: val_loss did not improve from 0.30907\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8452 - loss: 0.3039 - val_accuracy: 0.8389 - val_loss: 0.3092 - learning_rate: 1.5625e-04\n",
      "Epoch 300/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8479 - loss: 0.2996\n",
      "Epoch 300: val_loss did not improve from 0.30907\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8478 - loss: 0.2996 - val_accuracy: 0.8401 - val_loss: 0.3095 - learning_rate: 1.5625e-04\n",
      "Restoring model weights from the end of the best epoch: 294.\n",
      " Computing LSTM feature importance...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:01<00:00,  5.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- > Fold 9 < ---------------\n",
      "Epoch 1/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.6624 - loss: 0.6084\n",
      "Epoch 1: val_loss improved from inf to 0.55007, saving model to folds8.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 47ms/step - accuracy: 0.6776 - loss: 0.5918 - val_accuracy: 0.7065 - val_loss: 0.5501 - learning_rate: 0.0100\n",
      "Epoch 2/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7101 - loss: 0.5287\n",
      "Epoch 2: val_loss improved from 0.55007 to 0.52442, saving model to folds8.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.7126 - loss: 0.5258 - val_accuracy: 0.7047 - val_loss: 0.5244 - learning_rate: 0.0100\n",
      "Epoch 3/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7211 - loss: 0.5042\n",
      "Epoch 3: val_loss improved from 0.52442 to 0.51056, saving model to folds8.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.7219 - loss: 0.5024 - val_accuracy: 0.6986 - val_loss: 0.5106 - learning_rate: 0.0100\n",
      "Epoch 4/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7214 - loss: 0.4888\n",
      "Epoch 4: val_loss improved from 0.51056 to 0.50681, saving model to folds8.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.7218 - loss: 0.4884 - val_accuracy: 0.7047 - val_loss: 0.5068 - learning_rate: 0.0100\n",
      "Epoch 5/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7291 - loss: 0.4846\n",
      "Epoch 5: val_loss improved from 0.50681 to 0.50169, saving model to folds8.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.7280 - loss: 0.4843 - val_accuracy: 0.6992 - val_loss: 0.5017 - learning_rate: 0.0100\n",
      "Epoch 6/300\n",
      "\u001b[1m12/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7313 - loss: 0.4762\n",
      "Epoch 6: val_loss improved from 0.50169 to 0.48665, saving model to folds8.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.7312 - loss: 0.4768 - val_accuracy: 0.7145 - val_loss: 0.4867 - learning_rate: 0.0100\n",
      "Epoch 7/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7327 - loss: 0.4760  \n",
      "Epoch 7: val_loss improved from 0.48665 to 0.48476, saving model to folds8.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7351 - loss: 0.4743 - val_accuracy: 0.7218 - val_loss: 0.4848 - learning_rate: 0.0100\n",
      "Epoch 8/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7349 - loss: 0.4698 \n",
      "Epoch 8: val_loss did not improve from 0.48476\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7366 - loss: 0.4702 - val_accuracy: 0.7120 - val_loss: 0.4886 - learning_rate: 0.0100\n",
      "Epoch 9/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7348 - loss: 0.4743\n",
      "Epoch 9: val_loss improved from 0.48476 to 0.47976, saving model to folds8.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.7351 - loss: 0.4740 - val_accuracy: 0.7193 - val_loss: 0.4798 - learning_rate: 0.0100\n",
      "Epoch 10/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7320 - loss: 0.4726 \n",
      "Epoch 10: val_loss did not improve from 0.47976\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7324 - loss: 0.4730 - val_accuracy: 0.7059 - val_loss: 0.4869 - learning_rate: 0.0100\n",
      "Epoch 11/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7339 - loss: 0.4698 \n",
      "Epoch 11: val_loss improved from 0.47976 to 0.47479, saving model to folds8.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7374 - loss: 0.4689 - val_accuracy: 0.7291 - val_loss: 0.4748 - learning_rate: 0.0100\n",
      "Epoch 12/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7488 - loss: 0.4578 \n",
      "Epoch 12: val_loss did not improve from 0.47479\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7466 - loss: 0.4608 - val_accuracy: 0.7352 - val_loss: 0.4774 - learning_rate: 0.0100\n",
      "Epoch 13/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7456 - loss: 0.4650\n",
      "Epoch 13: val_loss improved from 0.47479 to 0.47240, saving model to folds8.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7457 - loss: 0.4648 - val_accuracy: 0.7218 - val_loss: 0.4724 - learning_rate: 0.0100\n",
      "Epoch 14/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7375 - loss: 0.4636 \n",
      "Epoch 14: val_loss did not improve from 0.47240\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7359 - loss: 0.4651 - val_accuracy: 0.7279 - val_loss: 0.4820 - learning_rate: 0.0100\n",
      "Epoch 15/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7504 - loss: 0.4566 \n",
      "Epoch 15: val_loss improved from 0.47240 to 0.47062, saving model to folds8.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7483 - loss: 0.4573 - val_accuracy: 0.7248 - val_loss: 0.4706 - learning_rate: 0.0100\n",
      "Epoch 16/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7389 - loss: 0.4586\n",
      "Epoch 16: val_loss did not improve from 0.47062\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7389 - loss: 0.4591 - val_accuracy: 0.7157 - val_loss: 0.4799 - learning_rate: 0.0100\n",
      "Epoch 17/300\n",
      "\u001b[1m13/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7486 - loss: 0.4567\n",
      "Epoch 17: val_loss did not improve from 0.47062\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7480 - loss: 0.4566 - val_accuracy: 0.7248 - val_loss: 0.4724 - learning_rate: 0.0100\n",
      "Epoch 18/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7498 - loss: 0.4508  \n",
      "Epoch 18: val_loss did not improve from 0.47062\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7485 - loss: 0.4531 - val_accuracy: 0.7096 - val_loss: 0.4710 - learning_rate: 0.0100\n",
      "Epoch 19/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7507 - loss: 0.4491 \n",
      "Epoch 19: val_loss improved from 0.47062 to 0.46443, saving model to folds8.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7487 - loss: 0.4503 - val_accuracy: 0.7254 - val_loss: 0.4644 - learning_rate: 0.0100\n",
      "Epoch 20/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7486 - loss: 0.4466 \n",
      "Epoch 20: val_loss improved from 0.46443 to 0.46242, saving model to folds8.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7472 - loss: 0.4476 - val_accuracy: 0.7206 - val_loss: 0.4624 - learning_rate: 0.0100\n",
      "Epoch 21/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7455 - loss: 0.4479\n",
      "Epoch 21: val_loss improved from 0.46242 to 0.45980, saving model to folds8.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.7454 - loss: 0.4480 - val_accuracy: 0.7407 - val_loss: 0.4598 - learning_rate: 0.0100\n",
      "Epoch 22/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7545 - loss: 0.4453 \n",
      "Epoch 22: val_loss did not improve from 0.45980\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7530 - loss: 0.4459 - val_accuracy: 0.7248 - val_loss: 0.4631 - learning_rate: 0.0100\n",
      "Epoch 23/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7495 - loss: 0.4461\n",
      "Epoch 23: val_loss improved from 0.45980 to 0.45787, saving model to folds8.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7496 - loss: 0.4462 - val_accuracy: 0.7419 - val_loss: 0.4579 - learning_rate: 0.0100\n",
      "Epoch 24/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7577 - loss: 0.4464\n",
      "Epoch 24: val_loss did not improve from 0.45787\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7571 - loss: 0.4468 - val_accuracy: 0.7230 - val_loss: 0.4682 - learning_rate: 0.0100\n",
      "Epoch 25/300\n",
      "\u001b[1m13/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7470 - loss: 0.4484\n",
      "Epoch 25: val_loss did not improve from 0.45787\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7467 - loss: 0.4484 - val_accuracy: 0.7334 - val_loss: 0.4614 - learning_rate: 0.0100\n",
      "Epoch 26/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7461 - loss: 0.4420 \n",
      "Epoch 26: val_loss improved from 0.45787 to 0.45768, saving model to folds8.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7479 - loss: 0.4419 - val_accuracy: 0.7187 - val_loss: 0.4577 - learning_rate: 0.0100\n",
      "Epoch 27/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7445 - loss: 0.4415 \n",
      "Epoch 27: val_loss improved from 0.45768 to 0.45747, saving model to folds8.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7454 - loss: 0.4424 - val_accuracy: 0.7328 - val_loss: 0.4575 - learning_rate: 0.0100\n",
      "Epoch 28/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7489 - loss: 0.4423 \n",
      "Epoch 28: val_loss improved from 0.45747 to 0.45154, saving model to folds8.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.7489 - loss: 0.4403 - val_accuracy: 0.7315 - val_loss: 0.4515 - learning_rate: 0.0100\n",
      "Epoch 29/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7554 - loss: 0.4366 \n",
      "Epoch 29: val_loss did not improve from 0.45154\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7551 - loss: 0.4362 - val_accuracy: 0.7383 - val_loss: 0.4553 - learning_rate: 0.0100\n",
      "Epoch 30/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7556 - loss: 0.4356\n",
      "Epoch 30: val_loss improved from 0.45154 to 0.44765, saving model to folds8.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.7558 - loss: 0.4354 - val_accuracy: 0.7425 - val_loss: 0.4477 - learning_rate: 0.0100\n",
      "Epoch 31/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7581 - loss: 0.4307 \n",
      "Epoch 31: val_loss did not improve from 0.44765\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7590 - loss: 0.4322 - val_accuracy: 0.7291 - val_loss: 0.4632 - learning_rate: 0.0100\n",
      "Epoch 32/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7499 - loss: 0.4425 \n",
      "Epoch 32: val_loss did not improve from 0.44765\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7520 - loss: 0.4432 - val_accuracy: 0.7456 - val_loss: 0.4547 - learning_rate: 0.0100\n",
      "Epoch 33/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7623 - loss: 0.4346\n",
      "Epoch 33: val_loss did not improve from 0.44765\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7621 - loss: 0.4346 - val_accuracy: 0.7389 - val_loss: 0.4490 - learning_rate: 0.0100\n",
      "Epoch 34/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7516 - loss: 0.4357\n",
      "Epoch 34: val_loss improved from 0.44765 to 0.44508, saving model to folds8.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.7517 - loss: 0.4356 - val_accuracy: 0.7425 - val_loss: 0.4451 - learning_rate: 0.0100\n",
      "Epoch 35/300\n",
      "\u001b[1m13/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7657 - loss: 0.4258\n",
      "Epoch 35: val_loss improved from 0.44508 to 0.44285, saving model to folds8.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.7655 - loss: 0.4261 - val_accuracy: 0.7523 - val_loss: 0.4429 - learning_rate: 0.0100\n",
      "Epoch 36/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7746 - loss: 0.4165 \n",
      "Epoch 36: val_loss did not improve from 0.44285\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7718 - loss: 0.4193 - val_accuracy: 0.7401 - val_loss: 0.4477 - learning_rate: 0.0100\n",
      "Epoch 37/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7651 - loss: 0.4264 \n",
      "Epoch 37: val_loss did not improve from 0.44285\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7638 - loss: 0.4256 - val_accuracy: 0.7480 - val_loss: 0.4429 - learning_rate: 0.0100\n",
      "Epoch 38/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7581 - loss: 0.4285\n",
      "Epoch 38: val_loss improved from 0.44285 to 0.43957, saving model to folds8.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.7581 - loss: 0.4287 - val_accuracy: 0.7535 - val_loss: 0.4396 - learning_rate: 0.0100\n",
      "Epoch 39/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7681 - loss: 0.4174\n",
      "Epoch 39: val_loss did not improve from 0.43957\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7676 - loss: 0.4177 - val_accuracy: 0.7328 - val_loss: 0.4516 - learning_rate: 0.0100\n",
      "Epoch 40/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7603 - loss: 0.4262\n",
      "Epoch 40: val_loss improved from 0.43957 to 0.43919, saving model to folds8.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7604 - loss: 0.4261 - val_accuracy: 0.7462 - val_loss: 0.4392 - learning_rate: 0.0100\n",
      "Epoch 41/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7657 - loss: 0.4170 \n",
      "Epoch 41: val_loss improved from 0.43919 to 0.43189, saving model to folds8.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7654 - loss: 0.4192 - val_accuracy: 0.7535 - val_loss: 0.4319 - learning_rate: 0.0100\n",
      "Epoch 42/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7576 - loss: 0.4267 \n",
      "Epoch 42: val_loss did not improve from 0.43189\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7599 - loss: 0.4253 - val_accuracy: 0.7669 - val_loss: 0.4319 - learning_rate: 0.0100\n",
      "Epoch 43/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7769 - loss: 0.4124\n",
      "Epoch 43: val_loss did not improve from 0.43189\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7768 - loss: 0.4126 - val_accuracy: 0.7389 - val_loss: 0.4349 - learning_rate: 0.0100\n",
      "Epoch 44/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7564 - loss: 0.4237 \n",
      "Epoch 44: val_loss improved from 0.43189 to 0.42565, saving model to folds8.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7642 - loss: 0.4190 - val_accuracy: 0.7492 - val_loss: 0.4257 - learning_rate: 0.0100\n",
      "Epoch 45/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7684 - loss: 0.4137\n",
      "Epoch 45: val_loss did not improve from 0.42565\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7686 - loss: 0.4135 - val_accuracy: 0.7456 - val_loss: 0.4273 - learning_rate: 0.0100\n",
      "Epoch 46/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7634 - loss: 0.4116\n",
      "Epoch 46: val_loss did not improve from 0.42565\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7635 - loss: 0.4119 - val_accuracy: 0.7547 - val_loss: 0.4383 - learning_rate: 0.0100\n",
      "Epoch 47/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7651 - loss: 0.4193\n",
      "Epoch 47: val_loss improved from 0.42565 to 0.42370, saving model to folds8.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7653 - loss: 0.4190 - val_accuracy: 0.7608 - val_loss: 0.4237 - learning_rate: 0.0100\n",
      "Epoch 48/300\n",
      "\u001b[1m13/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7742 - loss: 0.4043\n",
      "Epoch 48: val_loss did not improve from 0.42370\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.7739 - loss: 0.4050 - val_accuracy: 0.7535 - val_loss: 0.4272 - learning_rate: 0.0100\n",
      "Epoch 49/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7729 - loss: 0.4121\n",
      "Epoch 49: val_loss improved from 0.42370 to 0.42093, saving model to folds8.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.7728 - loss: 0.4118 - val_accuracy: 0.7559 - val_loss: 0.4209 - learning_rate: 0.0100\n",
      "Epoch 50/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7665 - loss: 0.4138\n",
      "Epoch 50: val_loss did not improve from 0.42093\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.7680 - loss: 0.4138 - val_accuracy: 0.7639 - val_loss: 0.4292 - learning_rate: 0.0100\n",
      "Epoch 51/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7689 - loss: 0.4207\n",
      "Epoch 51: val_loss did not improve from 0.42093\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.7690 - loss: 0.4205 - val_accuracy: 0.7383 - val_loss: 0.4383 - learning_rate: 0.0100\n",
      "Epoch 52/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7630 - loss: 0.4212\n",
      "Epoch 52: val_loss did not improve from 0.42093\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.7635 - loss: 0.4208 - val_accuracy: 0.7456 - val_loss: 0.4267 - learning_rate: 0.0100\n",
      "Epoch 53/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7777 - loss: 0.4134\n",
      "Epoch 53: val_loss did not improve from 0.42093\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.7769 - loss: 0.4147 - val_accuracy: 0.7590 - val_loss: 0.4361 - learning_rate: 0.0100\n",
      "Epoch 54/300\n",
      "\u001b[1m13/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7706 - loss: 0.4118\n",
      "Epoch 54: val_loss improved from 0.42093 to 0.41705, saving model to folds8.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.7713 - loss: 0.4118 - val_accuracy: 0.7724 - val_loss: 0.4170 - learning_rate: 0.0100\n",
      "Epoch 55/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7758 - loss: 0.4077\n",
      "Epoch 55: val_loss did not improve from 0.41705\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.7761 - loss: 0.4073 - val_accuracy: 0.7761 - val_loss: 0.4251 - learning_rate: 0.0100\n",
      "Epoch 56/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7790 - loss: 0.4094\n",
      "Epoch 56: val_loss improved from 0.41705 to 0.41390, saving model to folds8.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.7781 - loss: 0.4085 - val_accuracy: 0.7572 - val_loss: 0.4139 - learning_rate: 0.0100\n",
      "Epoch 57/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7779 - loss: 0.4036\n",
      "Epoch 57: val_loss did not improve from 0.41390\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.7776 - loss: 0.4032 - val_accuracy: 0.7608 - val_loss: 0.4152 - learning_rate: 0.0100\n",
      "Epoch 58/300\n",
      "\u001b[1m13/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7774 - loss: 0.4013 \n",
      "Epoch 58: val_loss did not improve from 0.41390\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7775 - loss: 0.4015 - val_accuracy: 0.7596 - val_loss: 0.4185 - learning_rate: 0.0100\n",
      "Epoch 59/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7772 - loss: 0.4031\n",
      "Epoch 59: val_loss did not improve from 0.41390\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7773 - loss: 0.4032 - val_accuracy: 0.7718 - val_loss: 0.4152 - learning_rate: 0.0100\n",
      "Epoch 60/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7665 - loss: 0.4128\n",
      "Epoch 60: val_loss did not improve from 0.41390\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7669 - loss: 0.4125 - val_accuracy: 0.7645 - val_loss: 0.4208 - learning_rate: 0.0100\n",
      "Epoch 61/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7814 - loss: 0.4070\n",
      "Epoch 61: val_loss did not improve from 0.41390\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7805 - loss: 0.4076 - val_accuracy: 0.7669 - val_loss: 0.4232 - learning_rate: 0.0100\n",
      "Epoch 62/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7771 - loss: 0.4083\n",
      "Epoch 62: val_loss did not improve from 0.41390\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7771 - loss: 0.4083 - val_accuracy: 0.7804 - val_loss: 0.4157 - learning_rate: 0.0100\n",
      "Epoch 63/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7843 - loss: 0.3978 \n",
      "Epoch 63: val_loss did not improve from 0.41390\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7819 - loss: 0.4008 - val_accuracy: 0.7584 - val_loss: 0.4154 - learning_rate: 0.0100\n",
      "Epoch 64/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7787 - loss: 0.4037\n",
      "Epoch 64: val_loss improved from 0.41390 to 0.41171, saving model to folds8.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.7787 - loss: 0.4036 - val_accuracy: 0.7584 - val_loss: 0.4117 - learning_rate: 0.0100\n",
      "Epoch 65/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7768 - loss: 0.4020\n",
      "Epoch 65: val_loss improved from 0.41171 to 0.41100, saving model to folds8.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7772 - loss: 0.4016 - val_accuracy: 0.7682 - val_loss: 0.4110 - learning_rate: 0.0100\n",
      "Epoch 66/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7828 - loss: 0.3948 \n",
      "Epoch 66: val_loss improved from 0.41100 to 0.40980, saving model to folds8.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7817 - loss: 0.3950 - val_accuracy: 0.7663 - val_loss: 0.4098 - learning_rate: 0.0100\n",
      "Epoch 67/300\n",
      "\u001b[1m13/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7832 - loss: 0.3925\n",
      "Epoch 67: val_loss improved from 0.40980 to 0.40683, saving model to folds8.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.7826 - loss: 0.3933 - val_accuracy: 0.7822 - val_loss: 0.4068 - learning_rate: 0.0100\n",
      "Epoch 68/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7897 - loss: 0.3876 \n",
      "Epoch 68: val_loss improved from 0.40683 to 0.40609, saving model to folds8.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.7885 - loss: 0.3891 - val_accuracy: 0.7767 - val_loss: 0.4061 - learning_rate: 0.0100\n",
      "Epoch 69/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7919 - loss: 0.3850 \n",
      "Epoch 69: val_loss improved from 0.40609 to 0.40094, saving model to folds8.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.7883 - loss: 0.3883 - val_accuracy: 0.7956 - val_loss: 0.4009 - learning_rate: 0.0100\n",
      "Epoch 70/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7939 - loss: 0.3909 \n",
      "Epoch 70: val_loss did not improve from 0.40094\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7937 - loss: 0.3898 - val_accuracy: 0.7761 - val_loss: 0.4033 - learning_rate: 0.0100\n",
      "Epoch 71/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7852 - loss: 0.3973  \n",
      "Epoch 71: val_loss did not improve from 0.40094\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7874 - loss: 0.3926 - val_accuracy: 0.7700 - val_loss: 0.4016 - learning_rate: 0.0100\n",
      "Epoch 72/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7864 - loss: 0.3908\n",
      "Epoch 72: val_loss did not improve from 0.40094\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7865 - loss: 0.3907 - val_accuracy: 0.7743 - val_loss: 0.4043 - learning_rate: 0.0100\n",
      "Epoch 73/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7890 - loss: 0.3887 \n",
      "Epoch 73: val_loss improved from 0.40094 to 0.40035, saving model to folds8.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7880 - loss: 0.3880 - val_accuracy: 0.7871 - val_loss: 0.4003 - learning_rate: 0.0100\n",
      "Epoch 74/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7896 - loss: 0.3865 \n",
      "Epoch 74: val_loss improved from 0.40035 to 0.39973, saving model to folds8.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7910 - loss: 0.3863 - val_accuracy: 0.7840 - val_loss: 0.3997 - learning_rate: 0.0100\n",
      "Epoch 75/300\n",
      "\u001b[1m13/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7922 - loss: 0.3835\n",
      "Epoch 75: val_loss improved from 0.39973 to 0.39399, saving model to folds8.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.7918 - loss: 0.3838 - val_accuracy: 0.7877 - val_loss: 0.3940 - learning_rate: 0.0100\n",
      "Epoch 76/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7909 - loss: 0.3831 \n",
      "Epoch 76: val_loss improved from 0.39399 to 0.39357, saving model to folds8.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7915 - loss: 0.3840 - val_accuracy: 0.7932 - val_loss: 0.3936 - learning_rate: 0.0100\n",
      "Epoch 77/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7910 - loss: 0.3804 \n",
      "Epoch 77: val_loss did not improve from 0.39357\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7903 - loss: 0.3815 - val_accuracy: 0.7822 - val_loss: 0.3990 - learning_rate: 0.0100\n",
      "Epoch 78/300\n",
      "\u001b[1m13/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7932 - loss: 0.3796\n",
      "Epoch 78: val_loss did not improve from 0.39357\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7935 - loss: 0.3798 - val_accuracy: 0.7706 - val_loss: 0.3965 - learning_rate: 0.0100\n",
      "Epoch 79/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7887 - loss: 0.3832 \n",
      "Epoch 79: val_loss did not improve from 0.39357\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7920 - loss: 0.3822 - val_accuracy: 0.7779 - val_loss: 0.3967 - learning_rate: 0.0100\n",
      "Epoch 80/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7978 - loss: 0.3797\n",
      "Epoch 80: val_loss did not improve from 0.39357\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7972 - loss: 0.3800 - val_accuracy: 0.7736 - val_loss: 0.3998 - learning_rate: 0.0100\n",
      "Epoch 81/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7866 - loss: 0.3879  \n",
      "Epoch 81: val_loss did not improve from 0.39357\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7886 - loss: 0.3861 - val_accuracy: 0.7804 - val_loss: 0.3983 - learning_rate: 0.0100\n",
      "Epoch 82/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7926 - loss: 0.3832\n",
      "Epoch 82: val_loss improved from 0.39357 to 0.38939, saving model to folds8.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7926 - loss: 0.3832 - val_accuracy: 0.7938 - val_loss: 0.3894 - learning_rate: 0.0100\n",
      "Epoch 83/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7989 - loss: 0.3744 \n",
      "Epoch 83: val_loss did not improve from 0.38939\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7964 - loss: 0.3762 - val_accuracy: 0.7883 - val_loss: 0.3953 - learning_rate: 0.0100\n",
      "Epoch 84/300\n",
      "\u001b[1m13/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7892 - loss: 0.3854 \n",
      "Epoch 84: val_loss did not improve from 0.38939\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7898 - loss: 0.3847 - val_accuracy: 0.7858 - val_loss: 0.3950 - learning_rate: 0.0100\n",
      "Epoch 85/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7899 - loss: 0.3841\n",
      "Epoch 85: val_loss did not improve from 0.38939\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7897 - loss: 0.3846 - val_accuracy: 0.7736 - val_loss: 0.4021 - learning_rate: 0.0100\n",
      "Epoch 86/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7739 - loss: 0.3986  \n",
      "Epoch 86: val_loss did not improve from 0.38939\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7804 - loss: 0.3922 - val_accuracy: 0.7730 - val_loss: 0.3995 - learning_rate: 0.0100\n",
      "Epoch 87/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7915 - loss: 0.3830 \n",
      "Epoch 87: val_loss did not improve from 0.38939\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7919 - loss: 0.3822 - val_accuracy: 0.7846 - val_loss: 0.4001 - learning_rate: 0.0100\n",
      "Epoch 88/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7900 - loss: 0.3799\n",
      "Epoch 88: val_loss did not improve from 0.38939\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7900 - loss: 0.3800 - val_accuracy: 0.7743 - val_loss: 0.3935 - learning_rate: 0.0100\n",
      "Epoch 89/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7922 - loss: 0.3793 \n",
      "Epoch 89: val_loss did not improve from 0.38939\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7928 - loss: 0.3794 - val_accuracy: 0.7840 - val_loss: 0.3921 - learning_rate: 0.0100\n",
      "Epoch 90/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7938 - loss: 0.3783\n",
      "Epoch 90: val_loss did not improve from 0.38939\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7939 - loss: 0.3782 - val_accuracy: 0.7797 - val_loss: 0.4069 - learning_rate: 0.0100\n",
      "Epoch 91/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7871 - loss: 0.3828\n",
      "Epoch 91: val_loss did not improve from 0.38939\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7872 - loss: 0.3827 - val_accuracy: 0.7877 - val_loss: 0.3984 - learning_rate: 0.0100\n",
      "Epoch 92/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7882 - loss: 0.3906 \n",
      "Epoch 92: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
      "\n",
      "Epoch 92: val_loss did not improve from 0.38939\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7882 - loss: 0.3871 - val_accuracy: 0.7913 - val_loss: 0.3911 - learning_rate: 0.0100\n",
      "Epoch 93/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7963 - loss: 0.3751\n",
      "Epoch 93: val_loss improved from 0.38939 to 0.38334, saving model to folds8.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.7965 - loss: 0.3748 - val_accuracy: 0.7993 - val_loss: 0.3833 - learning_rate: 0.0050\n",
      "Epoch 94/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8013 - loss: 0.3751 \n",
      "Epoch 94: val_loss improved from 0.38334 to 0.38290, saving model to folds8.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8011 - loss: 0.3731 - val_accuracy: 0.7883 - val_loss: 0.3829 - learning_rate: 0.0050\n",
      "Epoch 95/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7957 - loss: 0.3677 \n",
      "Epoch 95: val_loss improved from 0.38290 to 0.38129, saving model to folds8.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7979 - loss: 0.3672 - val_accuracy: 0.7974 - val_loss: 0.3813 - learning_rate: 0.0050\n",
      "Epoch 96/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8038 - loss: 0.3637\n",
      "Epoch 96: val_loss did not improve from 0.38129\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8035 - loss: 0.3639 - val_accuracy: 0.7828 - val_loss: 0.3859 - learning_rate: 0.0050\n",
      "Epoch 97/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7994 - loss: 0.3665\n",
      "Epoch 97: val_loss did not improve from 0.38129\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7995 - loss: 0.3665 - val_accuracy: 0.7968 - val_loss: 0.3817 - learning_rate: 0.0050\n",
      "Epoch 98/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8056 - loss: 0.3626 \n",
      "Epoch 98: val_loss did not improve from 0.38129\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8041 - loss: 0.3639 - val_accuracy: 0.7797 - val_loss: 0.3861 - learning_rate: 0.0050\n",
      "Epoch 99/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8007 - loss: 0.3676\n",
      "Epoch 99: val_loss did not improve from 0.38129\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8007 - loss: 0.3676 - val_accuracy: 0.7944 - val_loss: 0.3816 - learning_rate: 0.0050\n",
      "Epoch 100/300\n",
      "\u001b[1m13/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8057 - loss: 0.3619\n",
      "Epoch 100: val_loss did not improve from 0.38129\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.8050 - loss: 0.3624 - val_accuracy: 0.7950 - val_loss: 0.3829 - learning_rate: 0.0050\n",
      "Epoch 101/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8014 - loss: 0.3637\n",
      "Epoch 101: val_loss did not improve from 0.38129\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.8015 - loss: 0.3639 - val_accuracy: 0.7846 - val_loss: 0.3866 - learning_rate: 0.0050\n",
      "Epoch 102/300\n",
      "\u001b[1m13/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7969 - loss: 0.3675\n",
      "Epoch 102: val_loss improved from 0.38129 to 0.37671, saving model to folds8.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.7979 - loss: 0.3674 - val_accuracy: 0.8005 - val_loss: 0.3767 - learning_rate: 0.0050\n",
      "Epoch 103/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8042 - loss: 0.3645\n",
      "Epoch 103: val_loss did not improve from 0.37671\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.8042 - loss: 0.3645 - val_accuracy: 0.7993 - val_loss: 0.3784 - learning_rate: 0.0050\n",
      "Epoch 104/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8091 - loss: 0.3574\n",
      "Epoch 104: val_loss did not improve from 0.37671\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.8087 - loss: 0.3580 - val_accuracy: 0.7828 - val_loss: 0.3797 - learning_rate: 0.0050\n",
      "Epoch 105/300\n",
      "\u001b[1m13/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7996 - loss: 0.3630\n",
      "Epoch 105: val_loss did not improve from 0.37671\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.7999 - loss: 0.3634 - val_accuracy: 0.7987 - val_loss: 0.3790 - learning_rate: 0.0050\n",
      "Epoch 106/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7972 - loss: 0.3675\n",
      "Epoch 106: val_loss did not improve from 0.37671\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.7982 - loss: 0.3665 - val_accuracy: 0.7889 - val_loss: 0.3824 - learning_rate: 0.0050\n",
      "Epoch 107/300\n",
      "\u001b[1m12/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8013 - loss: 0.3627\n",
      "Epoch 107: val_loss improved from 0.37671 to 0.37663, saving model to folds8.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.8011 - loss: 0.3631 - val_accuracy: 0.8005 - val_loss: 0.3766 - learning_rate: 0.0050\n",
      "Epoch 108/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8044 - loss: 0.3633\n",
      "Epoch 108: val_loss did not improve from 0.37663\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.8050 - loss: 0.3628 - val_accuracy: 0.7956 - val_loss: 0.3767 - learning_rate: 0.0050\n",
      "Epoch 109/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7984 - loss: 0.3705\n",
      "Epoch 109: val_loss improved from 0.37663 to 0.37564, saving model to folds8.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.8010 - loss: 0.3673 - val_accuracy: 0.8060 - val_loss: 0.3756 - learning_rate: 0.0050\n",
      "Epoch 110/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8112 - loss: 0.3544 \n",
      "Epoch 110: val_loss improved from 0.37564 to 0.37498, saving model to folds8.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8081 - loss: 0.3577 - val_accuracy: 0.8035 - val_loss: 0.3750 - learning_rate: 0.0050\n",
      "Epoch 111/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7924 - loss: 0.3684  \n",
      "Epoch 111: val_loss did not improve from 0.37498\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7979 - loss: 0.3653 - val_accuracy: 0.7895 - val_loss: 0.3783 - learning_rate: 0.0050\n",
      "Epoch 112/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7973 - loss: 0.3597 \n",
      "Epoch 112: val_loss improved from 0.37498 to 0.37495, saving model to folds8.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8013 - loss: 0.3594 - val_accuracy: 0.7956 - val_loss: 0.3749 - learning_rate: 0.0050\n",
      "Epoch 113/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8010 - loss: 0.3597\n",
      "Epoch 113: val_loss improved from 0.37495 to 0.37468, saving model to folds8.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8012 - loss: 0.3598 - val_accuracy: 0.8005 - val_loss: 0.3747 - learning_rate: 0.0050\n",
      "Epoch 114/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8112 - loss: 0.3528 \n",
      "Epoch 114: val_loss did not improve from 0.37468\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8090 - loss: 0.3558 - val_accuracy: 0.7987 - val_loss: 0.3751 - learning_rate: 0.0050\n",
      "Epoch 115/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8130 - loss: 0.3566 \n",
      "Epoch 115: val_loss did not improve from 0.37468\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8104 - loss: 0.3583 - val_accuracy: 0.7993 - val_loss: 0.3777 - learning_rate: 0.0050\n",
      "Epoch 116/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8086 - loss: 0.3606\n",
      "Epoch 116: val_loss did not improve from 0.37468\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8083 - loss: 0.3607 - val_accuracy: 0.7968 - val_loss: 0.3785 - learning_rate: 0.0050\n",
      "Epoch 117/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8032 - loss: 0.3627\n",
      "Epoch 117: val_loss did not improve from 0.37468\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8033 - loss: 0.3628 - val_accuracy: 0.7901 - val_loss: 0.3811 - learning_rate: 0.0050\n",
      "Epoch 118/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8042 - loss: 0.3673\n",
      "Epoch 118: val_loss improved from 0.37468 to 0.37400, saving model to folds8.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8043 - loss: 0.3672 - val_accuracy: 0.7944 - val_loss: 0.3740 - learning_rate: 0.0050\n",
      "Epoch 119/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8118 - loss: 0.3566 \n",
      "Epoch 119: val_loss did not improve from 0.37400\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8092 - loss: 0.3591 - val_accuracy: 0.8072 - val_loss: 0.3815 - learning_rate: 0.0050\n",
      "Epoch 120/300\n",
      "\u001b[1m13/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8049 - loss: 0.3666\n",
      "Epoch 120: val_loss improved from 0.37400 to 0.37251, saving model to folds8.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8048 - loss: 0.3654 - val_accuracy: 0.8109 - val_loss: 0.3725 - learning_rate: 0.0050\n",
      "Epoch 121/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8165 - loss: 0.3601 \n",
      "Epoch 121: val_loss improved from 0.37251 to 0.37047, saving model to folds8.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8149 - loss: 0.3590 - val_accuracy: 0.7993 - val_loss: 0.3705 - learning_rate: 0.0050\n",
      "Epoch 122/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8071 - loss: 0.3660\n",
      "Epoch 122: val_loss did not improve from 0.37047\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8072 - loss: 0.3655 - val_accuracy: 0.7907 - val_loss: 0.3730 - learning_rate: 0.0050\n",
      "Epoch 123/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8139 - loss: 0.3543\n",
      "Epoch 123: val_loss did not improve from 0.37047\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8136 - loss: 0.3546 - val_accuracy: 0.8084 - val_loss: 0.3706 - learning_rate: 0.0050\n",
      "Epoch 124/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8134 - loss: 0.3538\n",
      "Epoch 124: val_loss improved from 0.37047 to 0.37027, saving model to folds8.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8132 - loss: 0.3539 - val_accuracy: 0.8084 - val_loss: 0.3703 - learning_rate: 0.0050\n",
      "Epoch 125/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8129 - loss: 0.3543\n",
      "Epoch 125: val_loss improved from 0.37027 to 0.37025, saving model to folds8.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8125 - loss: 0.3546 - val_accuracy: 0.8133 - val_loss: 0.3702 - learning_rate: 0.0050\n",
      "Epoch 126/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8108 - loss: 0.3610 \n",
      "Epoch 126: val_loss did not improve from 0.37025\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8105 - loss: 0.3601 - val_accuracy: 0.7858 - val_loss: 0.3774 - learning_rate: 0.0050\n",
      "Epoch 127/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7980 - loss: 0.3692 \n",
      "Epoch 127: val_loss improved from 0.37025 to 0.36826, saving model to folds8.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8034 - loss: 0.3632 - val_accuracy: 0.7987 - val_loss: 0.3683 - learning_rate: 0.0050\n",
      "Epoch 128/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8159 - loss: 0.3528\n",
      "Epoch 128: val_loss improved from 0.36826 to 0.36665, saving model to folds8.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8159 - loss: 0.3529 - val_accuracy: 0.8029 - val_loss: 0.3667 - learning_rate: 0.0050\n",
      "Epoch 129/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8175 - loss: 0.3482 \n",
      "Epoch 129: val_loss did not improve from 0.36665\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8155 - loss: 0.3514 - val_accuracy: 0.7974 - val_loss: 0.3692 - learning_rate: 0.0050\n",
      "Epoch 130/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8119 - loss: 0.3516\n",
      "Epoch 130: val_loss did not improve from 0.36665\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8120 - loss: 0.3517 - val_accuracy: 0.7962 - val_loss: 0.3685 - learning_rate: 0.0050\n",
      "Epoch 131/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8147 - loss: 0.3443 \n",
      "Epoch 131: val_loss did not improve from 0.36665\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8135 - loss: 0.3476 - val_accuracy: 0.8029 - val_loss: 0.3682 - learning_rate: 0.0050\n",
      "Epoch 132/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8157 - loss: 0.3525\n",
      "Epoch 132: val_loss did not improve from 0.36665\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8155 - loss: 0.3526 - val_accuracy: 0.8017 - val_loss: 0.3725 - learning_rate: 0.0050\n",
      "Epoch 133/300\n",
      "\u001b[1m13/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8147 - loss: 0.3557 \n",
      "Epoch 133: val_loss did not improve from 0.36665\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8143 - loss: 0.3554 - val_accuracy: 0.7889 - val_loss: 0.3697 - learning_rate: 0.0050\n",
      "Epoch 134/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8157 - loss: 0.3459\n",
      "Epoch 134: val_loss did not improve from 0.36665\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8154 - loss: 0.3464 - val_accuracy: 0.8121 - val_loss: 0.3671 - learning_rate: 0.0050\n",
      "Epoch 135/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8151 - loss: 0.3570  \n",
      "Epoch 135: val_loss did not improve from 0.36665\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8134 - loss: 0.3556 - val_accuracy: 0.7999 - val_loss: 0.3693 - learning_rate: 0.0050\n",
      "Epoch 136/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8053 - loss: 0.3549\n",
      "Epoch 136: val_loss did not improve from 0.36665\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8055 - loss: 0.3547 - val_accuracy: 0.8041 - val_loss: 0.3677 - learning_rate: 0.0050\n",
      "Epoch 137/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8185 - loss: 0.3521\n",
      "Epoch 137: val_loss improved from 0.36665 to 0.36631, saving model to folds8.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8182 - loss: 0.3520 - val_accuracy: 0.7919 - val_loss: 0.3663 - learning_rate: 0.0050\n",
      "Epoch 138/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8121 - loss: 0.3550 \n",
      "Epoch 138: val_loss did not improve from 0.36631\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8105 - loss: 0.3542 - val_accuracy: 0.7767 - val_loss: 0.3784 - learning_rate: 0.0050\n",
      "Epoch 139/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8047 - loss: 0.3599\n",
      "Epoch 139: val_loss did not improve from 0.36631\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8050 - loss: 0.3597 - val_accuracy: 0.8103 - val_loss: 0.3672 - learning_rate: 0.0050\n",
      "Epoch 140/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8112 - loss: 0.3535\n",
      "Epoch 140: val_loss did not improve from 0.36631\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8113 - loss: 0.3534 - val_accuracy: 0.8109 - val_loss: 0.3667 - learning_rate: 0.0050\n",
      "Epoch 141/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8136 - loss: 0.3523\n",
      "Epoch 141: val_loss improved from 0.36631 to 0.36538, saving model to folds8.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.8136 - loss: 0.3521 - val_accuracy: 0.8011 - val_loss: 0.3654 - learning_rate: 0.0050\n",
      "Epoch 142/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8092 - loss: 0.3566\n",
      "Epoch 142: val_loss improved from 0.36538 to 0.36256, saving model to folds8.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8096 - loss: 0.3557 - val_accuracy: 0.8017 - val_loss: 0.3626 - learning_rate: 0.0050\n",
      "Epoch 143/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8133 - loss: 0.3459 \n",
      "Epoch 143: val_loss improved from 0.36256 to 0.36135, saving model to folds8.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8131 - loss: 0.3475 - val_accuracy: 0.8078 - val_loss: 0.3614 - learning_rate: 0.0050\n",
      "Epoch 144/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8185 - loss: 0.3467\n",
      "Epoch 144: val_loss did not improve from 0.36135\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8181 - loss: 0.3469 - val_accuracy: 0.8023 - val_loss: 0.3635 - learning_rate: 0.0050\n",
      "Epoch 145/300\n",
      "\u001b[1m13/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8171 - loss: 0.3446\n",
      "Epoch 145: val_loss did not improve from 0.36135\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8169 - loss: 0.3456 - val_accuracy: 0.8035 - val_loss: 0.3635 - learning_rate: 0.0050\n",
      "Epoch 146/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8127 - loss: 0.3522 \n",
      "Epoch 146: val_loss did not improve from 0.36135\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8125 - loss: 0.3513 - val_accuracy: 0.8066 - val_loss: 0.3639 - learning_rate: 0.0050\n",
      "Epoch 147/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8159 - loss: 0.3518\n",
      "Epoch 147: val_loss did not improve from 0.36135\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8160 - loss: 0.3516 - val_accuracy: 0.8109 - val_loss: 0.3615 - learning_rate: 0.0050\n",
      "Epoch 148/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8139 - loss: 0.3483  \n",
      "Epoch 148: val_loss did not improve from 0.36135\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8120 - loss: 0.3497 - val_accuracy: 0.7980 - val_loss: 0.3632 - learning_rate: 0.0050\n",
      "Epoch 149/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8162 - loss: 0.3497\n",
      "Epoch 149: val_loss did not improve from 0.36135\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.8161 - loss: 0.3497 - val_accuracy: 0.7919 - val_loss: 0.3626 - learning_rate: 0.0050\n",
      "Epoch 150/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8119 - loss: 0.3453\n",
      "Epoch 150: val_loss did not improve from 0.36135\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.8120 - loss: 0.3458 - val_accuracy: 0.7926 - val_loss: 0.3683 - learning_rate: 0.0050\n",
      "Epoch 151/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8104 - loss: 0.3460\n",
      "Epoch 151: val_loss did not improve from 0.36135\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.8104 - loss: 0.3462 - val_accuracy: 0.8017 - val_loss: 0.3642 - learning_rate: 0.0050\n",
      "Epoch 152/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8147 - loss: 0.3465\n",
      "Epoch 152: val_loss improved from 0.36135 to 0.36076, saving model to folds8.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.8152 - loss: 0.3469 - val_accuracy: 0.8078 - val_loss: 0.3608 - learning_rate: 0.0050\n",
      "Epoch 153/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8151 - loss: 0.3480\n",
      "Epoch 153: val_loss did not improve from 0.36076\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.8141 - loss: 0.3481 - val_accuracy: 0.8078 - val_loss: 0.3642 - learning_rate: 0.0050\n",
      "Epoch 154/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8209 - loss: 0.3459\n",
      "Epoch 154: val_loss did not improve from 0.36076\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.8196 - loss: 0.3464 - val_accuracy: 0.7950 - val_loss: 0.3612 - learning_rate: 0.0050\n",
      "Epoch 155/300\n",
      "\u001b[1m13/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8162 - loss: 0.3470\n",
      "Epoch 155: val_loss improved from 0.36076 to 0.35962, saving model to folds8.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.8163 - loss: 0.3471 - val_accuracy: 0.8048 - val_loss: 0.3596 - learning_rate: 0.0050\n",
      "Epoch 156/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8148 - loss: 0.3420\n",
      "Epoch 156: val_loss did not improve from 0.35962\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.8147 - loss: 0.3423 - val_accuracy: 0.8029 - val_loss: 0.3597 - learning_rate: 0.0050\n",
      "Epoch 157/300\n",
      "\u001b[1m13/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8172 - loss: 0.3417 \n",
      "Epoch 157: val_loss did not improve from 0.35962\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8164 - loss: 0.3429 - val_accuracy: 0.7852 - val_loss: 0.3599 - learning_rate: 0.0050\n",
      "Epoch 158/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8105 - loss: 0.3503\n",
      "Epoch 158: val_loss improved from 0.35962 to 0.35824, saving model to folds8.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8108 - loss: 0.3500 - val_accuracy: 0.8121 - val_loss: 0.3582 - learning_rate: 0.0050\n",
      "Epoch 159/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8114 - loss: 0.3515\n",
      "Epoch 159: val_loss did not improve from 0.35824\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8116 - loss: 0.3512 - val_accuracy: 0.8078 - val_loss: 0.3610 - learning_rate: 0.0050\n",
      "Epoch 160/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8144 - loss: 0.3466\n",
      "Epoch 160: val_loss improved from 0.35824 to 0.35803, saving model to folds8.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8145 - loss: 0.3467 - val_accuracy: 0.8054 - val_loss: 0.3580 - learning_rate: 0.0050\n",
      "Epoch 161/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8175 - loss: 0.3463\n",
      "Epoch 161: val_loss did not improve from 0.35803\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8176 - loss: 0.3462 - val_accuracy: 0.8072 - val_loss: 0.3637 - learning_rate: 0.0050\n",
      "Epoch 162/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8122 - loss: 0.3473\n",
      "Epoch 162: val_loss did not improve from 0.35803\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8125 - loss: 0.3472 - val_accuracy: 0.7987 - val_loss: 0.3614 - learning_rate: 0.0050\n",
      "Epoch 163/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8205 - loss: 0.3426  \n",
      "Epoch 163: val_loss did not improve from 0.35803\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8192 - loss: 0.3435 - val_accuracy: 0.7956 - val_loss: 0.3683 - learning_rate: 0.0050\n",
      "Epoch 164/300\n",
      "\u001b[1m13/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8120 - loss: 0.3518 \n",
      "Epoch 164: val_loss did not improve from 0.35803\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8125 - loss: 0.3507 - val_accuracy: 0.7974 - val_loss: 0.3596 - learning_rate: 0.0050\n",
      "Epoch 165/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8120 - loss: 0.3497\n",
      "Epoch 165: val_loss did not improve from 0.35803\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8122 - loss: 0.3495 - val_accuracy: 0.8139 - val_loss: 0.3587 - learning_rate: 0.0050\n",
      "Epoch 166/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8191 - loss: 0.3437\n",
      "Epoch 166: val_loss improved from 0.35803 to 0.35744, saving model to folds8.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8189 - loss: 0.3438 - val_accuracy: 0.8127 - val_loss: 0.3574 - learning_rate: 0.0050\n",
      "Epoch 167/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8231 - loss: 0.3483\n",
      "Epoch 167: val_loss did not improve from 0.35744\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8228 - loss: 0.3483 - val_accuracy: 0.8066 - val_loss: 0.3605 - learning_rate: 0.0050\n",
      "Epoch 168/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8145 - loss: 0.3467\n",
      "Epoch 168: val_loss did not improve from 0.35744\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8145 - loss: 0.3467 - val_accuracy: 0.7974 - val_loss: 0.3617 - learning_rate: 0.0050\n",
      "Epoch 169/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8184 - loss: 0.3432\n",
      "Epoch 169: val_loss improved from 0.35744 to 0.35540, saving model to folds8.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.8182 - loss: 0.3433 - val_accuracy: 0.8096 - val_loss: 0.3554 - learning_rate: 0.0050\n",
      "Epoch 170/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8180 - loss: 0.3412\n",
      "Epoch 170: val_loss did not improve from 0.35540\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8180 - loss: 0.3412 - val_accuracy: 0.8103 - val_loss: 0.3594 - learning_rate: 0.0050\n",
      "Epoch 171/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8164 - loss: 0.3470 \n",
      "Epoch 171: val_loss did not improve from 0.35540\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8167 - loss: 0.3453 - val_accuracy: 0.7938 - val_loss: 0.3571 - learning_rate: 0.0050\n",
      "Epoch 172/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8136 - loss: 0.3478\n",
      "Epoch 172: val_loss improved from 0.35540 to 0.35400, saving model to folds8.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8137 - loss: 0.3476 - val_accuracy: 0.8096 - val_loss: 0.3540 - learning_rate: 0.0050\n",
      "Epoch 173/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8234 - loss: 0.3401 \n",
      "Epoch 173: val_loss did not improve from 0.35400\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8217 - loss: 0.3405 - val_accuracy: 0.8023 - val_loss: 0.3553 - learning_rate: 0.0050\n",
      "Epoch 174/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8123 - loss: 0.3443 \n",
      "Epoch 174: val_loss did not improve from 0.35400\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8141 - loss: 0.3430 - val_accuracy: 0.7907 - val_loss: 0.3630 - learning_rate: 0.0050\n",
      "Epoch 175/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8143 - loss: 0.3444\n",
      "Epoch 175: val_loss did not improve from 0.35400\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8144 - loss: 0.3443 - val_accuracy: 0.8035 - val_loss: 0.3553 - learning_rate: 0.0050\n",
      "Epoch 176/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8157 - loss: 0.3416\n",
      "Epoch 176: val_loss did not improve from 0.35400\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8158 - loss: 0.3416 - val_accuracy: 0.8066 - val_loss: 0.3579 - learning_rate: 0.0050\n",
      "Epoch 177/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8139 - loss: 0.3408\n",
      "Epoch 177: val_loss did not improve from 0.35400\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8140 - loss: 0.3409 - val_accuracy: 0.8096 - val_loss: 0.3582 - learning_rate: 0.0050\n",
      "Epoch 178/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8190 - loss: 0.3435 \n",
      "Epoch 178: val_loss improved from 0.35400 to 0.35342, saving model to folds8.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8186 - loss: 0.3435 - val_accuracy: 0.8066 - val_loss: 0.3534 - learning_rate: 0.0050\n",
      "Epoch 179/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8194 - loss: 0.3397\n",
      "Epoch 179: val_loss did not improve from 0.35342\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8193 - loss: 0.3398 - val_accuracy: 0.8048 - val_loss: 0.3547 - learning_rate: 0.0050\n",
      "Epoch 180/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8208 - loss: 0.3372\n",
      "Epoch 180: val_loss did not improve from 0.35342\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8207 - loss: 0.3374 - val_accuracy: 0.7932 - val_loss: 0.3595 - learning_rate: 0.0050\n",
      "Epoch 181/300\n",
      "\u001b[1m13/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8158 - loss: 0.3381 \n",
      "Epoch 181: val_loss did not improve from 0.35342\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8160 - loss: 0.3388 - val_accuracy: 0.7944 - val_loss: 0.3564 - learning_rate: 0.0050\n",
      "Epoch 182/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8154 - loss: 0.3424\n",
      "Epoch 182: val_loss did not improve from 0.35342\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8156 - loss: 0.3423 - val_accuracy: 0.8029 - val_loss: 0.3542 - learning_rate: 0.0050\n",
      "Epoch 183/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8058 - loss: 0.3439 \n",
      "Epoch 183: val_loss improved from 0.35342 to 0.35037, saving model to folds8.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8100 - loss: 0.3416 - val_accuracy: 0.8164 - val_loss: 0.3504 - learning_rate: 0.0050\n",
      "Epoch 184/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8226 - loss: 0.3399\n",
      "Epoch 184: val_loss did not improve from 0.35037\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8223 - loss: 0.3400 - val_accuracy: 0.8035 - val_loss: 0.3573 - learning_rate: 0.0050\n",
      "Epoch 185/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8124 - loss: 0.3457  \n",
      "Epoch 185: val_loss did not improve from 0.35037\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8134 - loss: 0.3443 - val_accuracy: 0.8109 - val_loss: 0.3527 - learning_rate: 0.0050\n",
      "Epoch 186/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8191 - loss: 0.3377\n",
      "Epoch 186: val_loss did not improve from 0.35037\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8191 - loss: 0.3378 - val_accuracy: 0.7980 - val_loss: 0.3556 - learning_rate: 0.0050\n",
      "Epoch 187/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8157 - loss: 0.3413\n",
      "Epoch 187: val_loss did not improve from 0.35037\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8157 - loss: 0.3413 - val_accuracy: 0.8048 - val_loss: 0.3532 - learning_rate: 0.0050\n",
      "Epoch 188/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8157 - loss: 0.3403\n",
      "Epoch 188: val_loss did not improve from 0.35037\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8157 - loss: 0.3403 - val_accuracy: 0.8139 - val_loss: 0.3542 - learning_rate: 0.0050\n",
      "Epoch 189/300\n",
      "\u001b[1m13/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8159 - loss: 0.3465\n",
      "Epoch 189: val_loss did not improve from 0.35037\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8164 - loss: 0.3453 - val_accuracy: 0.8078 - val_loss: 0.3524 - learning_rate: 0.0050\n",
      "Epoch 190/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8166 - loss: 0.3402\n",
      "Epoch 190: val_loss did not improve from 0.35037\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8164 - loss: 0.3402 - val_accuracy: 0.8103 - val_loss: 0.3519 - learning_rate: 0.0050\n",
      "Epoch 191/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8179 - loss: 0.3436\n",
      "Epoch 191: val_loss did not improve from 0.35037\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8178 - loss: 0.3434 - val_accuracy: 0.8011 - val_loss: 0.3534 - learning_rate: 0.0050\n",
      "Epoch 192/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8083 - loss: 0.3452 \n",
      "Epoch 192: val_loss did not improve from 0.35037\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8115 - loss: 0.3433 - val_accuracy: 0.8072 - val_loss: 0.3609 - learning_rate: 0.0050\n",
      "Epoch 193/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8173 - loss: 0.3434\n",
      "Epoch 193: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "\n",
      "Epoch 193: val_loss did not improve from 0.35037\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8174 - loss: 0.3430 - val_accuracy: 0.8023 - val_loss: 0.3516 - learning_rate: 0.0050\n",
      "Epoch 194/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8190 - loss: 0.3350 \n",
      "Epoch 194: val_loss improved from 0.35037 to 0.34830, saving model to folds8.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8182 - loss: 0.3356 - val_accuracy: 0.8072 - val_loss: 0.3483 - learning_rate: 0.0025\n",
      "Epoch 195/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8223 - loss: 0.3329 \n",
      "Epoch 195: val_loss improved from 0.34830 to 0.34830, saving model to folds8.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8200 - loss: 0.3341 - val_accuracy: 0.8054 - val_loss: 0.3483 - learning_rate: 0.0025\n",
      "Epoch 196/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8181 - loss: 0.3370\n",
      "Epoch 196: val_loss did not improve from 0.34830\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.8183 - loss: 0.3368 - val_accuracy: 0.8072 - val_loss: 0.3488 - learning_rate: 0.0025\n",
      "Epoch 197/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8201 - loss: 0.3272\n",
      "Epoch 197: val_loss did not improve from 0.34830\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.8193 - loss: 0.3299 - val_accuracy: 0.8145 - val_loss: 0.3491 - learning_rate: 0.0025\n",
      "Epoch 198/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8246 - loss: 0.3310\n",
      "Epoch 198: val_loss did not improve from 0.34830\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.8227 - loss: 0.3330 - val_accuracy: 0.8096 - val_loss: 0.3507 - learning_rate: 0.0025\n",
      "Epoch 199/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8207 - loss: 0.3369\n",
      "Epoch 199: val_loss did not improve from 0.34830\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.8206 - loss: 0.3367 - val_accuracy: 0.8029 - val_loss: 0.3486 - learning_rate: 0.0025\n",
      "Epoch 200/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8147 - loss: 0.3331\n",
      "Epoch 200: val_loss did not improve from 0.34830\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.8157 - loss: 0.3335 - val_accuracy: 0.8011 - val_loss: 0.3498 - learning_rate: 0.0025\n",
      "Epoch 201/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8158 - loss: 0.3342\n",
      "Epoch 201: val_loss did not improve from 0.34830\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.8166 - loss: 0.3349 - val_accuracy: 0.7974 - val_loss: 0.3485 - learning_rate: 0.0025\n",
      "Epoch 202/300\n",
      "\u001b[1m12/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8152 - loss: 0.3373\n",
      "Epoch 202: val_loss improved from 0.34830 to 0.34790, saving model to folds8.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.8157 - loss: 0.3367 - val_accuracy: 0.8103 - val_loss: 0.3479 - learning_rate: 0.0025\n",
      "Epoch 203/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8163 - loss: 0.3357\n",
      "Epoch 203: val_loss did not improve from 0.34790\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.8172 - loss: 0.3347 - val_accuracy: 0.8072 - val_loss: 0.3491 - learning_rate: 0.0025\n",
      "Epoch 204/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8174 - loss: 0.3358\n",
      "Epoch 204: val_loss improved from 0.34790 to 0.34781, saving model to folds8.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.8175 - loss: 0.3358 - val_accuracy: 0.8096 - val_loss: 0.3478 - learning_rate: 0.0025\n",
      "Epoch 205/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8134 - loss: 0.3370\n",
      "Epoch 205: val_loss improved from 0.34781 to 0.34677, saving model to folds8.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.8148 - loss: 0.3360 - val_accuracy: 0.8072 - val_loss: 0.3468 - learning_rate: 0.0025\n",
      "Epoch 206/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8207 - loss: 0.3325\n",
      "Epoch 206: val_loss did not improve from 0.34677\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.8198 - loss: 0.3331 - val_accuracy: 0.8011 - val_loss: 0.3480 - learning_rate: 0.0025\n",
      "Epoch 207/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8140 - loss: 0.3396\n",
      "Epoch 207: val_loss did not improve from 0.34677\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.8142 - loss: 0.3393 - val_accuracy: 0.8096 - val_loss: 0.3477 - learning_rate: 0.0025\n",
      "Epoch 208/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8191 - loss: 0.3342\n",
      "Epoch 208: val_loss did not improve from 0.34677\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8191 - loss: 0.3343 - val_accuracy: 0.8048 - val_loss: 0.3483 - learning_rate: 0.0025\n",
      "Epoch 209/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8165 - loss: 0.3400  \n",
      "Epoch 209: val_loss did not improve from 0.34677\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8181 - loss: 0.3381 - val_accuracy: 0.8170 - val_loss: 0.3481 - learning_rate: 0.0025\n",
      "Epoch 210/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8269 - loss: 0.3329  \n",
      "Epoch 210: val_loss did not improve from 0.34677\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8241 - loss: 0.3336 - val_accuracy: 0.8060 - val_loss: 0.3475 - learning_rate: 0.0025\n",
      "Epoch 211/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8165 - loss: 0.3388 \n",
      "Epoch 211: val_loss improved from 0.34677 to 0.34646, saving model to folds8.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8174 - loss: 0.3361 - val_accuracy: 0.8096 - val_loss: 0.3465 - learning_rate: 0.0025\n",
      "Epoch 212/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8245 - loss: 0.3324\n",
      "Epoch 212: val_loss improved from 0.34646 to 0.34624, saving model to folds8.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8238 - loss: 0.3326 - val_accuracy: 0.8041 - val_loss: 0.3462 - learning_rate: 0.0025\n",
      "Epoch 213/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8231 - loss: 0.3300\n",
      "Epoch 213: val_loss did not improve from 0.34624\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8227 - loss: 0.3302 - val_accuracy: 0.8066 - val_loss: 0.3486 - learning_rate: 0.0025\n",
      "Epoch 214/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8178 - loss: 0.3324 \n",
      "Epoch 214: val_loss did not improve from 0.34624\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8186 - loss: 0.3338 - val_accuracy: 0.8151 - val_loss: 0.3507 - learning_rate: 0.0025\n",
      "Epoch 215/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8217 - loss: 0.3432 \n",
      "Epoch 215: val_loss did not improve from 0.34624\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8207 - loss: 0.3394 - val_accuracy: 0.8029 - val_loss: 0.3534 - learning_rate: 0.0025\n",
      "Epoch 216/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8178 - loss: 0.3356\n",
      "Epoch 216: val_loss did not improve from 0.34624\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8179 - loss: 0.3355 - val_accuracy: 0.8121 - val_loss: 0.3463 - learning_rate: 0.0025\n",
      "Epoch 217/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8200 - loss: 0.3326\n",
      "Epoch 217: val_loss did not improve from 0.34624\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8198 - loss: 0.3327 - val_accuracy: 0.8115 - val_loss: 0.3464 - learning_rate: 0.0025\n",
      "Epoch 218/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8208 - loss: 0.3238 \n",
      "Epoch 218: val_loss did not improve from 0.34624\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8213 - loss: 0.3275 - val_accuracy: 0.8048 - val_loss: 0.3480 - learning_rate: 0.0025\n",
      "Epoch 219/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8213 - loss: 0.3271 \n",
      "Epoch 219: val_loss did not improve from 0.34624\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8198 - loss: 0.3298 - val_accuracy: 0.8121 - val_loss: 0.3487 - learning_rate: 0.0025\n",
      "Epoch 220/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8291 - loss: 0.3264  \n",
      "Epoch 220: val_loss did not improve from 0.34624\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8268 - loss: 0.3281 - val_accuracy: 0.8084 - val_loss: 0.3479 - learning_rate: 0.0025\n",
      "Epoch 221/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8193 - loss: 0.3320  \n",
      "Epoch 221: val_loss did not improve from 0.34624\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8189 - loss: 0.3337 - val_accuracy: 0.8151 - val_loss: 0.3475 - learning_rate: 0.0025\n",
      "Epoch 222/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8247 - loss: 0.3321  \n",
      "Epoch 222: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
      "\n",
      "Epoch 222: val_loss did not improve from 0.34624\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8222 - loss: 0.3336 - val_accuracy: 0.8103 - val_loss: 0.3472 - learning_rate: 0.0025\n",
      "Epoch 223/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8186 - loss: 0.3300\n",
      "Epoch 223: val_loss improved from 0.34624 to 0.34540, saving model to folds8.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8186 - loss: 0.3302 - val_accuracy: 0.8066 - val_loss: 0.3454 - learning_rate: 0.0012\n",
      "Epoch 224/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8186 - loss: 0.3332\n",
      "Epoch 224: val_loss did not improve from 0.34540\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8186 - loss: 0.3331 - val_accuracy: 0.8121 - val_loss: 0.3457 - learning_rate: 0.0012\n",
      "Epoch 225/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8209 - loss: 0.3271 \n",
      "Epoch 225: val_loss improved from 0.34540 to 0.34492, saving model to folds8.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8205 - loss: 0.3288 - val_accuracy: 0.8109 - val_loss: 0.3449 - learning_rate: 0.0012\n",
      "Epoch 226/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8212 - loss: 0.3336 \n",
      "Epoch 226: val_loss did not improve from 0.34492\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8216 - loss: 0.3327 - val_accuracy: 0.8041 - val_loss: 0.3455 - learning_rate: 0.0012\n",
      "Epoch 227/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8117 - loss: 0.3382\n",
      "Epoch 227: val_loss improved from 0.34492 to 0.34481, saving model to folds8.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.8121 - loss: 0.3378 - val_accuracy: 0.8084 - val_loss: 0.3448 - learning_rate: 0.0012\n",
      "Epoch 228/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8179 - loss: 0.3365\n",
      "Epoch 228: val_loss did not improve from 0.34481\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8180 - loss: 0.3362 - val_accuracy: 0.8048 - val_loss: 0.3456 - learning_rate: 0.0012\n",
      "Epoch 229/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8166 - loss: 0.3350 \n",
      "Epoch 229: val_loss did not improve from 0.34481\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8163 - loss: 0.3334 - val_accuracy: 0.8139 - val_loss: 0.3449 - learning_rate: 0.0012\n",
      "Epoch 230/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8155 - loss: 0.3314\n",
      "Epoch 230: val_loss did not improve from 0.34481\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8157 - loss: 0.3314 - val_accuracy: 0.8090 - val_loss: 0.3459 - learning_rate: 0.0012\n",
      "Epoch 231/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8172 - loss: 0.3350\n",
      "Epoch 231: val_loss did not improve from 0.34481\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8173 - loss: 0.3348 - val_accuracy: 0.8157 - val_loss: 0.3453 - learning_rate: 0.0012\n",
      "Epoch 232/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8186 - loss: 0.3342\n",
      "Epoch 232: val_loss improved from 0.34481 to 0.34446, saving model to folds8.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8186 - loss: 0.3340 - val_accuracy: 0.8096 - val_loss: 0.3445 - learning_rate: 0.0012\n",
      "Epoch 233/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8217 - loss: 0.3251 \n",
      "Epoch 233: val_loss did not improve from 0.34446\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8208 - loss: 0.3285 - val_accuracy: 0.8176 - val_loss: 0.3452 - learning_rate: 0.0012\n",
      "Epoch 234/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8232 - loss: 0.3315\n",
      "Epoch 234: val_loss did not improve from 0.34446\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8232 - loss: 0.3315 - val_accuracy: 0.8048 - val_loss: 0.3447 - learning_rate: 0.0012\n",
      "Epoch 235/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8235 - loss: 0.3248\n",
      "Epoch 235: val_loss did not improve from 0.34446\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8232 - loss: 0.3252 - val_accuracy: 0.8157 - val_loss: 0.3446 - learning_rate: 0.0012\n",
      "Epoch 236/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8245 - loss: 0.3225 \n",
      "Epoch 236: val_loss improved from 0.34446 to 0.34423, saving model to folds8.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8221 - loss: 0.3266 - val_accuracy: 0.8035 - val_loss: 0.3442 - learning_rate: 0.0012\n",
      "Epoch 237/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8216 - loss: 0.3332\n",
      "Epoch 237: val_loss did not improve from 0.34423\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8215 - loss: 0.3331 - val_accuracy: 0.8084 - val_loss: 0.3456 - learning_rate: 0.0012\n",
      "Epoch 238/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8155 - loss: 0.3353\n",
      "Epoch 238: val_loss did not improve from 0.34423\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8161 - loss: 0.3348 - val_accuracy: 0.8103 - val_loss: 0.3444 - learning_rate: 0.0012\n",
      "Epoch 239/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8171 - loss: 0.3320  \n",
      "Epoch 239: val_loss did not improve from 0.34423\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8180 - loss: 0.3311 - val_accuracy: 0.8182 - val_loss: 0.3448 - learning_rate: 0.0012\n",
      "Epoch 240/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8231 - loss: 0.3279\n",
      "Epoch 240: val_loss did not improve from 0.34423\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8230 - loss: 0.3281 - val_accuracy: 0.8066 - val_loss: 0.3449 - learning_rate: 0.0012\n",
      "Epoch 241/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8201 - loss: 0.3275 \n",
      "Epoch 241: val_loss did not improve from 0.34423\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8201 - loss: 0.3279 - val_accuracy: 0.8096 - val_loss: 0.3449 - learning_rate: 0.0012\n",
      "Epoch 242/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8293 - loss: 0.3245 \n",
      "Epoch 242: val_loss improved from 0.34423 to 0.34414, saving model to folds8.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8248 - loss: 0.3274 - val_accuracy: 0.8084 - val_loss: 0.3441 - learning_rate: 0.0012\n",
      "Epoch 243/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8301 - loss: 0.3269 \n",
      "Epoch 243: val_loss improved from 0.34414 to 0.34393, saving model to folds8.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8255 - loss: 0.3279 - val_accuracy: 0.8115 - val_loss: 0.3439 - learning_rate: 0.0012\n",
      "Epoch 244/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8207 - loss: 0.3285\n",
      "Epoch 244: val_loss did not improve from 0.34393\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8207 - loss: 0.3287 - val_accuracy: 0.8127 - val_loss: 0.3442 - learning_rate: 0.0012\n",
      "Epoch 245/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8237 - loss: 0.3319\n",
      "Epoch 245: val_loss did not improve from 0.34393\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8232 - loss: 0.3318 - val_accuracy: 0.8109 - val_loss: 0.3446 - learning_rate: 0.0012\n",
      "Epoch 246/300\n",
      "\u001b[1m13/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8128 - loss: 0.3323\n",
      "Epoch 246: val_loss did not improve from 0.34393\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.8135 - loss: 0.3322 - val_accuracy: 0.8078 - val_loss: 0.3462 - learning_rate: 0.0012\n",
      "Epoch 247/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8157 - loss: 0.3344\n",
      "Epoch 247: val_loss did not improve from 0.34393\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.8158 - loss: 0.3342 - val_accuracy: 0.8072 - val_loss: 0.3465 - learning_rate: 0.0012\n",
      "Epoch 248/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8143 - loss: 0.3339\n",
      "Epoch 248: val_loss did not improve from 0.34393\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.8147 - loss: 0.3336 - val_accuracy: 0.8072 - val_loss: 0.3449 - learning_rate: 0.0012\n",
      "Epoch 249/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8198 - loss: 0.3272\n",
      "Epoch 249: val_loss improved from 0.34393 to 0.34376, saving model to folds8.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.8203 - loss: 0.3281 - val_accuracy: 0.8072 - val_loss: 0.3438 - learning_rate: 0.0012\n",
      "Epoch 250/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8159 - loss: 0.3322\n",
      "Epoch 250: val_loss did not improve from 0.34376\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.8162 - loss: 0.3321 - val_accuracy: 0.8090 - val_loss: 0.3443 - learning_rate: 0.0012\n",
      "Epoch 251/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8201 - loss: 0.3342\n",
      "Epoch 251: val_loss did not improve from 0.34376\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.8200 - loss: 0.3335 - val_accuracy: 0.8139 - val_loss: 0.3447 - learning_rate: 0.0012\n",
      "Epoch 252/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8200 - loss: 0.3298\n",
      "Epoch 252: val_loss did not improve from 0.34376\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.8201 - loss: 0.3299 - val_accuracy: 0.8127 - val_loss: 0.3447 - learning_rate: 0.0012\n",
      "Epoch 253/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8185 - loss: 0.3310\n",
      "Epoch 253: val_loss did not improve from 0.34376\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.8189 - loss: 0.3308 - val_accuracy: 0.8103 - val_loss: 0.3441 - learning_rate: 0.0012\n",
      "Epoch 254/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8200 - loss: 0.3267\n",
      "Epoch 254: val_loss did not improve from 0.34376\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.8196 - loss: 0.3280 - val_accuracy: 0.8096 - val_loss: 0.3438 - learning_rate: 0.0012\n",
      "Epoch 255/300\n",
      "\u001b[1m13/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8212 - loss: 0.3295\n",
      "Epoch 255: val_loss did not improve from 0.34376\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.8209 - loss: 0.3296 - val_accuracy: 0.8023 - val_loss: 0.3441 - learning_rate: 0.0012\n",
      "Epoch 256/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8229 - loss: 0.3276 \n",
      "Epoch 256: val_loss did not improve from 0.34376\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8222 - loss: 0.3278 - val_accuracy: 0.8151 - val_loss: 0.3442 - learning_rate: 0.0012\n",
      "Epoch 257/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8213 - loss: 0.3293\n",
      "Epoch 257: val_loss did not improve from 0.34376\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8212 - loss: 0.3294 - val_accuracy: 0.8115 - val_loss: 0.3443 - learning_rate: 0.0012\n",
      "Epoch 258/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8190 - loss: 0.3350 \n",
      "Epoch 258: val_loss did not improve from 0.34376\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8199 - loss: 0.3331 - val_accuracy: 0.8060 - val_loss: 0.3442 - learning_rate: 0.0012\n",
      "Epoch 259/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8219 - loss: 0.3245\n",
      "Epoch 259: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "\n",
      "Epoch 259: val_loss did not improve from 0.34376\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8219 - loss: 0.3249 - val_accuracy: 0.8048 - val_loss: 0.3440 - learning_rate: 0.0012\n",
      "Epoch 260/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8155 - loss: 0.3290\n",
      "Epoch 260: val_loss improved from 0.34376 to 0.34368, saving model to folds8.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.8156 - loss: 0.3291 - val_accuracy: 0.8084 - val_loss: 0.3437 - learning_rate: 6.2500e-04\n",
      "Epoch 261/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8230 - loss: 0.3259 \n",
      "Epoch 261: val_loss improved from 0.34368 to 0.34314, saving model to folds8.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8225 - loss: 0.3278 - val_accuracy: 0.8115 - val_loss: 0.3431 - learning_rate: 6.2500e-04\n",
      "Epoch 262/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8211 - loss: 0.3295\n",
      "Epoch 262: val_loss did not improve from 0.34314\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8209 - loss: 0.3295 - val_accuracy: 0.8109 - val_loss: 0.3433 - learning_rate: 6.2500e-04\n",
      "Epoch 263/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8161 - loss: 0.3334\n",
      "Epoch 263: val_loss improved from 0.34314 to 0.34311, saving model to folds8.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.8162 - loss: 0.3331 - val_accuracy: 0.8164 - val_loss: 0.3431 - learning_rate: 6.2500e-04\n",
      "Epoch 264/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8247 - loss: 0.3263\n",
      "Epoch 264: val_loss improved from 0.34311 to 0.34281, saving model to folds8.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.8245 - loss: 0.3265 - val_accuracy: 0.8078 - val_loss: 0.3428 - learning_rate: 6.2500e-04\n",
      "Epoch 265/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8173 - loss: 0.3292\n",
      "Epoch 265: val_loss did not improve from 0.34281\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8174 - loss: 0.3292 - val_accuracy: 0.8072 - val_loss: 0.3435 - learning_rate: 6.2500e-04\n",
      "Epoch 266/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8164 - loss: 0.3350\n",
      "Epoch 266: val_loss did not improve from 0.34281\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8166 - loss: 0.3347 - val_accuracy: 0.8151 - val_loss: 0.3431 - learning_rate: 6.2500e-04\n",
      "Epoch 267/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8211 - loss: 0.3294\n",
      "Epoch 267: val_loss did not improve from 0.34281\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8210 - loss: 0.3294 - val_accuracy: 0.8096 - val_loss: 0.3430 - learning_rate: 6.2500e-04\n",
      "Epoch 268/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8219 - loss: 0.3245 \n",
      "Epoch 268: val_loss did not improve from 0.34281\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8205 - loss: 0.3270 - val_accuracy: 0.8157 - val_loss: 0.3428 - learning_rate: 6.2500e-04\n",
      "Epoch 269/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8176 - loss: 0.3274\n",
      "Epoch 269: val_loss did not improve from 0.34281\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8177 - loss: 0.3275 - val_accuracy: 0.8176 - val_loss: 0.3429 - learning_rate: 6.2500e-04\n",
      "Epoch 270/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8223 - loss: 0.3289\n",
      "Epoch 270: val_loss improved from 0.34281 to 0.34176, saving model to folds8.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.8222 - loss: 0.3289 - val_accuracy: 0.8127 - val_loss: 0.3418 - learning_rate: 6.2500e-04\n",
      "Epoch 271/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8141 - loss: 0.3322 \n",
      "Epoch 271: val_loss did not improve from 0.34176\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8159 - loss: 0.3318 - val_accuracy: 0.8145 - val_loss: 0.3420 - learning_rate: 6.2500e-04\n",
      "Epoch 272/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8184 - loss: 0.3258\n",
      "Epoch 272: val_loss improved from 0.34176 to 0.34084, saving model to folds8.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8184 - loss: 0.3259 - val_accuracy: 0.8164 - val_loss: 0.3408 - learning_rate: 6.2500e-04\n",
      "Epoch 273/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8227 - loss: 0.3243 \n",
      "Epoch 273: val_loss did not improve from 0.34084\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8207 - loss: 0.3259 - val_accuracy: 0.8170 - val_loss: 0.3410 - learning_rate: 6.2500e-04\n",
      "Epoch 274/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8266 - loss: 0.3262  \n",
      "Epoch 274: val_loss improved from 0.34084 to 0.34043, saving model to folds8.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8255 - loss: 0.3272 - val_accuracy: 0.8115 - val_loss: 0.3404 - learning_rate: 6.2500e-04\n",
      "Epoch 275/300\n",
      "\u001b[1m13/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8171 - loss: 0.3334\n",
      "Epoch 275: val_loss improved from 0.34043 to 0.34041, saving model to folds8.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8175 - loss: 0.3323 - val_accuracy: 0.8139 - val_loss: 0.3404 - learning_rate: 6.2500e-04\n",
      "Epoch 276/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8204 - loss: 0.3278\n",
      "Epoch 276: val_loss improved from 0.34041 to 0.34008, saving model to folds8.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8205 - loss: 0.3278 - val_accuracy: 0.8157 - val_loss: 0.3401 - learning_rate: 6.2500e-04\n",
      "Epoch 277/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8256 - loss: 0.3181 \n",
      "Epoch 277: val_loss improved from 0.34008 to 0.33977, saving model to folds8.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8238 - loss: 0.3220 - val_accuracy: 0.8115 - val_loss: 0.3398 - learning_rate: 6.2500e-04\n",
      "Epoch 278/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8187 - loss: 0.3284\n",
      "Epoch 278: val_loss improved from 0.33977 to 0.33973, saving model to folds8.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8188 - loss: 0.3283 - val_accuracy: 0.8121 - val_loss: 0.3397 - learning_rate: 6.2500e-04\n",
      "Epoch 279/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8210 - loss: 0.3263\n",
      "Epoch 279: val_loss improved from 0.33973 to 0.33947, saving model to folds8.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.8208 - loss: 0.3263 - val_accuracy: 0.8109 - val_loss: 0.3395 - learning_rate: 6.2500e-04\n",
      "Epoch 280/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8192 - loss: 0.3261 \n",
      "Epoch 280: val_loss did not improve from 0.33947\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8192 - loss: 0.3267 - val_accuracy: 0.8121 - val_loss: 0.3396 - learning_rate: 6.2500e-04\n",
      "Epoch 281/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8214 - loss: 0.3278 \n",
      "Epoch 281: val_loss improved from 0.33947 to 0.33946, saving model to folds8.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8217 - loss: 0.3269 - val_accuracy: 0.8127 - val_loss: 0.3395 - learning_rate: 6.2500e-04\n",
      "Epoch 282/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8202 - loss: 0.3315 \n",
      "Epoch 282: val_loss improved from 0.33946 to 0.33912, saving model to folds8.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8212 - loss: 0.3284 - val_accuracy: 0.8127 - val_loss: 0.3391 - learning_rate: 6.2500e-04\n",
      "Epoch 283/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8199 - loss: 0.3232\n",
      "Epoch 283: val_loss did not improve from 0.33912\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8199 - loss: 0.3234 - val_accuracy: 0.8127 - val_loss: 0.3392 - learning_rate: 6.2500e-04\n",
      "Epoch 284/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8220 - loss: 0.3240 \n",
      "Epoch 284: val_loss improved from 0.33912 to 0.33883, saving model to folds8.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8221 - loss: 0.3254 - val_accuracy: 0.8133 - val_loss: 0.3388 - learning_rate: 6.2500e-04\n",
      "Epoch 285/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8225 - loss: 0.3240\n",
      "Epoch 285: val_loss improved from 0.33883 to 0.33872, saving model to folds8.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8223 - loss: 0.3242 - val_accuracy: 0.8133 - val_loss: 0.3387 - learning_rate: 6.2500e-04\n",
      "Epoch 286/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8195 - loss: 0.3285 \n",
      "Epoch 286: val_loss improved from 0.33872 to 0.33848, saving model to folds8.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8204 - loss: 0.3276 - val_accuracy: 0.8145 - val_loss: 0.3385 - learning_rate: 6.2500e-04\n",
      "Epoch 287/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8234 - loss: 0.3236\n",
      "Epoch 287: val_loss did not improve from 0.33848\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8234 - loss: 0.3237 - val_accuracy: 0.8164 - val_loss: 0.3388 - learning_rate: 6.2500e-04\n",
      "Epoch 288/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8200 - loss: 0.3282  \n",
      "Epoch 288: val_loss did not improve from 0.33848\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8209 - loss: 0.3274 - val_accuracy: 0.8127 - val_loss: 0.3387 - learning_rate: 6.2500e-04\n",
      "Epoch 289/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8241 - loss: 0.3282\n",
      "Epoch 289: val_loss improved from 0.33848 to 0.33841, saving model to folds8.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.8241 - loss: 0.3281 - val_accuracy: 0.8139 - val_loss: 0.3384 - learning_rate: 6.2500e-04\n",
      "Epoch 290/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8251 - loss: 0.3258 \n",
      "Epoch 290: val_loss did not improve from 0.33841\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8240 - loss: 0.3260 - val_accuracy: 0.8188 - val_loss: 0.3386 - learning_rate: 6.2500e-04\n",
      "Epoch 291/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8275 - loss: 0.3232\n",
      "Epoch 291: val_loss did not improve from 0.33841\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8274 - loss: 0.3234 - val_accuracy: 0.8157 - val_loss: 0.3390 - learning_rate: 6.2500e-04\n",
      "Epoch 292/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8212 - loss: 0.3273\n",
      "Epoch 292: val_loss did not improve from 0.33841\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8212 - loss: 0.3272 - val_accuracy: 0.8182 - val_loss: 0.3385 - learning_rate: 6.2500e-04\n",
      "Epoch 293/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8314 - loss: 0.3216\n",
      "Epoch 293: val_loss did not improve from 0.33841\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8306 - loss: 0.3221 - val_accuracy: 0.8157 - val_loss: 0.3385 - learning_rate: 6.2500e-04\n",
      "Epoch 294/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8211 - loss: 0.3243 \n",
      "Epoch 294: val_loss improved from 0.33841 to 0.33763, saving model to folds8.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.8213 - loss: 0.3243 - val_accuracy: 0.8176 - val_loss: 0.3376 - learning_rate: 6.2500e-04\n",
      "Epoch 295/300\n",
      "\u001b[1m13/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8265 - loss: 0.3283\n",
      "Epoch 295: val_loss did not improve from 0.33763\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.8264 - loss: 0.3277 - val_accuracy: 0.8200 - val_loss: 0.3387 - learning_rate: 6.2500e-04\n",
      "Epoch 296/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8218 - loss: 0.3234\n",
      "Epoch 296: val_loss improved from 0.33763 to 0.33755, saving model to folds8.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.8218 - loss: 0.3235 - val_accuracy: 0.8170 - val_loss: 0.3375 - learning_rate: 6.2500e-04\n",
      "Epoch 297/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8239 - loss: 0.3260\n",
      "Epoch 297: val_loss did not improve from 0.33755\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.8241 - loss: 0.3257 - val_accuracy: 0.8176 - val_loss: 0.3378 - learning_rate: 6.2500e-04\n",
      "Epoch 298/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8268 - loss: 0.3226\n",
      "Epoch 298: val_loss improved from 0.33755 to 0.33725, saving model to folds8.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.8267 - loss: 0.3228 - val_accuracy: 0.8212 - val_loss: 0.3372 - learning_rate: 6.2500e-04\n",
      "Epoch 299/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8265 - loss: 0.3256\n",
      "Epoch 299: val_loss did not improve from 0.33725\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.8265 - loss: 0.3256 - val_accuracy: 0.8164 - val_loss: 0.3380 - learning_rate: 6.2500e-04\n",
      "Epoch 300/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8261 - loss: 0.3262\n",
      "Epoch 300: val_loss improved from 0.33725 to 0.33699, saving model to folds8.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.8260 - loss: 0.3256 - val_accuracy: 0.8176 - val_loss: 0.3370 - learning_rate: 6.2500e-04\n",
      "Restoring model weights from the end of the best epoch: 300.\n",
      " Computing LSTM feature importance...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:01<00:00,  5.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- > Fold 10 < ---------------\n",
      "Epoch 1/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6362 - loss: 0.6153\n",
      "Epoch 1: val_loss improved from inf to 0.52207, saving model to folds9.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step - accuracy: 0.6396 - loss: 0.6123 - val_accuracy: 0.7192 - val_loss: 0.5221 - learning_rate: 0.0100\n",
      "Epoch 2/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7188 - loss: 0.5184\n",
      "Epoch 2: val_loss improved from 0.52207 to 0.50193, saving model to folds9.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.7186 - loss: 0.5184 - val_accuracy: 0.7192 - val_loss: 0.5019 - learning_rate: 0.0100\n",
      "Epoch 3/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7119 - loss: 0.5112 \n",
      "Epoch 3: val_loss improved from 0.50193 to 0.48689, saving model to folds9.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7138 - loss: 0.5058 - val_accuracy: 0.7222 - val_loss: 0.4869 - learning_rate: 0.0100\n",
      "Epoch 4/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7249 - loss: 0.4898\n",
      "Epoch 4: val_loss improved from 0.48689 to 0.48158, saving model to folds9.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.7250 - loss: 0.4895 - val_accuracy: 0.7393 - val_loss: 0.4816 - learning_rate: 0.0100\n",
      "Epoch 5/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7330 - loss: 0.4826 \n",
      "Epoch 5: val_loss improved from 0.48158 to 0.47270, saving model to folds9.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7336 - loss: 0.4819 - val_accuracy: 0.7442 - val_loss: 0.4727 - learning_rate: 0.0100\n",
      "Epoch 6/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7342 - loss: 0.4806 \n",
      "Epoch 6: val_loss did not improve from 0.47270\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7342 - loss: 0.4785 - val_accuracy: 0.7259 - val_loss: 0.4815 - learning_rate: 0.0100\n",
      "Epoch 7/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7287 - loss: 0.4850\n",
      "Epoch 7: val_loss did not improve from 0.47270\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7287 - loss: 0.4845 - val_accuracy: 0.7198 - val_loss: 0.4795 - learning_rate: 0.0100\n",
      "Epoch 8/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7132 - loss: 0.4866 \n",
      "Epoch 8: val_loss improved from 0.47270 to 0.46624, saving model to folds9.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7189 - loss: 0.4821 - val_accuracy: 0.7418 - val_loss: 0.4662 - learning_rate: 0.0100\n",
      "Epoch 9/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7411 - loss: 0.4715 \n",
      "Epoch 9: val_loss improved from 0.46624 to 0.46493, saving model to folds9.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7431 - loss: 0.4701 - val_accuracy: 0.7466 - val_loss: 0.4649 - learning_rate: 0.0100\n",
      "Epoch 10/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7469 - loss: 0.4673\n",
      "Epoch 10: val_loss improved from 0.46493 to 0.46004, saving model to folds9.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7468 - loss: 0.4672 - val_accuracy: 0.7387 - val_loss: 0.4600 - learning_rate: 0.0100\n",
      "Epoch 11/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7448 - loss: 0.4589 \n",
      "Epoch 11: val_loss did not improve from 0.46004\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7420 - loss: 0.4609 - val_accuracy: 0.7131 - val_loss: 0.4734 - learning_rate: 0.0100\n",
      "Epoch 12/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7294 - loss: 0.4682\n",
      "Epoch 12: val_loss did not improve from 0.46004\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7298 - loss: 0.4681 - val_accuracy: 0.7344 - val_loss: 0.4612 - learning_rate: 0.0100\n",
      "Epoch 13/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7443 - loss: 0.4645 \n",
      "Epoch 13: val_loss did not improve from 0.46004\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7409 - loss: 0.4658 - val_accuracy: 0.7418 - val_loss: 0.4607 - learning_rate: 0.0100\n",
      "Epoch 14/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7386 - loss: 0.4631 \n",
      "Epoch 14: val_loss improved from 0.46004 to 0.45929, saving model to folds9.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7398 - loss: 0.4620 - val_accuracy: 0.7375 - val_loss: 0.4593 - learning_rate: 0.0100\n",
      "Epoch 15/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7493 - loss: 0.4551\n",
      "Epoch 15: val_loss improved from 0.45929 to 0.45667, saving model to folds9.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.7489 - loss: 0.4554 - val_accuracy: 0.7375 - val_loss: 0.4567 - learning_rate: 0.0100\n",
      "Epoch 16/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7421 - loss: 0.4532 \n",
      "Epoch 16: val_loss improved from 0.45667 to 0.45328, saving model to folds9.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7425 - loss: 0.4543 - val_accuracy: 0.7485 - val_loss: 0.4533 - learning_rate: 0.0100\n",
      "Epoch 17/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7461 - loss: 0.4546\n",
      "Epoch 17: val_loss improved from 0.45328 to 0.44742, saving model to folds9.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.7462 - loss: 0.4545 - val_accuracy: 0.7448 - val_loss: 0.4474 - learning_rate: 0.0100\n",
      "Epoch 18/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7408 - loss: 0.4497  \n",
      "Epoch 18: val_loss improved from 0.44742 to 0.44446, saving model to folds9.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.7426 - loss: 0.4492 - val_accuracy: 0.7570 - val_loss: 0.4445 - learning_rate: 0.0100\n",
      "Epoch 19/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7461 - loss: 0.4556\n",
      "Epoch 19: val_loss improved from 0.44446 to 0.44311, saving model to folds9.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.7463 - loss: 0.4553 - val_accuracy: 0.7570 - val_loss: 0.4431 - learning_rate: 0.0100\n",
      "Epoch 20/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7493 - loss: 0.4458 \n",
      "Epoch 20: val_loss did not improve from 0.44311\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7505 - loss: 0.4462 - val_accuracy: 0.7619 - val_loss: 0.4438 - learning_rate: 0.0100\n",
      "Epoch 21/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7554 - loss: 0.4420 \n",
      "Epoch 21: val_loss improved from 0.44311 to 0.44235, saving model to folds9.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7559 - loss: 0.4430 - val_accuracy: 0.7570 - val_loss: 0.4424 - learning_rate: 0.0100\n",
      "Epoch 22/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7599 - loss: 0.4424\n",
      "Epoch 22: val_loss did not improve from 0.44235\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.7598 - loss: 0.4424 - val_accuracy: 0.7350 - val_loss: 0.4534 - learning_rate: 0.0100\n",
      "Epoch 23/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7438 - loss: 0.4554\n",
      "Epoch 23: val_loss improved from 0.44235 to 0.43860, saving model to folds9.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.7443 - loss: 0.4545 - val_accuracy: 0.7631 - val_loss: 0.4386 - learning_rate: 0.0100\n",
      "Epoch 24/300\n",
      "\u001b[1m13/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7597 - loss: 0.4468\n",
      "Epoch 24: val_loss improved from 0.43860 to 0.43791, saving model to folds9.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.7606 - loss: 0.4456 - val_accuracy: 0.7637 - val_loss: 0.4379 - learning_rate: 0.0100\n",
      "Epoch 25/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7582 - loss: 0.4428\n",
      "Epoch 25: val_loss did not improve from 0.43791\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.7584 - loss: 0.4426 - val_accuracy: 0.7479 - val_loss: 0.4444 - learning_rate: 0.0100\n",
      "Epoch 26/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7625 - loss: 0.4365\n",
      "Epoch 26: val_loss improved from 0.43791 to 0.43679, saving model to folds9.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.7624 - loss: 0.4367 - val_accuracy: 0.7589 - val_loss: 0.4368 - learning_rate: 0.0100\n",
      "Epoch 27/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7601 - loss: 0.4375\n",
      "Epoch 27: val_loss improved from 0.43679 to 0.43521, saving model to folds9.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.7612 - loss: 0.4370 - val_accuracy: 0.7747 - val_loss: 0.4352 - learning_rate: 0.0100\n",
      "Epoch 28/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7666 - loss: 0.4399\n",
      "Epoch 28: val_loss improved from 0.43521 to 0.43029, saving model to folds9.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.7671 - loss: 0.4392 - val_accuracy: 0.7589 - val_loss: 0.4303 - learning_rate: 0.0100\n",
      "Epoch 29/300\n",
      "\u001b[1m12/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7669 - loss: 0.4241\n",
      "Epoch 29: val_loss did not improve from 0.43029\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.7671 - loss: 0.4257 - val_accuracy: 0.7656 - val_loss: 0.4322 - learning_rate: 0.0100\n",
      "Epoch 30/300\n",
      "\u001b[1m13/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7662 - loss: 0.4308\n",
      "Epoch 30: val_loss did not improve from 0.43029\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.7660 - loss: 0.4313 - val_accuracy: 0.7607 - val_loss: 0.4386 - learning_rate: 0.0100\n",
      "Epoch 31/300\n",
      "\u001b[1m12/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7603 - loss: 0.4367 \n",
      "Epoch 31: val_loss did not improve from 0.43029\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.7591 - loss: 0.4376 - val_accuracy: 0.7357 - val_loss: 0.4586 - learning_rate: 0.0100\n",
      "Epoch 32/300\n",
      "\u001b[1m12/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7582 - loss: 0.4413\n",
      "Epoch 32: val_loss did not improve from 0.43029\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.7582 - loss: 0.4410 - val_accuracy: 0.7546 - val_loss: 0.4407 - learning_rate: 0.0100\n",
      "Epoch 33/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7645 - loss: 0.4291\n",
      "Epoch 33: val_loss improved from 0.43029 to 0.42958, saving model to folds9.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.7643 - loss: 0.4293 - val_accuracy: 0.7650 - val_loss: 0.4296 - learning_rate: 0.0100\n",
      "Epoch 34/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7645 - loss: 0.4364 \n",
      "Epoch 34: val_loss improved from 0.42958 to 0.42762, saving model to folds9.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7651 - loss: 0.4336 - val_accuracy: 0.7686 - val_loss: 0.4276 - learning_rate: 0.0100\n",
      "Epoch 35/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7669 - loss: 0.4263\n",
      "Epoch 35: val_loss improved from 0.42762 to 0.42302, saving model to folds9.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7671 - loss: 0.4263 - val_accuracy: 0.7570 - val_loss: 0.4230 - learning_rate: 0.0100\n",
      "Epoch 36/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7721 - loss: 0.4241\n",
      "Epoch 36: val_loss improved from 0.42302 to 0.42123, saving model to folds9.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7719 - loss: 0.4241 - val_accuracy: 0.7802 - val_loss: 0.4212 - learning_rate: 0.0100\n",
      "Epoch 37/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7754 - loss: 0.4212 \n",
      "Epoch 37: val_loss did not improve from 0.42123\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7742 - loss: 0.4217 - val_accuracy: 0.7772 - val_loss: 0.4247 - learning_rate: 0.0100\n",
      "Epoch 38/300\n",
      "\u001b[1m12/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7736 - loss: 0.4246 \n",
      "Epoch 38: val_loss did not improve from 0.42123\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7731 - loss: 0.4243 - val_accuracy: 0.7650 - val_loss: 0.4240 - learning_rate: 0.0100\n",
      "Epoch 39/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7744 - loss: 0.4162\n",
      "Epoch 39: val_loss did not improve from 0.42123\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7741 - loss: 0.4166 - val_accuracy: 0.7766 - val_loss: 0.4260 - learning_rate: 0.0100\n",
      "Epoch 40/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7709 - loss: 0.4262\n",
      "Epoch 40: val_loss did not improve from 0.42123\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.7709 - loss: 0.4260 - val_accuracy: 0.7650 - val_loss: 0.4318 - learning_rate: 0.0100\n",
      "Epoch 41/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7643 - loss: 0.4328 \n",
      "Epoch 41: val_loss improved from 0.42123 to 0.42040, saving model to folds9.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7670 - loss: 0.4294 - val_accuracy: 0.7619 - val_loss: 0.4204 - learning_rate: 0.0100\n",
      "Epoch 42/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7707 - loss: 0.4228 \n",
      "Epoch 42: val_loss did not improve from 0.42040\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7706 - loss: 0.4227 - val_accuracy: 0.7570 - val_loss: 0.4306 - learning_rate: 0.0100\n",
      "Epoch 43/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7618 - loss: 0.4250\n",
      "Epoch 43: val_loss did not improve from 0.42040\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7620 - loss: 0.4249 - val_accuracy: 0.7680 - val_loss: 0.4221 - learning_rate: 0.0100\n",
      "Epoch 44/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7722 - loss: 0.4185\n",
      "Epoch 44: val_loss did not improve from 0.42040\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7721 - loss: 0.4186 - val_accuracy: 0.7656 - val_loss: 0.4209 - learning_rate: 0.0100\n",
      "Epoch 45/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7668 - loss: 0.4185\n",
      "Epoch 45: val_loss did not improve from 0.42040\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7671 - loss: 0.4184 - val_accuracy: 0.7674 - val_loss: 0.4237 - learning_rate: 0.0100\n",
      "Epoch 46/300\n",
      "\u001b[1m12/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7755 - loss: 0.4180\n",
      "Epoch 46: val_loss improved from 0.42040 to 0.42011, saving model to folds9.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.7752 - loss: 0.4187 - val_accuracy: 0.7668 - val_loss: 0.4201 - learning_rate: 0.0100\n",
      "Epoch 47/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7728 - loss: 0.4173\n",
      "Epoch 47: val_loss did not improve from 0.42011\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7725 - loss: 0.4175 - val_accuracy: 0.7857 - val_loss: 0.4234 - learning_rate: 0.0100\n",
      "Epoch 48/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7775 - loss: 0.4193  \n",
      "Epoch 48: val_loss did not improve from 0.42011\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7771 - loss: 0.4178 - val_accuracy: 0.7711 - val_loss: 0.4219 - learning_rate: 0.0100\n",
      "Epoch 49/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7721 - loss: 0.4157\n",
      "Epoch 49: val_loss improved from 0.42011 to 0.40957, saving model to folds9.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.7723 - loss: 0.4155 - val_accuracy: 0.7839 - val_loss: 0.4096 - learning_rate: 0.0100\n",
      "Epoch 50/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7801 - loss: 0.3995\n",
      "Epoch 50: val_loss did not improve from 0.40957\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7795 - loss: 0.4008 - val_accuracy: 0.7759 - val_loss: 0.4179 - learning_rate: 0.0100\n",
      "Epoch 51/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7770 - loss: 0.4166 \n",
      "Epoch 51: val_loss did not improve from 0.40957\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7754 - loss: 0.4169 - val_accuracy: 0.7796 - val_loss: 0.4173 - learning_rate: 0.0100\n",
      "Epoch 52/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7773 - loss: 0.4127 \n",
      "Epoch 52: val_loss improved from 0.40957 to 0.40571, saving model to folds9.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7760 - loss: 0.4123 - val_accuracy: 0.7772 - val_loss: 0.4057 - learning_rate: 0.0100\n",
      "Epoch 53/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7753 - loss: 0.4075\n",
      "Epoch 53: val_loss did not improve from 0.40571\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7753 - loss: 0.4074 - val_accuracy: 0.7778 - val_loss: 0.4098 - learning_rate: 0.0100\n",
      "Epoch 54/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7741 - loss: 0.4121\n",
      "Epoch 54: val_loss did not improve from 0.40571\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7741 - loss: 0.4119 - val_accuracy: 0.7717 - val_loss: 0.4155 - learning_rate: 0.0100\n",
      "Epoch 55/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7748 - loss: 0.4141 \n",
      "Epoch 55: val_loss did not improve from 0.40571\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7754 - loss: 0.4118 - val_accuracy: 0.7656 - val_loss: 0.4148 - learning_rate: 0.0100\n",
      "Epoch 56/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7749 - loss: 0.4067  \n",
      "Epoch 56: val_loss improved from 0.40571 to 0.40089, saving model to folds9.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7764 - loss: 0.4062 - val_accuracy: 0.7766 - val_loss: 0.4009 - learning_rate: 0.0100\n",
      "Epoch 57/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7740 - loss: 0.4015 \n",
      "Epoch 57: val_loss did not improve from 0.40089\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7772 - loss: 0.4006 - val_accuracy: 0.7790 - val_loss: 0.4031 - learning_rate: 0.0100\n",
      "Epoch 58/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7725 - loss: 0.4014\n",
      "Epoch 58: val_loss did not improve from 0.40089\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7728 - loss: 0.4013 - val_accuracy: 0.7735 - val_loss: 0.4141 - learning_rate: 0.0100\n",
      "Epoch 59/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7728 - loss: 0.4130  \n",
      "Epoch 59: val_loss did not improve from 0.40089\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7736 - loss: 0.4126 - val_accuracy: 0.7839 - val_loss: 0.4047 - learning_rate: 0.0100\n",
      "Epoch 60/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7864 - loss: 0.4062 \n",
      "Epoch 60: val_loss did not improve from 0.40089\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7817 - loss: 0.4071 - val_accuracy: 0.7723 - val_loss: 0.4121 - learning_rate: 0.0100\n",
      "Epoch 61/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7680 - loss: 0.4164  \n",
      "Epoch 61: val_loss did not improve from 0.40089\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7703 - loss: 0.4116 - val_accuracy: 0.7747 - val_loss: 0.4035 - learning_rate: 0.0100\n",
      "Epoch 62/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7714 - loss: 0.4023 \n",
      "Epoch 62: val_loss improved from 0.40089 to 0.39918, saving model to folds9.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7742 - loss: 0.4014 - val_accuracy: 0.7875 - val_loss: 0.3992 - learning_rate: 0.0100\n",
      "Epoch 63/300\n",
      "\u001b[1m12/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7867 - loss: 0.3958\n",
      "Epoch 63: val_loss improved from 0.39918 to 0.39660, saving model to folds9.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.7859 - loss: 0.3957 - val_accuracy: 0.7924 - val_loss: 0.3966 - learning_rate: 0.0100\n",
      "Epoch 64/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7897 - loss: 0.3961 \n",
      "Epoch 64: val_loss did not improve from 0.39660\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7861 - loss: 0.3970 - val_accuracy: 0.7827 - val_loss: 0.4052 - learning_rate: 0.0100\n",
      "Epoch 65/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7976 - loss: 0.3886  \n",
      "Epoch 65: val_loss did not improve from 0.39660\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7921 - loss: 0.3910 - val_accuracy: 0.7790 - val_loss: 0.3987 - learning_rate: 0.0100\n",
      "Epoch 66/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7812 - loss: 0.3976\n",
      "Epoch 66: val_loss did not improve from 0.39660\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7812 - loss: 0.3974 - val_accuracy: 0.7735 - val_loss: 0.4040 - learning_rate: 0.0100\n",
      "Epoch 67/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7717 - loss: 0.4011\n",
      "Epoch 67: val_loss did not improve from 0.39660\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.7721 - loss: 0.4010 - val_accuracy: 0.7582 - val_loss: 0.4082 - learning_rate: 0.0100\n",
      "Epoch 68/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7748 - loss: 0.3984\n",
      "Epoch 68: val_loss improved from 0.39660 to 0.39330, saving model to folds9.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.7750 - loss: 0.3983 - val_accuracy: 0.7857 - val_loss: 0.3933 - learning_rate: 0.0100\n",
      "Epoch 69/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7774 - loss: 0.4025\n",
      "Epoch 69: val_loss did not improve from 0.39330\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7775 - loss: 0.4022 - val_accuracy: 0.7759 - val_loss: 0.4012 - learning_rate: 0.0100\n",
      "Epoch 70/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7704 - loss: 0.4035\n",
      "Epoch 70: val_loss did not improve from 0.39330\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7706 - loss: 0.4032 - val_accuracy: 0.7839 - val_loss: 0.3956 - learning_rate: 0.0100\n",
      "Epoch 71/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7807 - loss: 0.3946 \n",
      "Epoch 71: val_loss did not improve from 0.39330\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7812 - loss: 0.3944 - val_accuracy: 0.7814 - val_loss: 0.3980 - learning_rate: 0.0100\n",
      "Epoch 72/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7797 - loss: 0.3947\n",
      "Epoch 72: val_loss did not improve from 0.39330\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7796 - loss: 0.3947 - val_accuracy: 0.7839 - val_loss: 0.3966 - learning_rate: 0.0100\n",
      "Epoch 73/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7765 - loss: 0.3950\n",
      "Epoch 73: val_loss did not improve from 0.39330\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.7770 - loss: 0.3947 - val_accuracy: 0.7741 - val_loss: 0.4014 - learning_rate: 0.0100\n",
      "Epoch 74/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7850 - loss: 0.3881\n",
      "Epoch 74: val_loss did not improve from 0.39330\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.7860 - loss: 0.3890 - val_accuracy: 0.7717 - val_loss: 0.3983 - learning_rate: 0.0100\n",
      "Epoch 75/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7682 - loss: 0.4012\n",
      "Epoch 75: val_loss did not improve from 0.39330\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.7686 - loss: 0.4011 - val_accuracy: 0.7888 - val_loss: 0.3945 - learning_rate: 0.0100\n",
      "Epoch 76/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7798 - loss: 0.3932\n",
      "Epoch 76: val_loss improved from 0.39330 to 0.39108, saving model to folds9.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.7792 - loss: 0.3933 - val_accuracy: 0.7973 - val_loss: 0.3911 - learning_rate: 0.0100\n",
      "Epoch 77/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7790 - loss: 0.3958\n",
      "Epoch 77: val_loss did not improve from 0.39108\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.7779 - loss: 0.3950 - val_accuracy: 0.7814 - val_loss: 0.3931 - learning_rate: 0.0100\n",
      "Epoch 78/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7780 - loss: 0.3876\n",
      "Epoch 78: val_loss did not improve from 0.39108\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.7782 - loss: 0.3880 - val_accuracy: 0.7650 - val_loss: 0.4013 - learning_rate: 0.0100\n",
      "Epoch 79/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7713 - loss: 0.4060\n",
      "Epoch 79: val_loss did not improve from 0.39108\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.7712 - loss: 0.4040 - val_accuracy: 0.7869 - val_loss: 0.3966 - learning_rate: 0.0100\n",
      "Epoch 80/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7776 - loss: 0.3932\n",
      "Epoch 80: val_loss did not improve from 0.39108\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.7784 - loss: 0.3923 - val_accuracy: 0.7906 - val_loss: 0.3919 - learning_rate: 0.0100\n",
      "Epoch 81/300\n",
      "\u001b[1m13/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7866 - loss: 0.3832\n",
      "Epoch 81: val_loss did not improve from 0.39108\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.7860 - loss: 0.3839 - val_accuracy: 0.7814 - val_loss: 0.3931 - learning_rate: 0.0100\n",
      "Epoch 82/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7779 - loss: 0.3905\n",
      "Epoch 82: val_loss improved from 0.39108 to 0.38720, saving model to folds9.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.7803 - loss: 0.3889 - val_accuracy: 0.7924 - val_loss: 0.3872 - learning_rate: 0.0100\n",
      "Epoch 83/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7892 - loss: 0.3801\n",
      "Epoch 83: val_loss improved from 0.38720 to 0.38698, saving model to folds9.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.7887 - loss: 0.3806 - val_accuracy: 0.7937 - val_loss: 0.3870 - learning_rate: 0.0100\n",
      "Epoch 84/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7878 - loss: 0.3810\n",
      "Epoch 84: val_loss did not improve from 0.38698\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.7862 - loss: 0.3823 - val_accuracy: 0.7906 - val_loss: 0.3871 - learning_rate: 0.0100\n",
      "Epoch 85/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7852 - loss: 0.3869  \n",
      "Epoch 85: val_loss improved from 0.38698 to 0.38604, saving model to folds9.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.7843 - loss: 0.3880 - val_accuracy: 0.7821 - val_loss: 0.3860 - learning_rate: 0.0100\n",
      "Epoch 86/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7848 - loss: 0.3801\n",
      "Epoch 86: val_loss improved from 0.38604 to 0.38595, saving model to folds9.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7848 - loss: 0.3801 - val_accuracy: 0.7967 - val_loss: 0.3860 - learning_rate: 0.0100\n",
      "Epoch 87/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7896 - loss: 0.3798\n",
      "Epoch 87: val_loss did not improve from 0.38595\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7894 - loss: 0.3799 - val_accuracy: 0.7790 - val_loss: 0.3880 - learning_rate: 0.0100\n",
      "Epoch 88/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7829 - loss: 0.3803\n",
      "Epoch 88: val_loss improved from 0.38595 to 0.38048, saving model to folds9.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.7832 - loss: 0.3802 - val_accuracy: 0.7863 - val_loss: 0.3805 - learning_rate: 0.0100\n",
      "Epoch 89/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7808 - loss: 0.3865 \n",
      "Epoch 89: val_loss did not improve from 0.38048\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7821 - loss: 0.3839 - val_accuracy: 0.7900 - val_loss: 0.3881 - learning_rate: 0.0100\n",
      "Epoch 90/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7836 - loss: 0.3821\n",
      "Epoch 90: val_loss did not improve from 0.38048\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7834 - loss: 0.3821 - val_accuracy: 0.7875 - val_loss: 0.3847 - learning_rate: 0.0100\n",
      "Epoch 91/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7844 - loss: 0.3836\n",
      "Epoch 91: val_loss did not improve from 0.38048\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.7844 - loss: 0.3833 - val_accuracy: 0.7882 - val_loss: 0.3823 - learning_rate: 0.0100\n",
      "Epoch 92/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7908 - loss: 0.3795\n",
      "Epoch 92: val_loss improved from 0.38048 to 0.37897, saving model to folds9.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.7906 - loss: 0.3794 - val_accuracy: 0.7796 - val_loss: 0.3790 - learning_rate: 0.0100\n",
      "Epoch 93/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7929 - loss: 0.3738\n",
      "Epoch 93: val_loss did not improve from 0.37897\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7926 - loss: 0.3741 - val_accuracy: 0.7802 - val_loss: 0.3790 - learning_rate: 0.0100\n",
      "Epoch 94/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7843 - loss: 0.3736 \n",
      "Epoch 94: val_loss did not improve from 0.37897\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7840 - loss: 0.3745 - val_accuracy: 0.7906 - val_loss: 0.3792 - learning_rate: 0.0100\n",
      "Epoch 95/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7922 - loss: 0.3718\n",
      "Epoch 95: val_loss improved from 0.37897 to 0.37817, saving model to folds9.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.7917 - loss: 0.3722 - val_accuracy: 0.7875 - val_loss: 0.3782 - learning_rate: 0.0100\n",
      "Epoch 96/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7884 - loss: 0.3791 \n",
      "Epoch 96: val_loss did not improve from 0.37817\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7875 - loss: 0.3798 - val_accuracy: 0.7979 - val_loss: 0.3797 - learning_rate: 0.0100\n",
      "Epoch 97/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7925 - loss: 0.3778\n",
      "Epoch 97: val_loss did not improve from 0.37817\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7922 - loss: 0.3777 - val_accuracy: 0.7735 - val_loss: 0.3799 - learning_rate: 0.0100\n",
      "Epoch 98/300\n",
      "\u001b[1m12/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7832 - loss: 0.3802\n",
      "Epoch 98: val_loss improved from 0.37817 to 0.37803, saving model to folds9.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.7836 - loss: 0.3794 - val_accuracy: 0.7869 - val_loss: 0.3780 - learning_rate: 0.0100\n",
      "Epoch 99/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7837 - loss: 0.3793\n",
      "Epoch 99: val_loss improved from 0.37803 to 0.37459, saving model to folds9.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.7838 - loss: 0.3791 - val_accuracy: 0.7912 - val_loss: 0.3746 - learning_rate: 0.0100\n",
      "Epoch 100/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7840 - loss: 0.3776 \n",
      "Epoch 100: val_loss did not improve from 0.37459\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7857 - loss: 0.3761 - val_accuracy: 0.7821 - val_loss: 0.3903 - learning_rate: 0.0100\n",
      "Epoch 101/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7837 - loss: 0.3836\n",
      "Epoch 101: val_loss did not improve from 0.37459\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.7840 - loss: 0.3833 - val_accuracy: 0.8010 - val_loss: 0.3773 - learning_rate: 0.0100\n",
      "Epoch 102/300\n",
      "\u001b[1m12/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7925 - loss: 0.3732\n",
      "Epoch 102: val_loss did not improve from 0.37459\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7918 - loss: 0.3733 - val_accuracy: 0.7802 - val_loss: 0.3763 - learning_rate: 0.0100\n",
      "Epoch 103/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7813 - loss: 0.3737 \n",
      "Epoch 103: val_loss improved from 0.37459 to 0.37327, saving model to folds9.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7826 - loss: 0.3744 - val_accuracy: 0.7875 - val_loss: 0.3733 - learning_rate: 0.0100\n",
      "Epoch 104/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7898 - loss: 0.3663\n",
      "Epoch 104: val_loss did not improve from 0.37327\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7897 - loss: 0.3666 - val_accuracy: 0.7802 - val_loss: 0.3763 - learning_rate: 0.0100\n",
      "Epoch 105/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7939 - loss: 0.3656\n",
      "Epoch 105: val_loss did not improve from 0.37327\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7936 - loss: 0.3664 - val_accuracy: 0.7912 - val_loss: 0.3754 - learning_rate: 0.0100\n",
      "Epoch 106/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7762 - loss: 0.3770  \n",
      "Epoch 106: val_loss did not improve from 0.37327\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7814 - loss: 0.3752 - val_accuracy: 0.7869 - val_loss: 0.3839 - learning_rate: 0.0100\n",
      "Epoch 107/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7927 - loss: 0.3786 \n",
      "Epoch 107: val_loss did not improve from 0.37327\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7901 - loss: 0.3784 - val_accuracy: 0.7759 - val_loss: 0.3873 - learning_rate: 0.0100\n",
      "Epoch 108/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7910 - loss: 0.3769\n",
      "Epoch 108: val_loss did not improve from 0.37327\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7910 - loss: 0.3770 - val_accuracy: 0.7869 - val_loss: 0.3783 - learning_rate: 0.0100\n",
      "Epoch 109/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7953 - loss: 0.3731\n",
      "Epoch 109: val_loss did not improve from 0.37327\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7948 - loss: 0.3734 - val_accuracy: 0.7741 - val_loss: 0.3896 - learning_rate: 0.0100\n",
      "Epoch 110/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7761 - loss: 0.3854\n",
      "Epoch 110: val_loss did not improve from 0.37327\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7773 - loss: 0.3845 - val_accuracy: 0.7833 - val_loss: 0.3985 - learning_rate: 0.0100\n",
      "Epoch 111/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7791 - loss: 0.3882\n",
      "Epoch 111: val_loss did not improve from 0.37327\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.7792 - loss: 0.3879 - val_accuracy: 0.7918 - val_loss: 0.3779 - learning_rate: 0.0100\n",
      "Epoch 112/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7829 - loss: 0.3762 \n",
      "Epoch 112: val_loss did not improve from 0.37327\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7847 - loss: 0.3755 - val_accuracy: 0.7808 - val_loss: 0.3792 - learning_rate: 0.0100\n",
      "Epoch 113/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7827 - loss: 0.3751 \n",
      "Epoch 113: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
      "\n",
      "Epoch 113: val_loss did not improve from 0.37327\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7859 - loss: 0.3736 - val_accuracy: 0.7924 - val_loss: 0.3757 - learning_rate: 0.0100\n",
      "Epoch 114/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7900 - loss: 0.3701\n",
      "Epoch 114: val_loss improved from 0.37327 to 0.36855, saving model to folds9.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.7901 - loss: 0.3699 - val_accuracy: 0.7949 - val_loss: 0.3686 - learning_rate: 0.0050\n",
      "Epoch 115/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7855 - loss: 0.3659\n",
      "Epoch 115: val_loss did not improve from 0.36855\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7858 - loss: 0.3659 - val_accuracy: 0.8010 - val_loss: 0.3700 - learning_rate: 0.0050\n",
      "Epoch 116/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7968 - loss: 0.3649\n",
      "Epoch 116: val_loss did not improve from 0.36855\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7967 - loss: 0.3649 - val_accuracy: 0.7851 - val_loss: 0.3694 - learning_rate: 0.0050\n",
      "Epoch 117/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7828 - loss: 0.3698\n",
      "Epoch 117: val_loss did not improve from 0.36855\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7832 - loss: 0.3696 - val_accuracy: 0.7912 - val_loss: 0.3690 - learning_rate: 0.0050\n",
      "Epoch 118/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7891 - loss: 0.3650\n",
      "Epoch 118: val_loss improved from 0.36855 to 0.36819, saving model to folds9.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.7893 - loss: 0.3651 - val_accuracy: 0.7943 - val_loss: 0.3682 - learning_rate: 0.0050\n",
      "Epoch 119/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7919 - loss: 0.3619\n",
      "Epoch 119: val_loss did not improve from 0.36819\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7919 - loss: 0.3621 - val_accuracy: 0.7949 - val_loss: 0.3722 - learning_rate: 0.0050\n",
      "Epoch 120/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7854 - loss: 0.3698  \n",
      "Epoch 120: val_loss did not improve from 0.36819\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7868 - loss: 0.3683 - val_accuracy: 0.7918 - val_loss: 0.3693 - learning_rate: 0.0050\n",
      "Epoch 121/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7921 - loss: 0.3666\n",
      "Epoch 121: val_loss did not improve from 0.36819\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.7922 - loss: 0.3666 - val_accuracy: 0.7912 - val_loss: 0.3762 - learning_rate: 0.0050\n",
      "Epoch 122/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7866 - loss: 0.3727\n",
      "Epoch 122: val_loss did not improve from 0.36819\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.7870 - loss: 0.3725 - val_accuracy: 0.7900 - val_loss: 0.3690 - learning_rate: 0.0050\n",
      "Epoch 123/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7921 - loss: 0.3659\n",
      "Epoch 123: val_loss did not improve from 0.36819\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.7923 - loss: 0.3659 - val_accuracy: 0.7930 - val_loss: 0.3696 - learning_rate: 0.0050\n",
      "Epoch 124/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7968 - loss: 0.3632\n",
      "Epoch 124: val_loss improved from 0.36819 to 0.36752, saving model to folds9.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.7965 - loss: 0.3633 - val_accuracy: 0.8004 - val_loss: 0.3675 - learning_rate: 0.0050\n",
      "Epoch 125/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7975 - loss: 0.3661\n",
      "Epoch 125: val_loss did not improve from 0.36752\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.7973 - loss: 0.3659 - val_accuracy: 0.7967 - val_loss: 0.3687 - learning_rate: 0.0050\n",
      "Epoch 126/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7896 - loss: 0.3637\n",
      "Epoch 126: val_loss did not improve from 0.36752\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.7912 - loss: 0.3633 - val_accuracy: 0.7949 - val_loss: 0.3693 - learning_rate: 0.0050\n",
      "Epoch 127/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7968 - loss: 0.3615\n",
      "Epoch 127: val_loss did not improve from 0.36752\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.7960 - loss: 0.3624 - val_accuracy: 0.7912 - val_loss: 0.3689 - learning_rate: 0.0050\n",
      "Epoch 128/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7902 - loss: 0.3673\n",
      "Epoch 128: val_loss improved from 0.36752 to 0.36684, saving model to folds9.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.7906 - loss: 0.3663 - val_accuracy: 0.7998 - val_loss: 0.3668 - learning_rate: 0.0050\n",
      "Epoch 129/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7932 - loss: 0.3687\n",
      "Epoch 129: val_loss did not improve from 0.36684\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.7932 - loss: 0.3685 - val_accuracy: 0.7930 - val_loss: 0.3681 - learning_rate: 0.0050\n",
      "Epoch 130/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7911 - loss: 0.3627\n",
      "Epoch 130: val_loss did not improve from 0.36684\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.7912 - loss: 0.3627 - val_accuracy: 0.7857 - val_loss: 0.3741 - learning_rate: 0.0050\n",
      "Epoch 131/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7931 - loss: 0.3626\n",
      "Epoch 131: val_loss did not improve from 0.36684\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.7930 - loss: 0.3627 - val_accuracy: 0.7961 - val_loss: 0.3717 - learning_rate: 0.0050\n",
      "Epoch 132/300\n",
      "\u001b[1m12/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7927 - loss: 0.3632\n",
      "Epoch 132: val_loss did not improve from 0.36684\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.7923 - loss: 0.3633 - val_accuracy: 0.7973 - val_loss: 0.3672 - learning_rate: 0.0050\n",
      "Epoch 133/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7974 - loss: 0.3581 \n",
      "Epoch 133: val_loss improved from 0.36684 to 0.36612, saving model to folds9.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7976 - loss: 0.3592 - val_accuracy: 0.7967 - val_loss: 0.3661 - learning_rate: 0.0050\n",
      "Epoch 134/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8010 - loss: 0.3587\n",
      "Epoch 134: val_loss did not improve from 0.36612\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8005 - loss: 0.3590 - val_accuracy: 0.7918 - val_loss: 0.3689 - learning_rate: 0.0050\n",
      "Epoch 135/300\n",
      "\u001b[1m13/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7978 - loss: 0.3579\n",
      "Epoch 135: val_loss improved from 0.36612 to 0.36407, saving model to folds9.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.7972 - loss: 0.3587 - val_accuracy: 0.8053 - val_loss: 0.3641 - learning_rate: 0.0050\n",
      "Epoch 136/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7934 - loss: 0.3630\n",
      "Epoch 136: val_loss did not improve from 0.36407\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7937 - loss: 0.3631 - val_accuracy: 0.7991 - val_loss: 0.3669 - learning_rate: 0.0050\n",
      "Epoch 137/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7911 - loss: 0.3591 \n",
      "Epoch 137: val_loss did not improve from 0.36407\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7918 - loss: 0.3609 - val_accuracy: 0.7991 - val_loss: 0.3672 - learning_rate: 0.0050\n",
      "Epoch 138/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7975 - loss: 0.3622  \n",
      "Epoch 138: val_loss did not improve from 0.36407\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7971 - loss: 0.3615 - val_accuracy: 0.8071 - val_loss: 0.3664 - learning_rate: 0.0050\n",
      "Epoch 139/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8007 - loss: 0.3624 \n",
      "Epoch 139: val_loss did not improve from 0.36407\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7994 - loss: 0.3619 - val_accuracy: 0.7973 - val_loss: 0.3672 - learning_rate: 0.0050\n",
      "Epoch 140/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7901 - loss: 0.3646\n",
      "Epoch 140: val_loss did not improve from 0.36407\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7911 - loss: 0.3641 - val_accuracy: 0.7930 - val_loss: 0.3674 - learning_rate: 0.0050\n",
      "Epoch 141/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7882 - loss: 0.3634 \n",
      "Epoch 141: val_loss did not improve from 0.36407\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7908 - loss: 0.3630 - val_accuracy: 0.7900 - val_loss: 0.3679 - learning_rate: 0.0050\n",
      "Epoch 142/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7862 - loss: 0.3615\n",
      "Epoch 142: val_loss did not improve from 0.36407\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7865 - loss: 0.3615 - val_accuracy: 0.7900 - val_loss: 0.3689 - learning_rate: 0.0050\n",
      "Epoch 143/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7954 - loss: 0.3620\n",
      "Epoch 143: val_loss did not improve from 0.36407\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7955 - loss: 0.3620 - val_accuracy: 0.7937 - val_loss: 0.3716 - learning_rate: 0.0050\n",
      "Epoch 144/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7939 - loss: 0.3643\n",
      "Epoch 144: val_loss improved from 0.36407 to 0.36406, saving model to folds9.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.7938 - loss: 0.3639 - val_accuracy: 0.7991 - val_loss: 0.3641 - learning_rate: 0.0050\n",
      "Epoch 145/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7898 - loss: 0.3640 \n",
      "Epoch 145: val_loss improved from 0.36406 to 0.36378, saving model to folds9.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7938 - loss: 0.3611 - val_accuracy: 0.7937 - val_loss: 0.3638 - learning_rate: 0.0050\n",
      "Epoch 146/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7970 - loss: 0.3591\n",
      "Epoch 146: val_loss did not improve from 0.36378\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7971 - loss: 0.3591 - val_accuracy: 0.8016 - val_loss: 0.3640 - learning_rate: 0.0050\n",
      "Epoch 147/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7921 - loss: 0.3600\n",
      "Epoch 147: val_loss did not improve from 0.36378\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7923 - loss: 0.3600 - val_accuracy: 0.8046 - val_loss: 0.3643 - learning_rate: 0.0050\n",
      "Epoch 148/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7984 - loss: 0.3569\n",
      "Epoch 148: val_loss did not improve from 0.36378\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7983 - loss: 0.3571 - val_accuracy: 0.7991 - val_loss: 0.3647 - learning_rate: 0.0050\n",
      "Epoch 149/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8007 - loss: 0.3550  \n",
      "Epoch 149: val_loss did not improve from 0.36378\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7988 - loss: 0.3568 - val_accuracy: 0.7979 - val_loss: 0.3642 - learning_rate: 0.0050\n",
      "Epoch 150/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7921 - loss: 0.3639 \n",
      "Epoch 150: val_loss did not improve from 0.36378\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7918 - loss: 0.3627 - val_accuracy: 0.7979 - val_loss: 0.3639 - learning_rate: 0.0050\n",
      "Epoch 151/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8069 - loss: 0.3523 \n",
      "Epoch 151: val_loss did not improve from 0.36378\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8022 - loss: 0.3562 - val_accuracy: 0.7979 - val_loss: 0.3663 - learning_rate: 0.0050\n",
      "Epoch 152/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8010 - loss: 0.3541\n",
      "Epoch 152: val_loss did not improve from 0.36378\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8003 - loss: 0.3548 - val_accuracy: 0.7943 - val_loss: 0.3649 - learning_rate: 0.0050\n",
      "Epoch 153/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7929 - loss: 0.3535 \n",
      "Epoch 153: val_loss did not improve from 0.36378\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7929 - loss: 0.3560 - val_accuracy: 0.7875 - val_loss: 0.3684 - learning_rate: 0.0050\n",
      "Epoch 154/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7925 - loss: 0.3587\n",
      "Epoch 154: val_loss improved from 0.36378 to 0.36346, saving model to folds9.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.7925 - loss: 0.3586 - val_accuracy: 0.8046 - val_loss: 0.3635 - learning_rate: 0.0050\n",
      "Epoch 155/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7904 - loss: 0.3629\n",
      "Epoch 155: val_loss did not improve from 0.36346\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.7906 - loss: 0.3627 - val_accuracy: 0.7973 - val_loss: 0.3674 - learning_rate: 0.0050\n",
      "Epoch 156/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8036 - loss: 0.3594\n",
      "Epoch 156: val_loss did not improve from 0.36346\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8034 - loss: 0.3594 - val_accuracy: 0.7875 - val_loss: 0.3637 - learning_rate: 0.0050\n",
      "Epoch 157/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7893 - loss: 0.3571 \n",
      "Epoch 157: val_loss did not improve from 0.36346\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7917 - loss: 0.3573 - val_accuracy: 0.7967 - val_loss: 0.3640 - learning_rate: 0.0050\n",
      "Epoch 158/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7987 - loss: 0.3579\n",
      "Epoch 158: val_loss improved from 0.36346 to 0.36314, saving model to folds9.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.7984 - loss: 0.3579 - val_accuracy: 0.8022 - val_loss: 0.3631 - learning_rate: 0.0050\n",
      "Epoch 159/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7947 - loss: 0.3607\n",
      "Epoch 159: val_loss improved from 0.36314 to 0.36273, saving model to folds9.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7950 - loss: 0.3605 - val_accuracy: 0.8046 - val_loss: 0.3627 - learning_rate: 0.0050\n",
      "Epoch 160/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7991 - loss: 0.3572\n",
      "Epoch 160: val_loss did not improve from 0.36273\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7991 - loss: 0.3573 - val_accuracy: 0.7918 - val_loss: 0.3680 - learning_rate: 0.0050\n",
      "Epoch 161/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7954 - loss: 0.3557\n",
      "Epoch 161: val_loss did not improve from 0.36273\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7956 - loss: 0.3559 - val_accuracy: 0.8071 - val_loss: 0.3658 - learning_rate: 0.0050\n",
      "Epoch 162/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8054 - loss: 0.3556 \n",
      "Epoch 162: val_loss did not improve from 0.36273\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8029 - loss: 0.3562 - val_accuracy: 0.7961 - val_loss: 0.3648 - learning_rate: 0.0050\n",
      "Epoch 163/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7932 - loss: 0.3667 \n",
      "Epoch 163: val_loss did not improve from 0.36273\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7951 - loss: 0.3610 - val_accuracy: 0.7979 - val_loss: 0.3630 - learning_rate: 0.0050\n",
      "Epoch 164/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7960 - loss: 0.3560\n",
      "Epoch 164: val_loss did not improve from 0.36273\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.7961 - loss: 0.3561 - val_accuracy: 0.7991 - val_loss: 0.3633 - learning_rate: 0.0050\n",
      "Epoch 165/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7966 - loss: 0.3592\n",
      "Epoch 165: val_loss did not improve from 0.36273\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.7967 - loss: 0.3591 - val_accuracy: 0.8016 - val_loss: 0.3755 - learning_rate: 0.0050\n",
      "Epoch 166/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7949 - loss: 0.3626\n",
      "Epoch 166: val_loss did not improve from 0.36273\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.7949 - loss: 0.3625 - val_accuracy: 0.8034 - val_loss: 0.3632 - learning_rate: 0.0050\n",
      "Epoch 167/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8053 - loss: 0.3593 \n",
      "Epoch 167: val_loss did not improve from 0.36273\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8031 - loss: 0.3579 - val_accuracy: 0.7998 - val_loss: 0.3667 - learning_rate: 0.0050\n",
      "Epoch 168/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7956 - loss: 0.3618\n",
      "Epoch 168: val_loss did not improve from 0.36273\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7961 - loss: 0.3612 - val_accuracy: 0.7857 - val_loss: 0.3660 - learning_rate: 0.0050\n",
      "Epoch 169/300\n",
      "\u001b[1m13/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7995 - loss: 0.3521 \n",
      "Epoch 169: val_loss improved from 0.36273 to 0.36155, saving model to folds9.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.7990 - loss: 0.3530 - val_accuracy: 0.8010 - val_loss: 0.3615 - learning_rate: 0.0050\n",
      "Epoch 170/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7990 - loss: 0.3544 \n",
      "Epoch 170: val_loss improved from 0.36155 to 0.36099, saving model to folds9.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7985 - loss: 0.3555 - val_accuracy: 0.8028 - val_loss: 0.3610 - learning_rate: 0.0050\n",
      "Epoch 171/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8058 - loss: 0.3544\n",
      "Epoch 171: val_loss did not improve from 0.36099\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8057 - loss: 0.3545 - val_accuracy: 0.7943 - val_loss: 0.3665 - learning_rate: 0.0050\n",
      "Epoch 172/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8039 - loss: 0.3540\n",
      "Epoch 172: val_loss did not improve from 0.36099\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.8025 - loss: 0.3548 - val_accuracy: 0.8028 - val_loss: 0.3636 - learning_rate: 0.0050\n",
      "Epoch 173/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7893 - loss: 0.3640\n",
      "Epoch 173: val_loss did not improve from 0.36099\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.7930 - loss: 0.3611 - val_accuracy: 0.7894 - val_loss: 0.3657 - learning_rate: 0.0050\n",
      "Epoch 174/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7998 - loss: 0.3601\n",
      "Epoch 174: val_loss did not improve from 0.36099\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.7999 - loss: 0.3599 - val_accuracy: 0.7961 - val_loss: 0.3621 - learning_rate: 0.0050\n",
      "Epoch 175/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7924 - loss: 0.3621\n",
      "Epoch 175: val_loss did not improve from 0.36099\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.7927 - loss: 0.3617 - val_accuracy: 0.7949 - val_loss: 0.3618 - learning_rate: 0.0050\n",
      "Epoch 176/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8031 - loss: 0.3475\n",
      "Epoch 176: val_loss improved from 0.36099 to 0.36031, saving model to folds9.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.8029 - loss: 0.3479 - val_accuracy: 0.7973 - val_loss: 0.3603 - learning_rate: 0.0050\n",
      "Epoch 177/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8015 - loss: 0.3509\n",
      "Epoch 177: val_loss improved from 0.36031 to 0.35906, saving model to folds9.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8016 - loss: 0.3510 - val_accuracy: 0.8004 - val_loss: 0.3591 - learning_rate: 0.0050\n",
      "Epoch 178/300\n",
      "\u001b[1m12/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8078 - loss: 0.3451\n",
      "Epoch 178: val_loss improved from 0.35906 to 0.35826, saving model to folds9.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.8066 - loss: 0.3469 - val_accuracy: 0.8010 - val_loss: 0.3583 - learning_rate: 0.0050\n",
      "Epoch 179/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7964 - loss: 0.3596\n",
      "Epoch 179: val_loss did not improve from 0.35826\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.7980 - loss: 0.3583 - val_accuracy: 0.7912 - val_loss: 0.3610 - learning_rate: 0.0050\n",
      "Epoch 180/300\n",
      "\u001b[1m13/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7939 - loss: 0.3597\n",
      "Epoch 180: val_loss did not improve from 0.35826\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.7949 - loss: 0.3592 - val_accuracy: 0.8059 - val_loss: 0.3619 - learning_rate: 0.0050\n",
      "Epoch 181/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8040 - loss: 0.3549 \n",
      "Epoch 181: val_loss improved from 0.35826 to 0.35711, saving model to folds9.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.8037 - loss: 0.3548 - val_accuracy: 0.8040 - val_loss: 0.3571 - learning_rate: 0.0050\n",
      "Epoch 182/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8047 - loss: 0.3450 \n",
      "Epoch 182: val_loss did not improve from 0.35711\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8032 - loss: 0.3470 - val_accuracy: 0.7979 - val_loss: 0.3602 - learning_rate: 0.0050\n",
      "Epoch 183/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8005 - loss: 0.3525\n",
      "Epoch 183: val_loss did not improve from 0.35711\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8008 - loss: 0.3525 - val_accuracy: 0.7955 - val_loss: 0.3626 - learning_rate: 0.0050\n",
      "Epoch 184/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7993 - loss: 0.3476 \n",
      "Epoch 184: val_loss did not improve from 0.35711\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7993 - loss: 0.3508 - val_accuracy: 0.7912 - val_loss: 0.3635 - learning_rate: 0.0050\n",
      "Epoch 185/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7902 - loss: 0.3578 \n",
      "Epoch 185: val_loss did not improve from 0.35711\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7953 - loss: 0.3561 - val_accuracy: 0.7998 - val_loss: 0.3572 - learning_rate: 0.0050\n",
      "Epoch 186/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7979 - loss: 0.3521\n",
      "Epoch 186: val_loss did not improve from 0.35711\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.7982 - loss: 0.3521 - val_accuracy: 0.8010 - val_loss: 0.3583 - learning_rate: 0.0050\n",
      "Epoch 187/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7945 - loss: 0.3533 \n",
      "Epoch 187: val_loss did not improve from 0.35711\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7974 - loss: 0.3534 - val_accuracy: 0.7998 - val_loss: 0.3604 - learning_rate: 0.0050\n",
      "Epoch 188/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7985 - loss: 0.3592 \n",
      "Epoch 188: val_loss did not improve from 0.35711\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7998 - loss: 0.3572 - val_accuracy: 0.7991 - val_loss: 0.3598 - learning_rate: 0.0050\n",
      "Epoch 189/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7996 - loss: 0.3545\n",
      "Epoch 189: val_loss did not improve from 0.35711\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7997 - loss: 0.3544 - val_accuracy: 0.7943 - val_loss: 0.3630 - learning_rate: 0.0050\n",
      "Epoch 190/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7991 - loss: 0.3532\n",
      "Epoch 190: val_loss did not improve from 0.35711\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.7994 - loss: 0.3532 - val_accuracy: 0.7979 - val_loss: 0.3639 - learning_rate: 0.0050\n",
      "Epoch 191/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8030 - loss: 0.3540\n",
      "Epoch 191: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "\n",
      "Epoch 191: val_loss did not improve from 0.35711\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8026 - loss: 0.3541 - val_accuracy: 0.7955 - val_loss: 0.3641 - learning_rate: 0.0050\n",
      "Epoch 192/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8075 - loss: 0.3494\n",
      "Epoch 192: val_loss did not improve from 0.35711\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8072 - loss: 0.3495 - val_accuracy: 0.7961 - val_loss: 0.3582 - learning_rate: 0.0025\n",
      "Epoch 193/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7992 - loss: 0.3542\n",
      "Epoch 193: val_loss improved from 0.35711 to 0.35563, saving model to folds9.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.7994 - loss: 0.3539 - val_accuracy: 0.8040 - val_loss: 0.3556 - learning_rate: 0.0025\n",
      "Epoch 194/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8127 - loss: 0.3390 \n",
      "Epoch 194: val_loss improved from 0.35563 to 0.35514, saving model to folds9.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8100 - loss: 0.3429 - val_accuracy: 0.7955 - val_loss: 0.3551 - learning_rate: 0.0025\n",
      "Epoch 195/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7978 - loss: 0.3461\n",
      "Epoch 195: val_loss did not improve from 0.35514\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7983 - loss: 0.3466 - val_accuracy: 0.7998 - val_loss: 0.3584 - learning_rate: 0.0025\n",
      "Epoch 196/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8006 - loss: 0.3496\n",
      "Epoch 196: val_loss improved from 0.35514 to 0.35472, saving model to folds9.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8010 - loss: 0.3496 - val_accuracy: 0.8065 - val_loss: 0.3547 - learning_rate: 0.0025\n",
      "Epoch 197/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8031 - loss: 0.3488\n",
      "Epoch 197: val_loss did not improve from 0.35472\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8032 - loss: 0.3488 - val_accuracy: 0.8059 - val_loss: 0.3558 - learning_rate: 0.0025\n",
      "Epoch 198/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8075 - loss: 0.3514 \n",
      "Epoch 198: val_loss did not improve from 0.35472\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8074 - loss: 0.3504 - val_accuracy: 0.7991 - val_loss: 0.3583 - learning_rate: 0.0025\n",
      "Epoch 199/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8061 - loss: 0.3449\n",
      "Epoch 199: val_loss did not improve from 0.35472\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8061 - loss: 0.3455 - val_accuracy: 0.8046 - val_loss: 0.3565 - learning_rate: 0.0025\n",
      "Epoch 200/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8112 - loss: 0.3511\n",
      "Epoch 200: val_loss did not improve from 0.35472\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8111 - loss: 0.3509 - val_accuracy: 0.8028 - val_loss: 0.3574 - learning_rate: 0.0025\n",
      "Epoch 201/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8021 - loss: 0.3501\n",
      "Epoch 201: val_loss improved from 0.35472 to 0.35451, saving model to folds9.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8023 - loss: 0.3500 - val_accuracy: 0.8126 - val_loss: 0.3545 - learning_rate: 0.0025\n",
      "Epoch 202/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8137 - loss: 0.3438\n",
      "Epoch 202: val_loss did not improve from 0.35451\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8134 - loss: 0.3441 - val_accuracy: 0.8071 - val_loss: 0.3548 - learning_rate: 0.0025\n",
      "Epoch 203/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8016 - loss: 0.3519 \n",
      "Epoch 203: val_loss improved from 0.35451 to 0.35395, saving model to folds9.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.8025 - loss: 0.3506 - val_accuracy: 0.8071 - val_loss: 0.3540 - learning_rate: 0.0025\n",
      "Epoch 204/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8035 - loss: 0.3423\n",
      "Epoch 204: val_loss did not improve from 0.35395\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8035 - loss: 0.3426 - val_accuracy: 0.8053 - val_loss: 0.3549 - learning_rate: 0.0025\n",
      "Epoch 205/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8036 - loss: 0.3518\n",
      "Epoch 205: val_loss improved from 0.35395 to 0.35373, saving model to folds9.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8039 - loss: 0.3516 - val_accuracy: 0.8132 - val_loss: 0.3537 - learning_rate: 0.0025\n",
      "Epoch 206/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8071 - loss: 0.3485\n",
      "Epoch 206: val_loss improved from 0.35373 to 0.35353, saving model to folds9.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8070 - loss: 0.3484 - val_accuracy: 0.8077 - val_loss: 0.3535 - learning_rate: 0.0025\n",
      "Epoch 207/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8058 - loss: 0.3480\n",
      "Epoch 207: val_loss did not improve from 0.35353\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8058 - loss: 0.3480 - val_accuracy: 0.8010 - val_loss: 0.3540 - learning_rate: 0.0025\n",
      "Epoch 208/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7928 - loss: 0.3581 \n",
      "Epoch 208: val_loss improved from 0.35353 to 0.35321, saving model to folds9.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7988 - loss: 0.3533 - val_accuracy: 0.8114 - val_loss: 0.3532 - learning_rate: 0.0025\n",
      "Epoch 209/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8174 - loss: 0.3420\n",
      "Epoch 209: val_loss did not improve from 0.35321\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8168 - loss: 0.3423 - val_accuracy: 0.8059 - val_loss: 0.3548 - learning_rate: 0.0025\n",
      "Epoch 210/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8172 - loss: 0.3375  \n",
      "Epoch 210: val_loss did not improve from 0.35321\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8123 - loss: 0.3409 - val_accuracy: 0.8095 - val_loss: 0.3538 - learning_rate: 0.0025\n",
      "Epoch 211/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8131 - loss: 0.3493 \n",
      "Epoch 211: val_loss improved from 0.35321 to 0.35318, saving model to folds9.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8114 - loss: 0.3478 - val_accuracy: 0.8071 - val_loss: 0.3532 - learning_rate: 0.0025\n",
      "Epoch 212/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8090 - loss: 0.3440 \n",
      "Epoch 212: val_loss improved from 0.35318 to 0.35307, saving model to folds9.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8091 - loss: 0.3455 - val_accuracy: 0.7973 - val_loss: 0.3531 - learning_rate: 0.0025\n",
      "Epoch 213/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7978 - loss: 0.3539 \n",
      "Epoch 213: val_loss did not improve from 0.35307\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8033 - loss: 0.3494 - val_accuracy: 0.8071 - val_loss: 0.3532 - learning_rate: 0.0025\n",
      "Epoch 214/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8115 - loss: 0.3416\n",
      "Epoch 214: val_loss improved from 0.35307 to 0.35268, saving model to folds9.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8115 - loss: 0.3419 - val_accuracy: 0.8095 - val_loss: 0.3527 - learning_rate: 0.0025\n",
      "Epoch 215/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8026 - loss: 0.3486\n",
      "Epoch 215: val_loss did not improve from 0.35268\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8027 - loss: 0.3485 - val_accuracy: 0.8053 - val_loss: 0.3541 - learning_rate: 0.0025\n",
      "Epoch 216/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8096 - loss: 0.3419 \n",
      "Epoch 216: val_loss did not improve from 0.35268\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8086 - loss: 0.3439 - val_accuracy: 0.8016 - val_loss: 0.3527 - learning_rate: 0.0025\n",
      "Epoch 217/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8079 - loss: 0.3445\n",
      "Epoch 217: val_loss did not improve from 0.35268\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8080 - loss: 0.3446 - val_accuracy: 0.8065 - val_loss: 0.3535 - learning_rate: 0.0025\n",
      "Epoch 218/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8098 - loss: 0.3440 \n",
      "Epoch 218: val_loss did not improve from 0.35268\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8089 - loss: 0.3437 - val_accuracy: 0.8046 - val_loss: 0.3544 - learning_rate: 0.0025\n",
      "Epoch 219/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8023 - loss: 0.3499\n",
      "Epoch 219: val_loss did not improve from 0.35268\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.8026 - loss: 0.3497 - val_accuracy: 0.8101 - val_loss: 0.3527 - learning_rate: 0.0025\n",
      "Epoch 220/300\n",
      "\u001b[1m12/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8024 - loss: 0.3436\n",
      "Epoch 220: val_loss improved from 0.35268 to 0.35233, saving model to folds9.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.8027 - loss: 0.3439 - val_accuracy: 0.8101 - val_loss: 0.3523 - learning_rate: 0.0025\n",
      "Epoch 221/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8066 - loss: 0.3461\n",
      "Epoch 221: val_loss improved from 0.35233 to 0.35161, saving model to folds9.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.8064 - loss: 0.3456 - val_accuracy: 0.8132 - val_loss: 0.3516 - learning_rate: 0.0025\n",
      "Epoch 222/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8107 - loss: 0.3470\n",
      "Epoch 222: val_loss did not improve from 0.35161\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.8106 - loss: 0.3461 - val_accuracy: 0.8095 - val_loss: 0.3521 - learning_rate: 0.0025\n",
      "Epoch 223/300\n",
      "\u001b[1m12/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8098 - loss: 0.3475\n",
      "Epoch 223: val_loss did not improve from 0.35161\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.8098 - loss: 0.3471 - val_accuracy: 0.8077 - val_loss: 0.3538 - learning_rate: 0.0025\n",
      "Epoch 224/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8135 - loss: 0.3443\n",
      "Epoch 224: val_loss did not improve from 0.35161\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.8131 - loss: 0.3444 - val_accuracy: 0.8034 - val_loss: 0.3534 - learning_rate: 0.0025\n",
      "Epoch 225/300\n",
      "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8068 - loss: 0.3465\n",
      "Epoch 225: val_loss did not improve from 0.35161\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.8078 - loss: 0.3460 - val_accuracy: 0.8083 - val_loss: 0.3519 - learning_rate: 0.0025\n",
      "Epoch 226/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8107 - loss: 0.3420\n",
      "Epoch 226: val_loss did not improve from 0.35161\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.8097 - loss: 0.3427 - val_accuracy: 0.8077 - val_loss: 0.3531 - learning_rate: 0.0025\n",
      "Epoch 227/300\n",
      "\u001b[1m12/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8143 - loss: 0.3454\n",
      "Epoch 227: val_loss did not improve from 0.35161\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.8135 - loss: 0.3451 - val_accuracy: 0.8065 - val_loss: 0.3537 - learning_rate: 0.0025\n",
      "Epoch 228/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8093 - loss: 0.3395\n",
      "Epoch 228: val_loss did not improve from 0.35161\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.8088 - loss: 0.3415 - val_accuracy: 0.8089 - val_loss: 0.3535 - learning_rate: 0.0025\n",
      "Epoch 229/300\n",
      "\u001b[1m12/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8150 - loss: 0.3461\n",
      "Epoch 229: val_loss did not improve from 0.35161\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.8136 - loss: 0.3457 - val_accuracy: 0.8071 - val_loss: 0.3542 - learning_rate: 0.0025\n",
      "Epoch 230/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8086 - loss: 0.3540\n",
      "Epoch 230: val_loss improved from 0.35161 to 0.35080, saving model to folds9.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.8087 - loss: 0.3535 - val_accuracy: 0.8101 - val_loss: 0.3508 - learning_rate: 0.0025\n",
      "Epoch 231/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8081 - loss: 0.3415\n",
      "Epoch 231: val_loss improved from 0.35080 to 0.35056, saving model to folds9.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.8081 - loss: 0.3416 - val_accuracy: 0.8077 - val_loss: 0.3506 - learning_rate: 0.0025\n",
      "Epoch 232/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8098 - loss: 0.3408 \n",
      "Epoch 232: val_loss did not improve from 0.35056\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8096 - loss: 0.3417 - val_accuracy: 0.8046 - val_loss: 0.3507 - learning_rate: 0.0025\n",
      "Epoch 233/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8080 - loss: 0.3463\n",
      "Epoch 233: val_loss did not improve from 0.35056\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8080 - loss: 0.3461 - val_accuracy: 0.7985 - val_loss: 0.3534 - learning_rate: 0.0025\n",
      "Epoch 234/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8092 - loss: 0.3429 \n",
      "Epoch 234: val_loss did not improve from 0.35056\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8093 - loss: 0.3431 - val_accuracy: 0.8034 - val_loss: 0.3521 - learning_rate: 0.0025\n",
      "Epoch 235/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8105 - loss: 0.3432\n",
      "Epoch 235: val_loss did not improve from 0.35056\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8104 - loss: 0.3432 - val_accuracy: 0.8083 - val_loss: 0.3509 - learning_rate: 0.0025\n",
      "Epoch 236/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8124 - loss: 0.3443\n",
      "Epoch 236: val_loss did not improve from 0.35056\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8123 - loss: 0.3442 - val_accuracy: 0.8046 - val_loss: 0.3529 - learning_rate: 0.0025\n",
      "Epoch 237/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8147 - loss: 0.3450 \n",
      "Epoch 237: val_loss did not improve from 0.35056\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8123 - loss: 0.3445 - val_accuracy: 0.8059 - val_loss: 0.3521 - learning_rate: 0.0025\n",
      "Epoch 238/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8117 - loss: 0.3415 \n",
      "Epoch 238: val_loss improved from 0.35056 to 0.35040, saving model to folds9.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.8097 - loss: 0.3424 - val_accuracy: 0.8089 - val_loss: 0.3504 - learning_rate: 0.0025\n",
      "Epoch 239/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8107 - loss: 0.3407 \n",
      "Epoch 239: val_loss did not improve from 0.35040\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8103 - loss: 0.3409 - val_accuracy: 0.8071 - val_loss: 0.3525 - learning_rate: 0.0025\n",
      "Epoch 240/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8158 - loss: 0.3390\n",
      "Epoch 240: val_loss did not improve from 0.35040\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.8150 - loss: 0.3394 - val_accuracy: 0.8083 - val_loss: 0.3511 - learning_rate: 0.0025\n",
      "Epoch 241/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8154 - loss: 0.3401\n",
      "Epoch 241: val_loss improved from 0.35040 to 0.34957, saving model to folds9.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.8152 - loss: 0.3404 - val_accuracy: 0.8083 - val_loss: 0.3496 - learning_rate: 0.0025\n",
      "Epoch 242/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8169 - loss: 0.3364\n",
      "Epoch 242: val_loss did not improve from 0.34957\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8167 - loss: 0.3367 - val_accuracy: 0.7991 - val_loss: 0.3520 - learning_rate: 0.0025\n",
      "Epoch 243/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8026 - loss: 0.3462\n",
      "Epoch 243: val_loss did not improve from 0.34957\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8029 - loss: 0.3460 - val_accuracy: 0.8101 - val_loss: 0.3497 - learning_rate: 0.0025\n",
      "Epoch 244/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8123 - loss: 0.3405\n",
      "Epoch 244: val_loss did not improve from 0.34957\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8122 - loss: 0.3405 - val_accuracy: 0.8114 - val_loss: 0.3517 - learning_rate: 0.0025\n",
      "Epoch 245/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8152 - loss: 0.3428 \n",
      "Epoch 245: val_loss did not improve from 0.34957\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8136 - loss: 0.3427 - val_accuracy: 0.8059 - val_loss: 0.3517 - learning_rate: 0.0025\n",
      "Epoch 246/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8097 - loss: 0.3406\n",
      "Epoch 246: val_loss did not improve from 0.34957\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8096 - loss: 0.3407 - val_accuracy: 0.8101 - val_loss: 0.3512 - learning_rate: 0.0025\n",
      "Epoch 247/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8120 - loss: 0.3435\n",
      "Epoch 247: val_loss did not improve from 0.34957\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8118 - loss: 0.3434 - val_accuracy: 0.8040 - val_loss: 0.3513 - learning_rate: 0.0025\n",
      "Epoch 248/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8106 - loss: 0.3460 \n",
      "Epoch 248: val_loss did not improve from 0.34957\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8107 - loss: 0.3440 - val_accuracy: 0.8083 - val_loss: 0.3509 - learning_rate: 0.0025\n",
      "Epoch 249/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8109 - loss: 0.3454\n",
      "Epoch 249: val_loss did not improve from 0.34957\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8110 - loss: 0.3452 - val_accuracy: 0.8083 - val_loss: 0.3497 - learning_rate: 0.0025\n",
      "Epoch 250/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8075 - loss: 0.3347 \n",
      "Epoch 250: val_loss did not improve from 0.34957\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8084 - loss: 0.3375 - val_accuracy: 0.8120 - val_loss: 0.3505 - learning_rate: 0.0025\n",
      "Epoch 251/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8191 - loss: 0.3368\n",
      "Epoch 251: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
      "\n",
      "Epoch 251: val_loss did not improve from 0.34957\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8187 - loss: 0.3372 - val_accuracy: 0.8046 - val_loss: 0.3525 - learning_rate: 0.0025\n",
      "Epoch 252/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8092 - loss: 0.3477\n",
      "Epoch 252: val_loss did not improve from 0.34957\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8094 - loss: 0.3473 - val_accuracy: 0.7985 - val_loss: 0.3522 - learning_rate: 0.0012\n",
      "Epoch 253/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8120 - loss: 0.3385\n",
      "Epoch 253: val_loss did not improve from 0.34957\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8120 - loss: 0.3386 - val_accuracy: 0.8059 - val_loss: 0.3520 - learning_rate: 0.0012\n",
      "Epoch 254/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8074 - loss: 0.3439\n",
      "Epoch 254: val_loss improved from 0.34957 to 0.34722, saving model to folds9.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8076 - loss: 0.3437 - val_accuracy: 0.8107 - val_loss: 0.3472 - learning_rate: 0.0012\n",
      "Epoch 255/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8185 - loss: 0.3386\n",
      "Epoch 255: val_loss improved from 0.34722 to 0.34708, saving model to folds9.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8184 - loss: 0.3387 - val_accuracy: 0.8126 - val_loss: 0.3471 - learning_rate: 0.0012\n",
      "Epoch 256/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8092 - loss: 0.3363 \n",
      "Epoch 256: val_loss did not improve from 0.34708\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8100 - loss: 0.3380 - val_accuracy: 0.8138 - val_loss: 0.3477 - learning_rate: 0.0012\n",
      "Epoch 257/300\n",
      "\u001b[1m13/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8112 - loss: 0.3399\n",
      "Epoch 257: val_loss did not improve from 0.34708\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8112 - loss: 0.3398 - val_accuracy: 0.8083 - val_loss: 0.3481 - learning_rate: 0.0012\n",
      "Epoch 258/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8162 - loss: 0.3362\n",
      "Epoch 258: val_loss did not improve from 0.34708\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8160 - loss: 0.3364 - val_accuracy: 0.8089 - val_loss: 0.3475 - learning_rate: 0.0012\n",
      "Epoch 259/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8198 - loss: 0.3395\n",
      "Epoch 259: val_loss did not improve from 0.34708\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8194 - loss: 0.3395 - val_accuracy: 0.8065 - val_loss: 0.3484 - learning_rate: 0.0012\n",
      "Epoch 260/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8126 - loss: 0.3372\n",
      "Epoch 260: val_loss did not improve from 0.34708\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8126 - loss: 0.3374 - val_accuracy: 0.8083 - val_loss: 0.3477 - learning_rate: 0.0012\n",
      "Epoch 261/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8135 - loss: 0.3405\n",
      "Epoch 261: val_loss did not improve from 0.34708\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8134 - loss: 0.3404 - val_accuracy: 0.8065 - val_loss: 0.3483 - learning_rate: 0.0012\n",
      "Epoch 262/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8093 - loss: 0.3378\n",
      "Epoch 262: val_loss did not improve from 0.34708\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8094 - loss: 0.3379 - val_accuracy: 0.8071 - val_loss: 0.3485 - learning_rate: 0.0012\n",
      "Epoch 263/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8216 - loss: 0.3312 \n",
      "Epoch 263: val_loss did not improve from 0.34708\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8174 - loss: 0.3351 - val_accuracy: 0.8071 - val_loss: 0.3481 - learning_rate: 0.0012\n",
      "Epoch 264/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8156 - loss: 0.3353\n",
      "Epoch 264: val_loss did not improve from 0.34708\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8155 - loss: 0.3355 - val_accuracy: 0.8071 - val_loss: 0.3487 - learning_rate: 0.0012\n",
      "Epoch 265/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8144 - loss: 0.3360\n",
      "Epoch 265: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "\n",
      "Epoch 265: val_loss did not improve from 0.34708\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8137 - loss: 0.3371 - val_accuracy: 0.8132 - val_loss: 0.3475 - learning_rate: 0.0012\n",
      "Epoch 266/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8089 - loss: 0.3414\n",
      "Epoch 266: val_loss improved from 0.34708 to 0.34671, saving model to folds9.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.8091 - loss: 0.3410 - val_accuracy: 0.8120 - val_loss: 0.3467 - learning_rate: 6.2500e-04\n",
      "Epoch 267/300\n",
      "\u001b[1m12/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8145 - loss: 0.3433\n",
      "Epoch 267: val_loss did not improve from 0.34671\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.8151 - loss: 0.3419 - val_accuracy: 0.8101 - val_loss: 0.3475 - learning_rate: 6.2500e-04\n",
      "Epoch 268/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8101 - loss: 0.3360\n",
      "Epoch 268: val_loss did not improve from 0.34671\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.8100 - loss: 0.3367 - val_accuracy: 0.8095 - val_loss: 0.3469 - learning_rate: 6.2500e-04\n",
      "Epoch 269/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8233 - loss: 0.3274\n",
      "Epoch 269: val_loss did not improve from 0.34671\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.8196 - loss: 0.3304 - val_accuracy: 0.8083 - val_loss: 0.3471 - learning_rate: 6.2500e-04\n",
      "Epoch 270/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8149 - loss: 0.3366\n",
      "Epoch 270: val_loss did not improve from 0.34671\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.8149 - loss: 0.3369 - val_accuracy: 0.8114 - val_loss: 0.3480 - learning_rate: 6.2500e-04\n",
      "Epoch 271/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8112 - loss: 0.3362\n",
      "Epoch 271: val_loss did not improve from 0.34671\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.8112 - loss: 0.3369 - val_accuracy: 0.8095 - val_loss: 0.3472 - learning_rate: 6.2500e-04\n",
      "Epoch 272/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8158 - loss: 0.3354\n",
      "Epoch 272: val_loss did not improve from 0.34671\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.8150 - loss: 0.3363 - val_accuracy: 0.8126 - val_loss: 0.3483 - learning_rate: 6.2500e-04\n",
      "Epoch 273/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8141 - loss: 0.3356\n",
      "Epoch 273: val_loss improved from 0.34671 to 0.34649, saving model to folds9.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8140 - loss: 0.3358 - val_accuracy: 0.8120 - val_loss: 0.3465 - learning_rate: 6.2500e-04\n",
      "Epoch 274/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8146 - loss: 0.3353 \n",
      "Epoch 274: val_loss did not improve from 0.34649\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8145 - loss: 0.3354 - val_accuracy: 0.8107 - val_loss: 0.3467 - learning_rate: 6.2500e-04\n",
      "Epoch 275/300\n",
      "\u001b[1m11/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8028 - loss: 0.3414\n",
      "Epoch 275: val_loss did not improve from 0.34649\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.8056 - loss: 0.3403 - val_accuracy: 0.8101 - val_loss: 0.3465 - learning_rate: 6.2500e-04\n",
      "Epoch 276/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8134 - loss: 0.3391\n",
      "Epoch 276: val_loss did not improve from 0.34649\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.8134 - loss: 0.3390 - val_accuracy: 0.8095 - val_loss: 0.3470 - learning_rate: 6.2500e-04\n",
      "Epoch 277/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8097 - loss: 0.3357\n",
      "Epoch 277: val_loss improved from 0.34649 to 0.34635, saving model to folds9.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8098 - loss: 0.3359 - val_accuracy: 0.8095 - val_loss: 0.3464 - learning_rate: 6.2500e-04\n",
      "Epoch 278/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8163 - loss: 0.3363\n",
      "Epoch 278: val_loss did not improve from 0.34635\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8158 - loss: 0.3364 - val_accuracy: 0.8107 - val_loss: 0.3468 - learning_rate: 6.2500e-04\n",
      "Epoch 279/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8148 - loss: 0.3331 \n",
      "Epoch 279: val_loss did not improve from 0.34635\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8144 - loss: 0.3347 - val_accuracy: 0.8120 - val_loss: 0.3464 - learning_rate: 6.2500e-04\n",
      "Epoch 280/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8173 - loss: 0.3333 \n",
      "Epoch 280: val_loss did not improve from 0.34635\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8143 - loss: 0.3355 - val_accuracy: 0.8083 - val_loss: 0.3471 - learning_rate: 6.2500e-04\n",
      "Epoch 281/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8151 - loss: 0.3335 \n",
      "Epoch 281: val_loss did not improve from 0.34635\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8149 - loss: 0.3355 - val_accuracy: 0.8107 - val_loss: 0.3477 - learning_rate: 6.2500e-04\n",
      "Epoch 282/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8105 - loss: 0.3384\n",
      "Epoch 282: val_loss did not improve from 0.34635\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8107 - loss: 0.3383 - val_accuracy: 0.8101 - val_loss: 0.3465 - learning_rate: 6.2500e-04\n",
      "Epoch 283/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8105 - loss: 0.3405 \n",
      "Epoch 283: val_loss did not improve from 0.34635\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8102 - loss: 0.3396 - val_accuracy: 0.8095 - val_loss: 0.3466 - learning_rate: 6.2500e-04\n",
      "Epoch 284/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8190 - loss: 0.3310\n",
      "Epoch 284: val_loss did not improve from 0.34635\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8185 - loss: 0.3314 - val_accuracy: 0.8114 - val_loss: 0.3465 - learning_rate: 6.2500e-04\n",
      "Epoch 285/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8185 - loss: 0.3357\n",
      "Epoch 285: val_loss did not improve from 0.34635\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8184 - loss: 0.3358 - val_accuracy: 0.8101 - val_loss: 0.3473 - learning_rate: 6.2500e-04\n",
      "Epoch 286/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8100 - loss: 0.3349\n",
      "Epoch 286: val_loss did not improve from 0.34635\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8099 - loss: 0.3350 - val_accuracy: 0.8107 - val_loss: 0.3465 - learning_rate: 6.2500e-04\n",
      "Epoch 287/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8140 - loss: 0.3395 \n",
      "Epoch 287: ReduceLROnPlateau reducing learning rate to 0.0003124999930150807.\n",
      "\n",
      "Epoch 287: val_loss did not improve from 0.34635\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8131 - loss: 0.3390 - val_accuracy: 0.8095 - val_loss: 0.3472 - learning_rate: 6.2500e-04\n",
      "Epoch 288/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8171 - loss: 0.3369\n",
      "Epoch 288: val_loss did not improve from 0.34635\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8171 - loss: 0.3369 - val_accuracy: 0.8126 - val_loss: 0.3466 - learning_rate: 3.1250e-04\n",
      "Epoch 289/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8155 - loss: 0.3388 \n",
      "Epoch 289: val_loss improved from 0.34635 to 0.34619, saving model to folds9.keras\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.8141 - loss: 0.3385 - val_accuracy: 0.8089 - val_loss: 0.3462 - learning_rate: 3.1250e-04\n",
      "Epoch 290/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8196 - loss: 0.3343 \n",
      "Epoch 290: val_loss did not improve from 0.34619\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8190 - loss: 0.3346 - val_accuracy: 0.8114 - val_loss: 0.3468 - learning_rate: 3.1250e-04\n",
      "Epoch 291/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8191 - loss: 0.3338\n",
      "Epoch 291: val_loss did not improve from 0.34619\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8190 - loss: 0.3339 - val_accuracy: 0.8101 - val_loss: 0.3464 - learning_rate: 3.1250e-04\n",
      "Epoch 292/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8158 - loss: 0.3330\n",
      "Epoch 292: val_loss did not improve from 0.34619\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8155 - loss: 0.3332 - val_accuracy: 0.8107 - val_loss: 0.3464 - learning_rate: 3.1250e-04\n",
      "Epoch 293/300\n",
      "\u001b[1m 9/15\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8229 - loss: 0.3368 \n",
      "Epoch 293: val_loss did not improve from 0.34619\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8200 - loss: 0.3372 - val_accuracy: 0.8095 - val_loss: 0.3466 - learning_rate: 3.1250e-04\n",
      "Epoch 294/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8181 - loss: 0.3318 \n",
      "Epoch 294: val_loss did not improve from 0.34619\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8158 - loss: 0.3345 - val_accuracy: 0.8138 - val_loss: 0.3464 - learning_rate: 3.1250e-04\n",
      "Epoch 295/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8184 - loss: 0.3349\n",
      "Epoch 295: val_loss did not improve from 0.34619\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8182 - loss: 0.3350 - val_accuracy: 0.8114 - val_loss: 0.3465 - learning_rate: 3.1250e-04\n",
      "Epoch 296/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8182 - loss: 0.3373\n",
      "Epoch 296: val_loss did not improve from 0.34619\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8182 - loss: 0.3372 - val_accuracy: 0.8101 - val_loss: 0.3468 - learning_rate: 3.1250e-04\n",
      "Epoch 297/300\n",
      "\u001b[1m 8/15\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8153 - loss: 0.3364  \n",
      "Epoch 297: val_loss did not improve from 0.34619\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8156 - loss: 0.3369 - val_accuracy: 0.8126 - val_loss: 0.3464 - learning_rate: 3.1250e-04\n",
      "Epoch 298/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8173 - loss: 0.3326\n",
      "Epoch 298: val_loss did not improve from 0.34619\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8170 - loss: 0.3328 - val_accuracy: 0.8126 - val_loss: 0.3467 - learning_rate: 3.1250e-04\n",
      "Epoch 299/300\n",
      "\u001b[1m14/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8181 - loss: 0.3373\n",
      "Epoch 299: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "\n",
      "Epoch 299: val_loss did not improve from 0.34619\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8178 - loss: 0.3372 - val_accuracy: 0.8126 - val_loss: 0.3463 - learning_rate: 3.1250e-04\n",
      "Epoch 300/300\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8093 - loss: 0.3450\n",
      "Epoch 300: val_loss did not improve from 0.34619\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8098 - loss: 0.3445 - val_accuracy: 0.8107 - val_loss: 0.3464 - learning_rate: 1.5625e-04\n",
      "Restoring model weights from the end of the best epoch: 289.\n",
      " Computing LSTM feature importance...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:01<00:00,  5.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# ---------------------------\n",
    "# CONFIG\n",
    "# ---------------------------\n",
    "EPOCH = 300\n",
    "BATCH_SIZE = 1024\n",
    "NUM_FOLDS = 10\n",
    "TRAIN_MODEL = True\n",
    "INFER_TEST = False\n",
    "COMPUTE_LSTM_IMPORTANCE = True\n",
    "ONE_FOLD_ONLY = False\n",
    "\n",
    "\n",
    "# Encode categorical column\n",
    "encoder = LabelEncoder()\n",
    "df['Gender of the patient'] = encoder.fit_transform(df['Gender of the patient'])\n",
    "\n",
    "# Features and target\n",
    "COLS = df.columns.drop('Result')   # list of feature column names\n",
    "X = df[COLS].values\n",
    "y = df[df.columns[-1]].values\n",
    "\n",
    "# reshape into (samples, timesteps, features)\n",
    "X = X.reshape((X.shape[0], 1, X.shape[1]))\n",
    "\n",
    "# encode target labels -> integers starting at 0\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "\n",
    "# one-hot encode target for classification\n",
    "num_classes = len(np.unique(y_encoded))\n",
    "y_cat = to_categorical(y_encoded, num_classes=num_classes)\n",
    "\n",
    "# for test placeholder (replace with real test set if available)\n",
    "test = X.copy()\n",
    "\n",
    "# ---------------------------\n",
    "# BUILD MODEL\n",
    "# ---------------------------\n",
    "def build_lstm(input_shape, num_classes):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(80, activation=\"tanh\", input_shape=input_shape))\n",
    "    model.add(Dense(num_classes, activation=\"softmax\"))\n",
    "    model.compile(optimizer=Adam(learning_rate=0.01),\n",
    "                  loss=\"categorical_crossentropy\",\n",
    "                  metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "# ---------------------------\n",
    "# STRATEGY\n",
    "# ---------------------------\n",
    "gpu_strategy = tf.distribute.get_strategy()\n",
    "\n",
    "with gpu_strategy.scope():\n",
    "    kf = KFold(n_splits=NUM_FOLDS, shuffle=True, random_state=2021)\n",
    "    test_preds = []\n",
    "\n",
    "    for fold, (train_idx, test_idx) in enumerate(kf.split(X, y_cat)):\n",
    "        K.clear_session()\n",
    "\n",
    "        print('-'*15, '>', f'Fold {fold+1}', '<', '-'*15)\n",
    "        X_train, X_valid = X[train_idx], X[test_idx]\n",
    "        y_train, y_valid = y_cat[train_idx], y_cat[test_idx]\n",
    "\n",
    "        checkpoint_filepath = f\"folds{fold}.keras\"\n",
    "\n",
    "        if TRAIN_MODEL:\n",
    "            model = build_lstm(input_shape=X.shape[-2:], num_classes=num_classes)\n",
    "\n",
    "            lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=10, verbose=1)\n",
    "            es = EarlyStopping(monitor=\"val_loss\", patience=60, verbose=1, mode=\"min\", restore_best_weights=True)\n",
    "            sv = keras.callbacks.ModelCheckpoint(\n",
    "                  checkpoint_filepath,\n",
    "                  monitor='val_loss',\n",
    "                  verbose=1,\n",
    "                  save_best_only=True,\n",
    "                  save_weights_only=False,\n",
    "                  mode='auto',\n",
    "                  save_freq='epoch'\n",
    "                  )\n",
    "\n",
    "            model.fit(X_train, y_train,\n",
    "                      validation_data=(X_valid, y_valid),\n",
    "                      epochs=EPOCH,\n",
    "                      batch_size=BATCH_SIZE,\n",
    "                      callbacks=[lr, es, sv])\n",
    "\n",
    "        else:\n",
    "            model = keras.models.load_model(checkpoint_filepath)\n",
    "\n",
    "        # ---------------------------\n",
    "        # TEST PREDICTION\n",
    "        # ---------------------------\n",
    "        if INFER_TEST:\n",
    "            print(' Predicting test data...')\n",
    "            test_preds.append(model.predict(test, verbose=0).squeeze())\n",
    "\n",
    "        # ---------------------------\n",
    "        # FEATURE IMPORTANCE (Permutation)\n",
    "        # ---------------------------\n",
    "        if COMPUTE_LSTM_IMPORTANCE:\n",
    "            results = []\n",
    "            print(' Computing LSTM feature importance...')\n",
    "\n",
    "            # Baseline accuracy\n",
    "            oof_preds = model.predict(X_valid, verbose=0)\n",
    "            baseline_acc = np.mean(np.argmax(oof_preds, axis=1) == np.argmax(y_valid, axis=1))\n",
    "            results.append({'feature':'BASELINE','accuracy':baseline_acc})\n",
    "\n",
    "            for k in tqdm(range(len(COLS))):\n",
    "                # Shuffle feature k\n",
    "                save_col = X_valid[:,:,k].copy()\n",
    "                np.random.shuffle(X_valid[:,:,k])\n",
    "\n",
    "                # Compute accuracy with shuffled feature\n",
    "                oof_preds = model.predict(X_valid, verbose=0)\n",
    "                acc = np.mean(np.argmax(oof_preds, axis=1) == np.argmax(y_valid, axis=1))\n",
    "                results.append({'feature':COLS[k],'accuracy':acc})\n",
    "\n",
    "                # Restore column\n",
    "                X_valid[:,:,k] = save_col\n",
    "\n",
    "            # Display importance\n",
    "            print()\n",
    "            df_imp = pd.DataFrame(results)\n",
    "            df_imp = df_imp.sort_values('accuracy')\n",
    "            plt.figure(figsize=(20,10))\n",
    "            plt.barh(np.arange(len(COLS)+1), df_imp.accuracy)\n",
    "            plt.yticks(np.arange(len(COLS)+1), df_imp.feature.values)\n",
    "            plt.title('LSTM Feature Importance', size=16)\n",
    "            plt.ylim((-1,len(COLS)+1))\n",
    "            plt.plot([baseline_acc,baseline_acc],[-1,len(COLS)+1], '--', color='orange',\n",
    "                     label=f'Baseline OOF\\nAcc={baseline_acc:.3f}')\n",
    "            plt.xlabel(f'Fold {fold+1} OOF Accuracy with feature permuted', size=14)\n",
    "            plt.ylabel('Feature', size=14)\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "\n",
    "            # Save importance\n",
    "            df_imp = df_imp.sort_values('accuracy',ascending=False)\n",
    "            df_imp.to_csv(f'lstm_feature_importance_fold_{fold+1}.csv',index=False)\n",
    "\n",
    "        # ---------------------------\n",
    "        # ONLY ONE FOLD?\n",
    "        # ---------------------------\n",
    "        if ONE_FOLD_ONLY:\n",
    "            break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
